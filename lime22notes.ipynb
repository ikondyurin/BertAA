{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def __data_labels_distances(\n",
    "                                indexed_string,\n",
    "                                classifier_fn,\n",
    "                                num_samples,\n",
    "                                distance_metric='cosine'):\n",
    "\n",
    "    def distance_fn(x):\n",
    "        return sklearn.metrics.pairwise.pairwise_distances(\n",
    "            x, x[0], metric=distance_metric).ravel() * 100\n",
    "\n",
    "    doc_size = indexed_string.num_words()\n",
    "    sample = random_state.randint(1, doc_size + 1, num_samples - 1)\n",
    "    data = np.ones((num_samples, doc_size))\n",
    "    data[0] = np.ones(doc_size)\n",
    "    features_range = range(doc_size)\n",
    "    inverse_data = [indexed_string.raw_string()]\n",
    "    for i, size in enumerate(sample, start=1):\n",
    "        inactive = random_state.choice(features_range, size,\n",
    "                                            replace=False)\n",
    "        data[i, inactive] = 0\n",
    "        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "    #labels = classifier_fn(inverse_data)\n",
    "    distances = distance_fn(sp.sparse.csr_matrix(data))\n",
    "    #return data, labels, distances\n",
    "    return data, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = __data_labels_distances(indexed_string=indexed_string, classifier_fn=None, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = distances[0]\n",
    "d2 = distances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"textcomb4.txt\", 'r') as text:\n",
    "    text_instance = text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width=25,\n",
    "kernel=None,\n",
    "verbose=False,\n",
    "class_names=None,\n",
    "feature_selection='auto',\n",
    "split_expression=r'\\W+',\n",
    "bow=True,\n",
    "mask_string=None,\n",
    "char_level=False\n",
    "random_state = check_random_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 0\n",
      "1 8\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 3\n",
      "2 8\n",
      "2 9\n"
     ]
    }
   ],
   "source": [
    "indexed_string = (MyIndexedString(text_instance, bow=bow,\n",
    "                                        split_expression=split_expression,\n",
    "                                        mask_string=mask_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_instance = 'Rinoa let let out a soft giggle. \"Okay Uncle Rinoa Laguna.\" $&*&*&$\"As always, make a giggle Rinoa yourselves at a good home!\" '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rinoa': 0, 'let': 1, 'out': 2, 'a': 3, 'soft': 4, 'giggle': 5, 'Okay': 6, 'Uncle': 7, 'Laguna': 8} {'As': 0, 'always': 1, 'make': 2, 'a': 3, 'giggle': 4, 'Rinoa': 5, 'yourselves': 6, 'at': 7, 'good': 8, 'home': 9}\n"
     ]
    }
   ],
   "source": [
    "print(vocab, vocab_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 18], [2, 4], [6], [8], [10], [12], [14], [16], [20]] 9 [[22], [24], [26], [28, 38], [30], [32], [34], [36], [40], [42]] 19\n",
      "['Rinoa', 'let', 'out', 'a', 'soft', 'giggle', 'Okay', 'Uncle', 'Laguna'] ['As', 'always', 'make', 'a', 'giggle', 'Rinoa', 'yourselves', 'at', 'good', 'home']\n"
     ]
    }
   ],
   "source": [
    "l = indexed_string\n",
    "print(l.positions, l.sep, l.positions_right, l.num_words())\n",
    "print(l.inverse_vocab, l.inverse_vocab_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sep:  9 docsize:  19\n",
      "left bow\n",
      "sample:  [8 5]\n",
      "1 9 8\n",
      "inactive [4 2 5 1 6 7 8 3]\n",
      "2 9 5\n",
      "inactive [3 5 0 1 4]\n",
      "['Rinoa let let out a soft giggle. \"Okay Uncle Rinoa Laguna.\" $&*&*&$\"As always, make a giggle Rinoa yourselves at a good home!\" ', 'Rinoa      . \"  Rinoa .\" $&*&*&$\"As always, make a giggle Rinoa yourselves at a good home!\" ', '   out   . \"Okay Uncle  Laguna.\" $&*&*&$\"As always, make a giggle Rinoa yourselves at a good home!\" ']\n"
     ]
    }
   ],
   "source": [
    "distances = __data_labels_distances(indexed_string=indexed_string, classifier_fn=None, num_samples=3, mode='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(distances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __data_labels_distances(\n",
    "                                indexed_string,\n",
    "                                classifier_fn,\n",
    "                                num_samples,\n",
    "                                distance_metric='cosine',\n",
    "                                mode='rand'):\n",
    "        \"\"\"Generates a neighborhood around a prediction.\n",
    "        Generates neighborhood data by randomly removing words from\n",
    "        the instance, and predicting with the classifier. Uses cosine distance\n",
    "        to compute distances between original and perturbed instances.\n",
    "        Args:\n",
    "            indexed_string: document (IndexedString) to be explained,\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a string and outputs prediction probabilities. For\n",
    "                ScikitClassifier, this is classifier.predict_proba.\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for sample weighting,\n",
    "                defaults to cosine similarity.\n",
    "        Returns:\n",
    "            A tuple (data, labels, distances), where:\n",
    "                data: dense num_samples * K binary matrix, where K is the\n",
    "                    number of tokens in indexed_string. The first row is the\n",
    "                    original instance, and thus a row of ones.\n",
    "                labels: num_samples * L matrix, where L is the number of target\n",
    "                    labels\n",
    "                distances: cosine distance between the original instance and\n",
    "                    each perturbed instance (computed in the binary 'data'\n",
    "                    matrix), times 100.\n",
    "        \"\"\"\n",
    "        def distance_fn(x):\n",
    "            return sklearn.metrics.pairwise.pairwise_distances(\n",
    "                    x, x[0], metric=distance_metric).ravel() * 100\n",
    "\n",
    "\n",
    "        doc_size = indexed_string.num_words()\n",
    "\n",
    "        sep = indexed_string.return_sep()\n",
    "        print(\"sep: \", sep, \"docsize: \", doc_size)\n",
    "\n",
    "        global sample\n",
    "        #\n",
    "        sample = random_state.randint(1, doc_size + 1, num_samples - 1)\n",
    "\n",
    "        data = np.ones((num_samples, doc_size))\n",
    "        data[0] = np.ones(doc_size)\n",
    "        inverse_data = [indexed_string.raw_string()]\n",
    "\n",
    "        if not indexed_string.bow: \n",
    "            \n",
    "            if sep < doc_size:\n",
    "                print(\"separation on\")\n",
    "                #\n",
    "                #\n",
    "                sample_left = random_state.randint(1, sep + 1, num_samples - 1)\n",
    "                sample_right = random_state.randint(1, doc_size - sep, num_samples - 1)\n",
    "\n",
    "                features_range_left = range(0, sep)\n",
    "                features_range_right = range(sep, doc_size)\n",
    "\n",
    "                if mode == 'left':\n",
    "                    print(\"left\")\n",
    "                    sample = sample_left\n",
    "                    features_range = features_range_left\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        #\n",
    "                        inactive = random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                elif mode == 'right':\n",
    "                    print(\"right\")\n",
    "                    sample = sample_right\n",
    "                    features_range = features_range_right\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        print(i, len(features_range), size)\n",
    "                        #\n",
    "                        inactive = random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                else:\n",
    "                    print('rand')\n",
    "                    dir = self.random_state.randint(2, size = num_samples - 1)\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        #print(dir[i-1])\n",
    "                        if dir[i-1]:\n",
    "                            #\n",
    "                            inactive = random_state.choice(features_range_right, sample_right[i-1],\n",
    "                                                        replace=False)\n",
    "                        else:\n",
    "                            #\n",
    "                            inactive = random_state.choice(features_range_left, sample_left[i-1],\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "            else:\n",
    "                features_range = range(doc_size)\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    #\n",
    "                    inactive = random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    inverse_data.append(indexed_string.inverse_removing(inactive))            \n",
    "\n",
    "        else:\n",
    "\n",
    "            if indexed_string.positions_right:\n",
    "                #\n",
    "                #\n",
    "                sample_left = random_state.randint(1, sep + 1, num_samples - 1)\n",
    "                sample_right = random_state.randint(1, doc_size - sep, num_samples - 1)\n",
    "\n",
    "                features_range_left = range(0, sep)\n",
    "                features_range_right = range(0, doc_size - sep)\n",
    "\n",
    "                global save_sample\n",
    "                save_sample = sample\n",
    "\n",
    "                if mode == 'left':\n",
    "                    print(\"left bow\")\n",
    "                    sample = sample_left\n",
    "                    print(\"sample: \", sample)\n",
    "                    features_range = features_range_left\n",
    "\n",
    "                elif mode == 'right':\n",
    "                    print(\"right bow\")\n",
    "                    sample = sample_right\n",
    "                    print(\"sample: \", sample)\n",
    "                    features_range = features_range_right\n",
    "\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    print(i, len(features_range), size)\n",
    "                    #\n",
    "                    inactive = random_state.choice(features_range, size,\n",
    "                                                    replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    print('inactive', inactive)\n",
    "                    inverse_data.append(indexed_string.inverse_removing(inactive, mode=mode))\n",
    "\n",
    "            else:\n",
    "                features_range = range(doc_size)\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    #\n",
    "                    inactive = random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "        #labels = classifier_fn(inverse_data)\n",
    "        print(inverse_data)\n",
    "        distances = distance_fn(sp.sparse.csr_matrix(data))\n",
    "        #return data, labels, distances\n",
    "        return data, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIndexedString(object):\n",
    "    \"\"\"String with various indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_string, split_expression=r'\\W+', bow=True,\n",
    "                 mask_string=None):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            raw_string: string with raw text in it\n",
    "            split_expression: Regex string or callable. If regex string, will be used with re.split.\n",
    "                If callable, the function should return a list of tokens.\n",
    "            bow: if True, a word is the same everywhere in the text - i.e. we\n",
    "                 will index multiple occurrences of the same word. If False,\n",
    "                 order matters, so that the same word will have different ids\n",
    "                 according to position.\n",
    "            mask_string: If not None, replace words with this if bow=False\n",
    "                if None, default value is UNKWORDZ\n",
    "        \"\"\"\n",
    "        self.raw = raw_string\n",
    "        self.mask_string = 'UNKWORDZ' if mask_string is None else mask_string\n",
    "\n",
    "        if callable(split_expression):\n",
    "            tokens = split_expression(self.raw)\n",
    "            self.as_list = self._segment_with_tokens(self.raw, tokens)\n",
    "            tokens = set(tokens)\n",
    "\n",
    "            def non_word(string):\n",
    "                return string not in tokens\n",
    "\n",
    "        else:\n",
    "            # with the split_expression as a non-capturing group (?:), we don't need to filter out\n",
    "            # the separator character from the split results.\n",
    "            splitter = re.compile(r'(%s)|$' % split_expression)\n",
    "            self.as_list = [s for s in splitter.split(self.raw) if s]\n",
    "            non_word = splitter.match\n",
    "\n",
    "        self.as_np = np.array(self.as_list)\n",
    "        self.string_start = np.hstack(\n",
    "            ([0], np.cumsum([len(x) for x in self.as_np[:-1]])))\n",
    "\n",
    "        global vocab\n",
    "\n",
    "        vocab = {}\n",
    "        self.inverse_vocab = []\n",
    "        self.positions = []\n",
    "        self.bow = bow\n",
    "\n",
    "        self.sep = None\n",
    "\n",
    "\n",
    "        #added in lime22\n",
    "        self.positions_right = []\n",
    "        self.inverse_vocab_right = []\n",
    "        global vocab_right\n",
    "        vocab_right = {}\n",
    "\n",
    "        non_vocab = set()\n",
    "        for i, word in enumerate(self.as_np):\n",
    "            if \"$&*&*&$\" in word:\n",
    "                self.sep = len(self.inverse_vocab)\n",
    "            if word in non_vocab:\n",
    "                continue\n",
    "            if non_word(word):\n",
    "                non_vocab.add(word)\n",
    "                continue\n",
    "            if bow:\n",
    "\n",
    "                \"\"\"if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "                    self.inverse_vocab.append(word)\n",
    "                    self.positions.append([])\n",
    "                idx_word = vocab[word]\n",
    "                self.positions[idx_word].append(i)\"\"\"\n",
    "\n",
    "                if not self.sep:\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = len(vocab)\n",
    "                        self.inverse_vocab.append(word)\n",
    "                        self.positions.append([])\n",
    "                    idx_word = vocab[word]\n",
    "                    print(1, idx_word)\n",
    "                    self.positions[idx_word].append(i)\n",
    "                else:\n",
    "                    if word not in vocab_right:\n",
    "                        vocab_right[word] = len(vocab_right)\n",
    "                        self.inverse_vocab_right.append(word)\n",
    "                        self.positions_right.append([])\n",
    "                    idx_word = vocab_right[word]\n",
    "                    print(2, idx_word)\n",
    "                    self.positions_right[idx_word].append(i)\n",
    "\n",
    "            else:\n",
    "                self.inverse_vocab.append(word)\n",
    "                self.positions.append(i)\n",
    "\n",
    "        if not bow:\n",
    "            self.positions = np.array(self.positions)\n",
    "\n",
    "        if not self.sep:\n",
    "            self.sep = len(self.inverse_vocab)\n",
    "\n",
    "    def return_sep(self):\n",
    "        \"\"\"Return the index of the separator sequence\"\"\"\n",
    "        return self.sep\n",
    "\n",
    "    def raw_string(self):\n",
    "        \"\"\"Returns the original raw string\"\"\"\n",
    "        return self.raw\n",
    "\n",
    "    def num_words(self):\n",
    "        \"\"\"Returns the number of tokens in the vocabulary for this document.\"\"\"\n",
    "        return len(self.inverse_vocab) if not self.inverse_vocab_right else len(self.inverse_vocab) + len(self.inverse_vocab_right)\n",
    "\n",
    "    def word(self, id_):\n",
    "        \"\"\"Returns the word that corresponds to id_ (int)\"\"\"\n",
    "        return self.inverse_vocab[id_]\n",
    "\n",
    "    def string_position(self, id_):\n",
    "        \"\"\"Returns a np array with indices to id_ (int) occurrences\"\"\"\n",
    "        if self.bow:\n",
    "            return self.string_start[self.positions[id_]]\n",
    "        else:\n",
    "            return self.string_start[[self.positions[id_]]]\n",
    "\n",
    "    def inverse_removing(self, words_to_remove, mode='left'):\n",
    "        \"\"\"Returns a string after removing the appropriate words.\n",
    "        If self.bow is false, replaces word with UNKWORDZ instead of removing\n",
    "        it.\n",
    "        Args:\n",
    "            words_to_remove: list of ids (ints) to remove\n",
    "        Returns:\n",
    "            original raw string with appropriate words removed.\n",
    "        \"\"\"\n",
    "        mask = np.ones(self.as_np.shape[0], dtype='bool')\n",
    "        if mode == \"left\":\n",
    "            mask[self.__get_idxs(words_to_remove)] = False\n",
    "        elif mode == \"right\":\n",
    "            mask[self.__get_idxs_right(words_to_remove)] = False\n",
    "        if not self.bow:\n",
    "            return ''.join(\n",
    "                [self.as_list[i] if mask[i] else self.mask_string\n",
    "                 for i in range(mask.shape[0])])\n",
    "        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n",
    "\n",
    "    @staticmethod\n",
    "    def _segment_with_tokens(text, tokens):\n",
    "        \"\"\"Segment a string around the tokens created by a passed-in tokenizer\"\"\"\n",
    "        list_form = []\n",
    "        text_ptr = 0\n",
    "        for token in tokens:\n",
    "            inter_token_string = []\n",
    "            while not text[text_ptr:].startswith(token):\n",
    "                inter_token_string.append(text[text_ptr])\n",
    "                text_ptr += 1\n",
    "                if text_ptr >= len(text):\n",
    "                    raise ValueError(\"Tokenization produced tokens that do not belong in string!\")\n",
    "            text_ptr += len(token)\n",
    "            if inter_token_string:\n",
    "                list_form.append(''.join(inter_token_string))\n",
    "            list_form.append(token)\n",
    "        if text_ptr < len(text):\n",
    "            list_form.append(text[text_ptr:])\n",
    "        return list_form\n",
    "\n",
    "    def __get_idxs(self, words):\n",
    "        \"\"\"Returns indexes to appropriate words.\"\"\"\n",
    "        if self.bow:\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [self.positions[z] for z in words]))\n",
    "        else:\n",
    "            return self.positions[words]\n",
    "\n",
    "    def __get_idxs_right(self, words):\n",
    "        \"\"\"Returns indexes to appropriate words.\"\"\"\n",
    "        if self.bow:\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [self.positions_right[z] for z in words]))\n",
    "        else:\n",
    "            return self.positions_right[words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code to compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldMyIndexedString(object):\n",
    "    \"\"\"String with various indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_string, split_expression=r'\\W+', bow=True,\n",
    "                 mask_string=None):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            raw_string: string with raw text in it\n",
    "            split_expression: Regex string or callable. If regex string, will be used with re.split.\n",
    "                If callable, the function should return a list of tokens.\n",
    "            bow: if True, a word is the same everywhere in the text - i.e. we\n",
    "                 will index multiple occurrences of the same word. If False,\n",
    "                 order matters, so that the same word will have different ids\n",
    "                 according to position.\n",
    "            mask_string: If not None, replace words with this if bow=False\n",
    "                if None, default value is UNKWORDZ\n",
    "        \"\"\"\n",
    "        self.raw = raw_string\n",
    "        self.mask_string = 'UNKWORDZ' if mask_string is None else mask_string\n",
    "\n",
    "        if callable(split_expression):\n",
    "            tokens = split_expression(self.raw)\n",
    "            self.as_list = self._segment_with_tokens(self.raw, tokens)\n",
    "            tokens = set(tokens)\n",
    "\n",
    "            def non_word(string):\n",
    "                return string not in tokens\n",
    "\n",
    "        else:\n",
    "            # with the split_expression as a non-capturing group (?:), we don't need to filter out\n",
    "            # the separator character from the split results.\n",
    "            splitter = re.compile(r'(%s)|$' % split_expression)\n",
    "            self.as_list = [s for s in splitter.split(self.raw) if s]\n",
    "            non_word = splitter.match\n",
    "\n",
    "        self.as_np = np.array(self.as_list)\n",
    "        self.string_start = np.hstack(\n",
    "            ([0], np.cumsum([len(x) for x in self.as_np[:-1]])))\n",
    "        vocab = {}\n",
    "        self.inverse_vocab = []\n",
    "        self.positions = []\n",
    "        self.bow = bow\n",
    "\n",
    "        self.sep = None\n",
    "\n",
    "        non_vocab = set()\n",
    "        for i, word in enumerate(self.as_np):\n",
    "            if \"$&*&*&$\" in word:\n",
    "                self.sep = len(self.inverse_vocab)\n",
    "            if word in non_vocab:\n",
    "                continue\n",
    "            if non_word(word):\n",
    "                non_vocab.add(word)\n",
    "                continue\n",
    "            if bow:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "                    self.inverse_vocab.append(word)\n",
    "                    self.positions.append([])\n",
    "                idx_word = vocab[word]\n",
    "                self.positions[idx_word].append(i)\n",
    "            else:\n",
    "                self.inverse_vocab.append(word)\n",
    "                self.positions.append(i)\n",
    "        if not bow:\n",
    "            self.positions = np.array(self.positions)\n",
    "\n",
    "        if not self.sep:\n",
    "            self.sep = len(self.inverse_vocab)\n",
    "\n",
    "    def return_sep(self):\n",
    "        \"\"\"Return the index of the separator sequence\"\"\"\n",
    "        return self.sep\n",
    "\n",
    "    def raw_string(self):\n",
    "        \"\"\"Returns the original raw string\"\"\"\n",
    "        return self.raw\n",
    "\n",
    "    def num_words(self):\n",
    "        \"\"\"Returns the number of tokens in the vocabulary for this document.\"\"\"\n",
    "        return len(self.inverse_vocab)\n",
    "\n",
    "    def word(self, id_):\n",
    "        \"\"\"Returns the word that corresponds to id_ (int)\"\"\"\n",
    "        return self.inverse_vocab[id_]\n",
    "\n",
    "    def string_position(self, id_):\n",
    "        \"\"\"Returns a np array with indices to id_ (int) occurrences\"\"\"\n",
    "        if self.bow:\n",
    "            return self.string_start[self.positions[id_]]\n",
    "        else:\n",
    "            return self.string_start[[self.positions[id_]]]\n",
    "\n",
    "    def inverse_removing(self, words_to_remove):\n",
    "        \"\"\"Returns a string after removing the appropriate words.\n",
    "        If self.bow is false, replaces word with UNKWORDZ instead of removing\n",
    "        it.\n",
    "        Args:\n",
    "            words_to_remove: list of ids (ints) to remove\n",
    "        Returns:\n",
    "            original raw string with appropriate words removed.\n",
    "        \"\"\"\n",
    "        mask = np.ones(self.as_np.shape[0], dtype='bool')\n",
    "        mask[self.__get_idxs(words_to_remove)] = False\n",
    "        if not self.bow:\n",
    "            return ''.join(\n",
    "                [self.as_list[i] if mask[i] else self.mask_string\n",
    "                 for i in range(mask.shape[0])])\n",
    "        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n",
    "\n",
    "    @staticmethod\n",
    "    def _segment_with_tokens(text, tokens):\n",
    "        \"\"\"Segment a string around the tokens created by a passed-in tokenizer\"\"\"\n",
    "        list_form = []\n",
    "        text_ptr = 0\n",
    "        for token in tokens:\n",
    "            inter_token_string = []\n",
    "            while not text[text_ptr:].startswith(token):\n",
    "                inter_token_string.append(text[text_ptr])\n",
    "                text_ptr += 1\n",
    "                if text_ptr >= len(text):\n",
    "                    raise ValueError(\"Tokenization produced tokens that do not belong in string!\")\n",
    "            text_ptr += len(token)\n",
    "            if inter_token_string:\n",
    "                list_form.append(''.join(inter_token_string))\n",
    "            list_form.append(token)\n",
    "        if text_ptr < len(text):\n",
    "            list_form.append(text[text_ptr:])\n",
    "        return list_form\n",
    "\n",
    "    def __get_idxs(self, words):\n",
    "        \"\"\"Returns indexes to appropriate words.\"\"\"\n",
    "        if self.bow:\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [self.positions[z] for z in words]))\n",
    "        else:\n",
    "            return self.positions[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from lime import explanation\n",
    "from lime import lime_base\n",
    "\n",
    "class OldMyLimeTextExplainer(LimeTextExplainer):\n",
    "    \"\"\"Explains text classifiers.\n",
    "       Currently, we are using an exponential kernel on cosine distance, and\n",
    "       restricting explanations to words that are present in documents.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel_width=25,\n",
    "                 kernel=None,\n",
    "                 verbose=False,\n",
    "                 class_names=None,\n",
    "                 feature_selection='auto',\n",
    "                 split_expression=r'\\W+',\n",
    "                 bow=True,\n",
    "                 mask_string=None,\n",
    "                 random_state=None,\n",
    "                 char_level=False,\n",
    "                 mode='rand'):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "            kernel: similarity kernel that takes euclidean distances and kernel\n",
    "                width as input and outputs weights in (0,1). If None, defaults to\n",
    "                an exponential kernel.\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            class_names: list of class names, ordered according to whatever the\n",
    "                classifier is using. If not present, class names will be '0',\n",
    "                '1', ...\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            split_expression: Regex string or callable. If regex string, will be used with re.split.\n",
    "                If callable, the function should return a list of tokens.\n",
    "            bow: if True (bag of words), will perturb input data by removing\n",
    "                all occurrences of individual words or characters.\n",
    "                Explanations will be in terms of these words. Otherwise, will\n",
    "                explain in terms of word-positions, so that a word may be\n",
    "                important the first time it appears and unimportant the second.\n",
    "                Only set to false if the classifier uses word order in some way\n",
    "                (bigrams, etc), or if you set char_level=True.\n",
    "            mask_string: String used to mask tokens or characters if bow=False\n",
    "                if None, will be 'UNKWORDZ' if char_level=False, chr(0)\n",
    "                otherwise.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "            char_level: an boolean identifying that we treat each character\n",
    "                as an independent occurence in the string\n",
    "        \"\"\"\n",
    "\n",
    "        if kernel is None:\n",
    "            def kernel(d, kernel_width):\n",
    "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
    "\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.base = lime_base.LimeBase(kernel_fn, verbose,\n",
    "                                       random_state=self.random_state)\n",
    "        self.class_names = class_names\n",
    "        self.vocabulary = None\n",
    "        self.feature_selection = feature_selection\n",
    "        self.bow = bow\n",
    "        self.mask_string = mask_string\n",
    "        self.split_expression = split_expression\n",
    "        self.char_level = char_level\n",
    "        self.mode = mode\n",
    "\n",
    "    def explain_instance(self,\n",
    "                         text_instance,\n",
    "                         classifier_fn,\n",
    "                         labels=(1,),\n",
    "                         top_labels=None,\n",
    "                         num_features=10,\n",
    "                         num_samples=5000,\n",
    "                         distance_metric='cosine',\n",
    "                         model_regressor=None):\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "        First, we generate neighborhood data by randomly hiding features from\n",
    "        the instance (see __data_labels_distance_mapping). We then learn\n",
    "        locally weighted linear models on this neighborhood data to explain\n",
    "        each of the classes in an interpretable way (see lime_base.py).\n",
    "        Args:\n",
    "            text_instance: raw text string to be explained.\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a list of d strings and outputs a (d, k) numpy array with\n",
    "                prediction probabilities, where k is the number of classes.\n",
    "                For ScikitClassifiers , this is classifier.predict_proba.\n",
    "            labels: iterable with labels to be explained.\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for sample weighting,\n",
    "                defaults to cosine similarity\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "            to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
    "            and 'sample_weight' as a parameter to model_regressor.fit()\n",
    "        Returns:\n",
    "            An Explanation object (see explanation.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        indexed_string = (IndexedCharacters(\n",
    "            text_instance, bow=self.bow, mask_string=self.mask_string)\n",
    "                          if self.char_level else\n",
    "                          MyIndexedString(text_instance, bow=self.bow,\n",
    "                                        split_expression=self.split_expression,\n",
    "                                        mask_string=self.mask_string))\n",
    "        domain_mapper = TextDomainMapper(indexed_string)\n",
    "        data, yss, distances = self.__data_labels_distances(\n",
    "            indexed_string, classifier_fn, num_samples,\n",
    "            distance_metric=distance_metric, mode = self.mode)\n",
    "        if self.class_names is None:\n",
    "            self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "        #ret_exp = explanation.Explanation(domain_mapper=domain_mapper,\n",
    "        ret_exp = MyExplanation(domain_mapper=domain_mapper,\n",
    "                                          class_names=self.class_names,\n",
    "                                          random_state=self.random_state)\n",
    "        ret_exp.predict_proba = yss[0]\n",
    "        if top_labels:\n",
    "            labels = np.argsort(yss[0])[-top_labels:]\n",
    "            ret_exp.top_labels = list(labels)\n",
    "            ret_exp.top_labels.reverse()\n",
    "        for label in labels:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score[label],\n",
    "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
    "                data, yss, distances, label, num_features,\n",
    "                model_regressor=model_regressor,\n",
    "                feature_selection=self.feature_selection)\n",
    "        return ret_exp\n",
    "\n",
    "    def __data_labels_distances(self,\n",
    "                                indexed_string,\n",
    "                                classifier_fn,\n",
    "                                num_samples,\n",
    "                                distance_metric='cosine',\n",
    "                                mode='rand'):\n",
    "        \"\"\"Generates a neighborhood around a prediction.\n",
    "        Generates neighborhood data by randomly removing words from\n",
    "        the instance, and predicting with the classifier. Uses cosine distance\n",
    "        to compute distances between original and perturbed instances.\n",
    "        Args:\n",
    "            indexed_string: document (IndexedString) to be explained,\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a string and outputs prediction probabilities. For\n",
    "                ScikitClassifier, this is classifier.predict_proba.\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for sample weighting,\n",
    "                defaults to cosine similarity.\n",
    "        Returns:\n",
    "            A tuple (data, labels, distances), where:\n",
    "                data: dense num_samples * K binary matrix, where K is the\n",
    "                    number of tokens in indexed_string. The first row is the\n",
    "                    original instance, and thus a row of ones.\n",
    "                labels: num_samples * L matrix, where L is the number of target\n",
    "                    labels\n",
    "                distances: cosine distance between the original instance and\n",
    "                    each perturbed instance (computed in the binary 'data'\n",
    "                    matrix), times 100.\n",
    "        \"\"\"\n",
    "        def distance_fn(x):\n",
    "            return sklearn.metrics.pairwise.pairwise_distances(\n",
    "                    x, x[0], metric=distance_metric).ravel() * 100\n",
    "\n",
    "        if not indexed_string.bow: \n",
    "\n",
    "            doc_size = indexed_string.num_words()\n",
    "\n",
    "            sep = indexed_string.return_sep()\n",
    "            print(\"sep: \", sep, \"docsize: \", doc_size)\n",
    "\n",
    "            global sample\n",
    "            sample = self.random_state.randint(1, doc_size + 1, num_samples - 1)\n",
    "\n",
    "            data = np.ones((num_samples, doc_size))\n",
    "            data[0] = np.ones(doc_size)\n",
    "            inverse_data = [indexed_string.raw_string()]\n",
    "            \n",
    "            if sep < doc_size:\n",
    "                print(\"separation on\")\n",
    "\n",
    "                sample_left = self.random_state.randint(1, sep + 1, num_samples - 1)\n",
    "                sample_right = self.random_state.randint(1, doc_size - sep, num_samples - 1)\n",
    "\n",
    "                features_range_left = range(0, sep)\n",
    "                features_range_right = range(sep, doc_size)\n",
    "\n",
    "                if mode == 'left':\n",
    "                    print(\"left\")\n",
    "                    sample = sample_left\n",
    "                    features_range = features_range_left\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                elif mode == 'right':\n",
    "                    print(\"right\")\n",
    "                    sample = sample_right\n",
    "                    features_range = features_range_right\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        print(i, len(features_range), size)\n",
    "                        inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                else:\n",
    "                    print('rand')\n",
    "                    dir = self.random_state.randint(2, size = num_samples - 1)\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        print(dir[i-1])\n",
    "                        if dir[i-1]:\n",
    "                            inactive = self.random_state.choice(features_range_right, sample_right[i-1],\n",
    "                                                        replace=False)\n",
    "                        else:\n",
    "                            inactive = self.random_state.choice(features_range_left, sample_left[i-1],\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "            else:\n",
    "                features_range = range(doc_size)\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "        \"\"\"\n",
    "        features_range = range(doc_size)\n",
    "        for i, size in enumerate(sample, start=1):\n",
    "            inactive = self.random_state.choice(features_range, size,\n",
    "                                                replace=False)\n",
    "            data[i, inactive] = 0\n",
    "            inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "        \"\"\"\n",
    "\n",
    "        labels = classifier_fn(inverse_data)\n",
    "        distances = distance_fn(sp.sparse.csr_matrix(data))\n",
    "        return data, labels, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIndexedString(object):\n",
    "    \"\"\"String with various indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_string, split_expression=r'\\W+', bow=True,\n",
    "                 mask_string=None):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            raw_string: string with raw text in it\n",
    "            split_expression: Regex string or callable. If regex string, will be used with re.split.\n",
    "                If callable, the function should return a list of tokens.\n",
    "            bow: if True, a word is the same everywhere in the text - i.e. we\n",
    "                 will index multiple occurrences of the same word. If False,\n",
    "                 order matters, so that the same word will have different ids\n",
    "                 according to position.\n",
    "            mask_string: If not None, replace words with this if bow=False\n",
    "                if None, default value is UNKWORDZ\n",
    "        \"\"\"\n",
    "        self.raw = raw_string\n",
    "        self.mask_string = 'UNKWORDZ' if mask_string is None else mask_string\n",
    "\n",
    "        if callable(split_expression):\n",
    "            tokens = split_expression(self.raw)\n",
    "            self.as_list = self._segment_with_tokens(self.raw, tokens)\n",
    "            tokens = set(tokens)\n",
    "\n",
    "            def non_word(string):\n",
    "                return string not in tokens\n",
    "\n",
    "        else:\n",
    "            # with the split_expression as a non-capturing group (?:), we don't need to filter out\n",
    "            # the separator character from the split results.\n",
    "            splitter = re.compile(r'(%s)|$' % split_expression)\n",
    "            self.as_list = [s for s in splitter.split(self.raw) if s]\n",
    "            non_word = splitter.match\n",
    "\n",
    "        self.as_np = np.array(self.as_list)\n",
    "        self.string_start = np.hstack(\n",
    "            ([0], np.cumsum([len(x) for x in self.as_np[:-1]])))\n",
    "        vocab = {}\n",
    "        self.inverse_vocab = []\n",
    "        self.positions = []\n",
    "        self.bow = bow\n",
    "\n",
    "        self.sep = None\n",
    "\n",
    "        non_vocab = set()\n",
    "        for i, word in enumerate(self.as_np):\n",
    "            if \"$&*&*&$\" in word:\n",
    "                self.sep = len(self.inverse_vocab)\n",
    "            if word in non_vocab:\n",
    "                continue\n",
    "            if non_word(word):\n",
    "                non_vocab.add(word)\n",
    "                continue\n",
    "            if bow:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "                    self.inverse_vocab.append(word)\n",
    "                    self.positions.append([])\n",
    "                idx_word = vocab[word]\n",
    "                self.positions[idx_word].append(i)\n",
    "            else:\n",
    "                self.inverse_vocab.append(word)\n",
    "                self.positions.append(i)\n",
    "        if not bow:\n",
    "            self.positions = np.array(self.positions)\n",
    "\n",
    "        if not self.sep:\n",
    "            self.sep = len(self.inverse_vocab)\n",
    "\n",
    "    def return_sep(self):\n",
    "        \"\"\"Return the index of the separator sequence\"\"\"\n",
    "        return self.sep\n",
    "\n",
    "    def raw_string(self):\n",
    "        \"\"\"Returns the original raw string\"\"\"\n",
    "        return self.raw\n",
    "\n",
    "    def num_words(self):\n",
    "        \"\"\"Returns the number of tokens in the vocabulary for this document.\"\"\"\n",
    "        return len(self.inverse_vocab)\n",
    "\n",
    "    def word(self, id_):\n",
    "        \"\"\"Returns the word that corresponds to id_ (int)\"\"\"\n",
    "        return self.inverse_vocab[id_]\n",
    "\n",
    "    def string_position(self, id_):\n",
    "        \"\"\"Returns a np array with indices to id_ (int) occurrences\"\"\"\n",
    "        if self.bow:\n",
    "            return self.string_start[self.positions[id_]]\n",
    "        else:\n",
    "            return self.string_start[[self.positions[id_]]]\n",
    "\n",
    "    def inverse_removing(self, words_to_remove):\n",
    "        \"\"\"Returns a string after removing the appropriate words.\n",
    "        If self.bow is false, replaces word with UNKWORDZ instead of removing\n",
    "        it.\n",
    "        Args:\n",
    "            words_to_remove: list of ids (ints) to remove\n",
    "        Returns:\n",
    "            original raw string with appropriate words removed.\n",
    "        \"\"\"\n",
    "        mask = np.ones(self.as_np.shape[0], dtype='bool')\n",
    "        mask[self.__get_idxs(words_to_remove)] = False\n",
    "        if not self.bow:\n",
    "            return ''.join(\n",
    "                [self.as_list[i] if mask[i] else self.mask_string\n",
    "                 for i in range(mask.shape[0])])\n",
    "        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n",
    "\n",
    "    @staticmethod\n",
    "    def _segment_with_tokens(text, tokens):\n",
    "        \"\"\"Segment a string around the tokens created by a passed-in tokenizer\"\"\"\n",
    "        list_form = []\n",
    "        text_ptr = 0\n",
    "        for token in tokens:\n",
    "            inter_token_string = []\n",
    "            while not text[text_ptr:].startswith(token):\n",
    "                inter_token_string.append(text[text_ptr])\n",
    "                text_ptr += 1\n",
    "                if text_ptr >= len(text):\n",
    "                    raise ValueError(\"Tokenization produced tokens that do not belong in string!\")\n",
    "            text_ptr += len(token)\n",
    "            if inter_token_string:\n",
    "                list_form.append(''.join(inter_token_string))\n",
    "            list_form.append(token)\n",
    "        if text_ptr < len(text):\n",
    "            list_form.append(text[text_ptr:])\n",
    "        return list_form\n",
    "\n",
    "    def __get_idxs(self, words):\n",
    "        \"\"\"Returns indexes to appropriate words.\"\"\"\n",
    "        if self.bow:\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [self.positions[z] for z in words]))\n",
    "        else:\n",
    "            return self.positions[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from lime import lime_base\n",
    "\n",
    "from lime.lime_text import TextDomainMapper, LimeTextExplainer\n",
    "\n",
    "class MyLimeTextExplainer(LimeTextExplainer):\n",
    "    \"\"\"Explains text classifiers.\n",
    "       Currently, we are using an exponential kernel on cosine distance, and\n",
    "       restricting explanations to words that are present in documents.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel_width=25,\n",
    "                 kernel=None,\n",
    "                 verbose=False,\n",
    "                 class_names=None,\n",
    "                 feature_selection='auto',\n",
    "                 split_expression=r'\\W+',\n",
    "                 bow=True,\n",
    "                 mask_string=None,\n",
    "                 random_state=None,\n",
    "                 char_level=False,\n",
    "                 mode='rand'):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "            kernel: similarity kernel that takes euclidean distances and kernel\n",
    "                width as input and outputs weights in (0,1). If None, defaults to\n",
    "                an exponential kernel.\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            class_names: list of class names, ordered according to whatever the\n",
    "                classifier is using. If not present, class names will be '0',\n",
    "                '1', ...\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            split_expression: Regex string or callable. If regex string, will be used with re.split.\n",
    "                If callable, the function should return a list of tokens.\n",
    "            bow: if True (bag of words), will perturb input data by removing\n",
    "                all occurrences of individual words or characters.\n",
    "                Explanations will be in terms of these words. Otherwise, will\n",
    "                explain in terms of word-positions, so that a word may be\n",
    "                important the first time it appears and unimportant the second.\n",
    "                Only set to false if the classifier uses word order in some way\n",
    "                (bigrams, etc), or if you set char_level=True.\n",
    "            mask_string: String used to mask tokens or characters if bow=False\n",
    "                if None, will be 'UNKWORDZ' if char_level=False, chr(0)\n",
    "                otherwise.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "            char_level: an boolean identifying that we treat each character\n",
    "                as an independent occurence in the string\n",
    "        \"\"\"\n",
    "\n",
    "        if kernel is None:\n",
    "            def kernel(d, kernel_width):\n",
    "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
    "\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.base = lime_base.LimeBase(kernel_fn, verbose,\n",
    "                                       random_state=self.random_state)\n",
    "        self.class_names = class_names\n",
    "        self.vocabulary = None\n",
    "        self.feature_selection = feature_selection\n",
    "        self.bow = bow\n",
    "        self.mask_string = mask_string\n",
    "        self.split_expression = split_expression\n",
    "        self.char_level = char_level\n",
    "        self.mode = mode\n",
    "\n",
    "    def explain_instance(self,\n",
    "                         text_instance,\n",
    "                         classifier_fn,\n",
    "                         labels=(1,),\n",
    "                         top_labels=None,\n",
    "                         num_features=10,\n",
    "                         num_samples=5000,\n",
    "                         distance_metric='cosine',\n",
    "                         model_regressor=None):\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "        First, we generate neighborhood data by randomly hiding features from\n",
    "        the instance (see __data_labels_distance_mapping). We then learn\n",
    "        locally weighted linear models on this neighborhood data to explain\n",
    "        each of the classes in an interpretable way (see lime_base.py).\n",
    "        Args:\n",
    "            text_instance: raw text string to be explained.\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a list of d strings and outputs a (d, k) numpy array with\n",
    "                prediction probabilities, where k is the number of classes.\n",
    "                For ScikitClassifiers , this is classifier.predict_proba.\n",
    "            labels: iterable with labels to be explained.\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for sample weighting,\n",
    "                defaults to cosine similarity\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "            to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
    "            and 'sample_weight' as a parameter to model_regressor.fit()\n",
    "        Returns:\n",
    "            An Explanation object (see explanation.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        indexed_string = (IndexedCharacters(\n",
    "            text_instance, bow=self.bow, mask_string=self.mask_string)\n",
    "                          if self.char_level else\n",
    "                          MyIndexedString(text_instance, bow=self.bow,\n",
    "                                        split_expression=self.split_expression,\n",
    "                                        mask_string=self.mask_string))\n",
    "        domain_mapper = TextDomainMapper(indexed_string)\n",
    "        data, yss, distances = self.__data_labels_distances(\n",
    "            indexed_string, classifier_fn, num_samples,\n",
    "            distance_metric=distance_metric, mode = self.mode)\n",
    "        if self.class_names is None:\n",
    "            self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "        #ret_exp = explanation.Explanation(domain_mapper=domain_mapper,\n",
    "        ret_exp = MyExplanation(domain_mapper=domain_mapper,\n",
    "                                          class_names=self.class_names,\n",
    "                                          random_state=self.random_state)\n",
    "        ret_exp.predict_proba = yss[0]\n",
    "        if top_labels:\n",
    "            labels = np.argsort(yss[0])[-top_labels:]\n",
    "            ret_exp.top_labels = list(labels)\n",
    "            ret_exp.top_labels.reverse()\n",
    "        for label in labels:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score[label],\n",
    "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
    "                data, yss, distances, label, num_features,\n",
    "                model_regressor=model_regressor,\n",
    "                feature_selection=self.feature_selection)\n",
    "        return ret_exp\n",
    "\n",
    "    def __data_labels_distances(self,\n",
    "                                indexed_string,\n",
    "                                classifier_fn,\n",
    "                                num_samples,\n",
    "                                distance_metric='cosine',\n",
    "                                mode='rand'):\n",
    "        \"\"\"Generates a neighborhood around a prediction.\n",
    "        Generates neighborhood data by randomly removing words from\n",
    "        the instance, and predicting with the classifier. Uses cosine distance\n",
    "        to compute distances between original and perturbed instances.\n",
    "        Args:\n",
    "            indexed_string: document (IndexedString) to be explained,\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a string and outputs prediction probabilities. For\n",
    "                ScikitClassifier, this is classifier.predict_proba.\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for sample weighting,\n",
    "                defaults to cosine similarity.\n",
    "        Returns:\n",
    "            A tuple (data, labels, distances), where:\n",
    "                data: dense num_samples * K binary matrix, where K is the\n",
    "                    number of tokens in indexed_string. The first row is the\n",
    "                    original instance, and thus a row of ones.\n",
    "                labels: num_samples * L matrix, where L is the number of target\n",
    "                    labels\n",
    "                distances: cosine distance between the original instance and\n",
    "                    each perturbed instance (computed in the binary 'data'\n",
    "                    matrix), times 100.\n",
    "        \"\"\"\n",
    "        def distance_fn(x):\n",
    "            return sklearn.metrics.pairwise.pairwise_distances(\n",
    "                    x, x[0], metric=distance_metric).ravel() * 100\n",
    "\n",
    "        if not indexed_string.bow: \n",
    "\n",
    "            doc_size = indexed_string.num_words()\n",
    "\n",
    "            sep = indexed_string.return_sep()\n",
    "            print(\"sep: \", sep, \"docsize: \", doc_size)\n",
    "\n",
    "            global sample\n",
    "            sample = self.random_state.randint(1, doc_size + 1, num_samples - 1)\n",
    "\n",
    "            data = np.ones((num_samples, doc_size))\n",
    "            data[0] = np.ones(doc_size)\n",
    "            inverse_data = [indexed_string.raw_string()]\n",
    "            \n",
    "            if sep < doc_size:\n",
    "                print(\"separation on\")\n",
    "\n",
    "                sample_left = self.random_state.randint(1, sep + 1, num_samples - 1)\n",
    "                sample_right = self.random_state.randint(1, doc_size - sep, num_samples - 1)\n",
    "\n",
    "                features_range_left = range(0, sep)\n",
    "                features_range_right = range(sep, doc_size)\n",
    "\n",
    "                if mode == 'left':\n",
    "                    print(\"left\")\n",
    "                    sample = sample_left\n",
    "                    features_range = features_range_left\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                elif mode == 'right':\n",
    "                    print(\"right\")\n",
    "                    sample = sample_right\n",
    "                    features_range = features_range_right\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        print(i, len(features_range), size)\n",
    "                        inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                else:\n",
    "                    print('rand')\n",
    "                    dir = self.random_state.randint(2, size = num_samples - 1)\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        #print(dir[i-1])\n",
    "                        if dir[i-1]:\n",
    "                            inactive = self.random_state.choice(features_range_right, sample_right[i-1],\n",
    "                                                        replace=False)\n",
    "                        else:\n",
    "                            inactive = self.random_state.choice(features_range_left, sample_left[i-1],\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "            else:\n",
    "                features_range = range(doc_size)\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "        \"\"\"\n",
    "        features_range = range(doc_size)\n",
    "        for i, size in enumerate(sample, start=1):\n",
    "            inactive = self.random_state.choice(features_range, size,\n",
    "                                                replace=False)\n",
    "            data[i, inactive] = 0\n",
    "            inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "        \"\"\"\n",
    "\n",
    "        labels = classifier_fn(inverse_data)\n",
    "        distances = distance_fn(sp.sparse.csr_matrix(data))\n",
    "        return data, labels, distances"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b2b79d7c925dc537986e7099a9668dba419b885563f1cfe4e7cfa1327f89933"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
