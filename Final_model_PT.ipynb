{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT embeddings for the classifier input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final output(45000)\n",
    "\n",
    "train_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\45000_epoch_5\\\\final_train_outputs_reshaped.pt\")\n",
    "test_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\45000_epoch_5\\\\final_eval_outputs_reshaped.pt\")\n",
    "\n",
    "train_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\45000_epoch_5\\\\final_train_output_labels.pt\")\n",
    "test_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\45000_epoch_5\\\\final_eval_output_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final output 512 (45000)\n",
    "\n",
    "train_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_1_train_outputs_reshaped.pt\")\n",
    "test_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_1_eval_outputs_reshaped.pt\")\n",
    "\n",
    "train_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_train_output_labels.pt\")\n",
    "test_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_eval_output_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final output 512 (45000)\n",
    "\n",
    "train_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_train_outputs_logits_reshaped.pt\")\n",
    "test_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_eval_outputs_logits_reshaped.pt\")\n",
    "\n",
    "train_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_train_output_labels.pt\")\n",
    "test_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\512_eval_output_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from training on 42601 items with more options: we have unsqueezed labels and large validation set\n",
    "\n",
    "train_features = torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Embeddings\\42601_new\\new_train_outputs_reshaped.pt\")\n",
    "test_features = torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Embeddings\\42601_new\\new_eval_outputs_reshaped.pt\")\n",
    "\n",
    "train_labels = torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Embeddings\\42601_new\\new_train_output_labels.pt\")\n",
    "test_labels = torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Embeddings\\42601_new\\new_eval_output_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\new_validation_outputs_reshaped.pt\")\n",
    "#val_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\new_validation_outputs_logits_reshaped.pt\")\n",
    "val_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\new_validation_output_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_val_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\new_large_validation_outputs_reshaped.pt\")\n",
    "#large_val_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\new_large_validation_outputs_logits_reshaped.pt\")\n",
    "large_val_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\new_large_validation_output_labels.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For using per-sequence input, flatten the features and load flattened labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.load(\"C:\\\\Users\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\Unsqweezed labels\\\\unsqueezed_train_output_labels.pt\")\n",
    "test_labels = torch.load(\"C:\\\\Users\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\Unsqweezed labels\\\\unsqueezed_eval_output_labels.pt\")\n",
    "#val_labels = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\unsqueezed_validation_output_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.FloatTensor(train_labels).to(device)\n",
    "test_labels = torch.FloatTensor(test_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to verify using the same data\n",
    "\n",
    "str(test_labels[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.reshape(train_features, (1278180, 768))\n",
    "test_features = torch.reshape(test_features, (142020, 768))\n",
    "#val_features = torch.reshape(val_features, (157830, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.reshape(train_features, (1350000, 768))\n",
    "test_features = torch.reshape(test_features, (228030, 768))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load untuned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.load(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\Untuned\\\\final_train_outputs_reshaped.pt\")\n",
    "#train_labels = torch.load(\"C:\\\\Users\\ivank\\\\Documents\\\\BERT_projects\\\\Embeddings\\\\Unsqweezed labels\\\\unsqueezed_train_output_labels.pt\")\n",
    "\n",
    "#for untuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.reshape(train_features, (1350000, 768))\n",
    "#train_labels = train_labels.to(device)\n",
    "\n",
    "#for untuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For training a model without pooling, apply averaging to features. Labels don't need to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.mean(train_features, axis = 1)\n",
    "test_features = torch.mean(test_features, axis = 1)\n",
    "#val_features = torch.mean(val_features, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze input embeddings: see the difference by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = torch.cat((train_features, train_labels.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_all.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To analyze test features weights\"\"\"\n",
    "\n",
    "#train_all = torch.cat((test_features, test_labels.reshape(-1,1)), axis=1)\n",
    "#train_all = train_all.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_dataframe = pd.DataFrame(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe_neg = train_dataframe[train_dataframe[768] == 0.0]\n",
    "train_dataframe_pos = train_dataframe[train_dataframe[768] == 1.0]\n",
    "train_dataframe_neg = train_dataframe_neg.drop(columns=[768])\n",
    "train_dataframe_pos = train_dataframe_pos.drop(columns=[768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mean = train_dataframe_pos.mean(axis = 0)\n",
    "neg_mean = train_dataframe_neg.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_std = train_dataframe_pos.std(axis = 0)\n",
    "neg_std = train_dataframe_neg.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pos_mean - neg_mean\n",
    "absdiff = abs(diff)\n",
    "diff_sort = absdiff.sort_values(ascending=False)\n",
    "pos_sort = pos_mean.sort_values()\n",
    "neg_sort = neg_mean.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(pos_std)), pos_std, width=1, edgecolor=(0, 0, 0), color = 'lime', label = \"Feature weights STD for class 1\")\n",
    "plt.bar(range(len(pos_std)), neg_std, width=1, edgecolor=(0, 0, 0), color = 'bisque', label = \"Feature weights STD for class 0\")\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Standard deviation\")\n",
    "plt.title(\"Standard deviation of feature weights in all segments with labels '1' and '0'\")\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7, ax7 = plt.subplots()\n",
    "ax7.set_title('Averaged feature weights for untuned train, tuned train, and tuned test data (each for classes \"1\" and \"0\")')\n",
    "ax7.boxplot([pos_mean, neg_mean, pos_mean_train, neg_mean_train, pos_mean_test, neg_mean_test])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(pos_mean)), pos_mean, width=1, edgecolor=(0, 0, 0), label = \"Feature weights for class 1\")\n",
    "plt.bar(range(len(neg_mean)), neg_mean, width=1, edgecolor=(0, 0, 0), color = 'peachpuff', label = \"Feature weights for class 0\")\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Average weights of features in all segments with labels '1' and '0'\")\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(pos_mean)), pos_mean, width=1, edgecolor=(0, 0, 0), label = \"Feature weights for class 1\")\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Average weights of features in all segments with labels '1' in untuned model\")\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(neg_mean)), neg_mean, width=1, edgecolor=(0, 0, 0), color = 'peachpuff', label = \"Feature weights for class 0\")\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Average weights of features in all segments with labels '0' in untuned model\")\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(pos_sort)), pos_sort, width=1, edgecolor=(0, 0, 0))\n",
    "#plt.bar(range(len(neg_sort)), neg_sort, width=1, edgecolor=(0, 0, 0), color = 'peachpuff', label = \"Total top-feature counts for 0\")\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Average weights of features in all segments with label '1', sorted\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(diff_sort)), diff_sort, width=1, edgecolor=(0, 0, 0))\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Absolute difference in feature weights between classes, sorted\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(diff_sort)), diff_sort, width=1, edgecolor=(0, 0, 0))\n",
    "y = np.array([2, 4, 6, 8, 9, 10, 12, 14, 16])\n",
    "\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Absolute difference in feature weights between classes, sorted\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.yticks(np.arange(0, 5, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of overlapping features between most popular extracted from LIME and most salient in value difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the saliency values (absolute difference in weights between classes) from 45000 training dataset [without averaging sequences into texts]\n",
    "# and LIME explanations from PyTorch classifier on 7601 validation data (\"test data\") and 45000 training data (\"training data\"):\n",
    "\n",
    "# training data in LIME / training data in saliency after finetuning:\n",
    "# {130, 382, 517, 135, 137, 638, 653, 275, 153, 412, 416, 294, 553, 297, 47, 566, 438, 585, 715, 215, 216, 222, 478, 498, 243, 766} \n",
    "# 52.0% of features are overlapping\n",
    "\n",
    "# test data in LIME / training data in saliency after finetuning:\n",
    "# {382, 389, 137, 522, 524, 653, 275, 276, 277, 416, 553, 297, 312, 443, 455, 585, 715, 205, 598, 216, 95, 745, 498, 375, 766} 25 \n",
    "# 50.0% of features are overlapping (numbers were up to 57% for 40 most popular for each class)\n",
    "\n",
    "# trainin data in LIME / training data in saliency with untuned embeddings\n",
    "# {176, 758, 153, 222, 286} \n",
    "# 10.0% of features are overlapping\n",
    "\n",
    "# test data in LIME / training data in saliency with untuned embeddings\n",
    "# {312, 620, 277} \n",
    "# 6.0% of features are overlapping\n",
    "\n",
    "# training data in LIME / test data in saliency after finetuning:\n",
    "# {130, 382, 517, 135, 137, 638, 653, 275, 153, 412, 416, 294, 553, 297, 47, 563, 566, 438, 585, 715, 215, 216, 222, 478, 498, 243, 766} \n",
    "# 54.0% of features are overlapping\n",
    "\n",
    "# test data in LIME / test data in saliency after finetuning:\n",
    "# {382, 389, 137, 522, 524, 653, 275, 276, 277, 416, 553, 297, 312, 443, 455, 585, 715, 205, 216, 95, 745, 498, 375, 766} \n",
    "# 48.0% of features are overlapping\n",
    "\n",
    "n = 50\n",
    "\n",
    "diff_dict = diff_sort[:n].to_dict()\n",
    "\n",
    "#old, test data\n",
    "\n",
    "\"\"\"top_pos_lime_dict = {553: 51, 522: 35, 95: 33, 455: 31, 297: 27, 715: 21, 416: 18, 216: 17, 766: 14, 389: 13, 556: 13, 277: 12, 276: 10, 152: 9, 598: 9, 382: 9, 498: 8, \n",
    "137: 8, 585: 7, 289: 7, 36: 6, 524: 6, 195: 6, 85: 5, 461: 5, 746: 5, 100: 5, 559: 5, 502: 4, 384: 4, 443: 4, 205: 4, 125: 3, 138: 3, 31: 3, 438: 3, 673: 3, 653: 3, 582: 2, 607: 2,\n",
    " 724: 2, 745: 2, 127: 2, 44: 2, 754: 2, 375: 2, 554: 2, 345: 2, 532: 2, 533: 2, 381: 2, 635: 2, 579: 2, 312: 2, 292: 2, 218: 2, 437: 2, 275: 2, 751: 2, 96: 2, 566: 2, 729: 2, 620: 2, \n",
    " 227: 2, 322: 1, 563: 1, 494: 1, 158: 1, 426: 1, 298: 1, 290: 1, 704: 1, 758: 1, 288: 1, 574: 1, 485: 1, 517: 1, 244: 1, 66: 1, 87: 1, 29: 1, 13: 1, 2: 1, 698: 1, 611: 1, 108: 1,\n",
    "  113: 1, 243: 1, 173: 1, 399: 1, 454: 1, 613: 1, 240: 1, 733: 1, 341: 1, 86: 1, 222: 1, 626: 1, 471: 1, 519: 1, 77: 1, 428: 1, 667: 1, 463: 1, 712: 1, 258: 1, 175: 1, 293: 1, 752: 1, \n",
    "59: 1, 54: 1, 615: 1, 565: 1, 738: 1, 135: 1, 475: 1, 376: 1, 342: 1, 534: 1, 725: 1, 181: 1, 98: 1, 50: 1, 130: 1, 164: 1, 153: 1, 5: 1, 308: 1, 360: 1, 144: 1, 422: 1, 284: 1, 198: 1}\n",
    "\n",
    "top_neg_lime_dict = {553: 31, 455: 29, 95: 26, 522: 23, 389: 22, 297: 20, 715: 20, 416: 14, 216: 12, 382: 11, 277: 10, 556: 9, 559: 8, 766: 7, 137: 7, 312: 6, 152: 6, \n",
    "276: 6, 85: 5, 36: 5, 275: 4, 375: 4, 289: 4, 402: 4, 215: 3, 673: 3, 751: 3, 471: 3, 329: 3, 585: 3, 195: 3, 620: 3, 524: 3, 443: 3, 598: 3, 381: 2, 127: 2, 653: 2, 540: 2, 502: 2,\n",
    " 66: 2, 175: 2, 218: 2, 53: 2, 164: 2, 474: 2, 41: 2, 399: 2, 712: 2, 498: 2, 100: 2, 441: 2, 745: 2, 59: 2, 138: 2, 516: 2, 205: 2, 461: 2, 563: 1, 13: 1, 463: 1, 475: 1, 239: 1, \n",
    " 344: 1, 353: 1, 188: 1, 144: 1, 31: 1, 110: 1, 557: 1, 737: 1, 532: 1, 0: 1, 408: 1, 32: 1, 725: 1, 512: 1, 286: 1, 658: 1, 161: 1, 393: 1, 676: 1, 404: 1, 572: 1, 348: 1, 724: 1, \n",
    " 582: 1, 384: 1, 321: 1, 499: 1, 194: 1, 309: 1, 20: 1, 372: 1, 328: 1, 367: 1, 153: 1, 255: 1, 636: 1, 200: 1, 73: 1, 185: 1, 523: 1, 517: 1, 298: 1, 351: 1, 258: 1, 413: 1, 135: 1, \n",
    " 593: 1, 478: 1, 345: 1, 212: 1, 130: 1, 60: 1, 149: 1, 278: 1, 44: 1, 354: 1, 292: 1, 533: 1, 223: 1, 141: 1, 597: 1, 534: 1, 424: 1, 547: 1, 727: 1, 469: 1, 181: 1, 226: 1, 703: 1, 357: 1, 635: 1, 158: 1, 392: 1}\"\"\"\n",
    "\n",
    "#new, training data\n",
    "top_pos_lime_dict = {135: 44, 498: 32, 47: 28, 553: 22, 216: 21, 297: 17, 585: 17, 153: 16, 137: 16, 766: 14, 294: 13, 275: 12, 638: 12, 727: 11, 715: 11, 758: 10, 416: 10, 517: 8, 502: 8, 378: 8, \n",
    "653: 7, 345: 7, 130: 6, 596: 6, 222: 6, 532: 6, 412: 6, 646: 6, 608: 6, 114: 6, 127: 5, 635: 5, 382: 5, 241: 4, 215: 4, 66: 4, 295: 4, 243: 4, 401: 4, 729: 4, 566: 3, 286: 3, 438: 3, 624: 3, 265: 3, 613: 3, \n",
    "253: 3, 708: 3, 85: 3, 478: 3, 492: 3, 522: 2, 573: 2, 176: 2, 540: 2, 121: 2, 304: 2, 421: 2, 159: 2, 337: 2, 484: 2, 178: 2, 563: 2, 361: 2, 181: 2, 334: 2, 661: 2, 523: 1, 289: 1, 1: 1, 639: 1, \n",
    "544: 1, 480: 1, 166: 1, 357: 1, 156: 1, 16: 1, 651: 1, 163: 1, 761: 1, 543: 1, 597: 1, 11: 1, 524: 1, 384: 1, 161: 1, 360: 1, 175: 1, 410: 1, 724: 1, 172: 1, 106: 1, 226: 1, 701: 1, 329: 1, 691: 1, \n",
    "168: 1, 647: 1, 154: 1, 35: 1, 736: 1, 533: 1, 618: 1, 255: 1, 399: 1, 270: 1, 636: 1, 370: 1, 276: 1, 108: 1, 31: 1, 94: 1, 753: 1, 408: 1, 299: 1, 198: 1, 245: 1}\n",
    "\n",
    "top_neg_lime_dict = {135: 25, 553: 22, 498: 20, 294: 16, 297: 14, 47: 13, 608: 13, 137: 12, 766: 12, 275: 11, 758: 10, 416: 10, 585: 10, 153: 9, 216: 8, 638: 8, 540: 7, 215: 7, 378: 7, \n",
    "176: 7, 596: 6, 517: 6, 624: 6, 653: 6, 85: 6, 127: 6, 295: 6, 94: 6, 712: 5, 243: 5, 502: 5, 727: 5, 532: 5, 635: 4, 130: 4, 412: 4, 566: 4, 159: 4, 729: 4, 444: 4, 646: 4, 401: 3, 114: 3, 478: 3, \n",
    "426: 3, 438: 3, 241: 3, 563: 3, 286: 3, 222: 3, 636: 2, 715: 2, 745: 2, 337: 2, 484: 2, 265: 2, 181: 2, 724: 2, 108: 2, 528: 2, 625: 2, 676: 2, 691: 2, 31: 2, 345: 2, 615: 1, 4: 1, 370: 1, 708: 1, \n",
    "524: 1, 658: 1, 329: 1, 522: 1, 685: 1, 687: 1, 173: 1, 255: 1, 273: 1, 387: 1, 321: 1, 382: 1, 379: 1, 665: 1, 324: 1, 251: 1, 562: 1, 572: 1, 253: 1, 357: 1, 629: 1, 613: 1, 603: 1, 259: 1, 573: 1, \n",
    "556: 1, 597: 1, 258: 1, 66: 1, 10: 1, 761: 1, 58: 1, 225: 1, 178: 1, 375: 1, 460: 1, 583: 1, 399: 1, 334: 1, 516: 1, 72: 1, 592: 1, 747: 1}\n",
    "\n",
    "total_lime_dict = {k: top_pos_lime_dict.get(k, 0) + top_neg_lime_dict.get(k, 0) for k in set(top_pos_lime_dict) | set(top_neg_lime_dict)}\n",
    "total_lime_dict = dict(sorted(total_lime_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "common_features = set(diff_dict.keys()).intersection(set(list(total_lime_dict.keys())[:n]))\n",
    "#print(common_features, \"\\n{}% of features are overlapping\".format(len(common_features)/((len(diff_dict)+len(total_lime_dict))/2)*100))\n",
    "print(common_features, \"\\n{}% of features are overlapping\".format((len(common_features)/n)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize the dataloaders. Use True for training, False for getting predictions\n",
    "\"\"\"\n",
    "\n",
    "train_tensor = TensorDataset(train_features, train_labels) \n",
    "train_dataloader = DataLoader(train_tensor, shuffle=True, batch_size=30)\n",
    "\n",
    "test_tensor = TensorDataset(test_features, test_labels) \n",
    "eval_dataloader = DataLoader(test_tensor, shuffle=True, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tool for comparing the ouput of Torch average pooling and mean functions\n",
    "\"\"\"\n",
    "\n",
    "i = 0       \n",
    "for batch in train_dataloader:\n",
    "    i += 1\n",
    "    m = nn.AdaptiveAvgPool2d((1,768))\n",
    "    #input = torch.rand(64, 30, 768)\n",
    "    input = batch[0]\n",
    "    print(input.size())\n",
    "    output = m(input)\n",
    "    output = torch.flatten(output, 1)\n",
    "    print(\"tensor1\", output.size())\n",
    "    \n",
    "    output2 = torch.mean(input, axis = 1)\n",
    "    print(\"tensor2\", output2.size())\n",
    "\n",
    "    for j in range(768):\n",
    "        if not (torch.eq(output[0][j-1], output2[0][j-1])):\n",
    "            print(output[0][j-1] - output2[0][j-1])\n",
    "    if i == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple model with structure identical to FinalNet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 16)\n",
    "        self.fc2 = nn.Linear(16,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A Sequential model without pooling (use with averaged input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FinalNet, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalNetAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    A Sequential model with pooling (use with raw input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FinalNetAvg, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 768))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalNetAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    A Sequential model with pooling (use with raw input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FinalNetAvg, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    A Sequential model with pooling (use with raw input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(TwoAvg, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JustAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    A Sequential model with pooling (use with raw input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(JustAvg, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JustAvg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoAvg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalNetAvg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(torch.unsqueeze(test_features[0], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.5f}, Validation Loss: {:.5f}, accuracy = {:.5f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, torch.nn.CrossEntropyLoss(), train_dataloader, eval_dataloader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, torch.nn.CrossEntropyLoss(), train_dataloader, eval_dataloader, epochs=10, device=device)\n",
    "#512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, torch.nn.CrossEntropyLoss(), train_dataloader, eval_dataloader, epochs=10, device=device)\n",
    "#512-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, torch.nn.CrossEntropyLoss(), train_dataloader, eval_dataloader, epochs=10, device=device)\n",
    "#512-logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, torch.nn.CrossEntropyLoss(), train_dataloader, eval_dataloader, epochs=2, device=device)\n",
    "#512-logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "def eval(model, loss_fn, val_loader, device=\"cpu\"):\n",
    "\n",
    "    metric1 = load_metric(\"accuracy\")\n",
    "    metric2 = load_metric(\"f1\")\n",
    "\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(logits,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            metric1.add_batch(predictions=predictions, references=targets)\n",
    "            metric2.add_batch(predictions=predictions, references=targets)\n",
    "\n",
    "            correct = torch.eq(torch.max(F.softmax(logits, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "    valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "    return(metric1.compute(), metric2.compute(), valid_loss, num_correct / num_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, torch.nn.CrossEntropyLoss(), eval_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, torch.nn.CrossEntropyLoss(), eval_dataloader, device=device)\n",
    "#512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, torch.nn.CrossEntropyLoss(), eval_dataloader, device=device)\n",
    "#512 logits JustAvg Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, torch.nn.CrossEntropyLoss(), eval_dataloader, device=device)\n",
    "#512 logits JustAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, torch.nn.CrossEntropyLoss(), eval_dataloader, device=device)\n",
    "#512 logits no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.state_dict()['classifier.0.weight']\n",
    "\n",
    "#-0.1429, -0.36, -0.128, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Classifier\\model_simple.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalNetAvg()\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Classifier\\model_45000.pth\"))\n",
    "model.to(device)\n",
    "#eval(model, torch.nn.CrossEntropyLoss(), eval_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Classifier\\model_simple.pth\"))\n",
    "model.to(device)\n",
    "eval(model, torch.nn.CrossEntropyLoss(), eval_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.state_dict()['fc1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.state_dict()['classifier.2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = weight.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightdf = pd.DataFrame(weight.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_and_posneg = weightdf.append(pos_mean, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_and_posneg = weight_and_posneg.append(neg_mean, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_and_posneg = weight_and_posneg.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_and_posneg.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_and_posneg.corr(method='pearson').to_csv('correlation5simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(weight[0])), weight[0].cpu().numpy(), width=1, edgecolor=(0, 0, 0), label = \"Feature weights by the classifier\")\n",
    "plt.bar(range(len(weight[1])), weight[1].cpu().numpy(), width=1, edgecolor=(0, 0, 0), color = 'teal', label = \"Feature weights by the classifier\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Average weights of features at the first layer of the Classifier\")\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize the dataloaders. Use True for training, False for getting predictions\n",
    "\"\"\"\n",
    "\n",
    "train_tensor = TensorDataset(train_features, train_labels) \n",
    "train_dataloader = DataLoader(train_tensor, shuffle=False, batch_size=64)\n",
    "\n",
    "test_tensor = TensorDataset(test_features, test_labels) \n",
    "eval_dataloader = DataLoader(test_tensor, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict for items from a dataloader\n",
    "\"\"\"\n",
    "\n",
    "def dataloader_predict(dataloader):\n",
    "    predictions = torch.Tensor().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs.double())\n",
    "            targets = targets.to(device)\n",
    "            pred  = torch.argmax(logits, dim=-1)\n",
    "            predictions = torch.cat((predictions, pred), 0)\n",
    "    return predictions\n",
    "\n",
    "predictions = dataloader_predict(train_dataloader)\n",
    "\n",
    "print(predictions[:10])\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "keras.metrics.binary_accuracy(test_labels, predictions.to('cpu'), threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = keras.metrics.Precision()\n",
    "m.update_state(val_labels, predictions)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = keras.metrics.Recall()\n",
    "m.update_state(val_labels, predictions)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = keras.metrics.AUC(num_thresholds=10)\n",
    "m.update_state(test_labels, predictions)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics, backend\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    m = metrics.Recall()\n",
    "    m.update_state(y_true, y_pred)\n",
    "    recall = m.result().numpy()\n",
    "    m = metrics.Precision()\n",
    "    m.update_state(y_true, y_pred)\n",
    "    precision = m.result().numpy()\n",
    "    return 2*((precision*recall)/(precision+recall+backend.epsilon()))\n",
    "    \n",
    "f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for explaning predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This pipeline is used for LIME and thus needs numpy data for training and as input\n",
    "\"\"\"\n",
    "\n",
    "train_features = train_features.cpu().numpy() \n",
    "test_features = test_features.cpu().numpy()\n",
    "train_labels = train_labels.cpu().numpy() \n",
    "test_labels = test_labels.cpu().numpy()\n",
    "#val_features = val_features.cpu().numpy()\n",
    "#val_labels = val_labels.cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytpredict(input, model=model):\n",
    "    \"\"\"\n",
    "    Predict for a single input, accepts a model WITHOUT pooling with input being a 2d numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.double()\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        input = torch.from_numpy(input)\n",
    "        input = input.to(device)\n",
    "        logits = model(input.double())\n",
    "        for prediction in logits:\n",
    "            prediction = F.softmax(logits).cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            predictions.append(prediction)\n",
    "        return(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pytpredict(test_features[0])\n",
    "print(z, z[0] + z[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get explanations for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from __future__ import print_function\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.mean(train_features, axis = 1)\n",
    "test_features = torch.mean(test_features, axis = 1)\n",
    "#val_features = torch.mean(val_features, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If usual conversion to Numpy doesn't work, try this\n",
    "\"\"\"\n",
    "\n",
    "test_labels = numpy.array(test_labels)\n",
    "test_features = test_features.cpu().numpy()\n",
    "val_labels = numpy.array(val_labels)\n",
    "val_features = val_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data=train_features, mode='classification', training_labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(train_features[0], pytpredict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a series of explanations (Torch version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "examples = pd.read_csv('100 examples to explain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n=100\n",
    "sample_indices = random.sample(range(0, len(test_features)-1), n)\n",
    "print(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n=100\n",
    "sample_indices_train = random.sample(range(0, len(train_features)-1), n)\n",
    "print(sample_indices_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = [4433, 7582, 2457, 2291, 6345, 6737, 2662, 3839, 5039, 7304, 6388, 2813, 2869, 6150, 2665, 1756, 4589, \n",
    "6286, 4001, 1596, 4817, 2261, 7292, 2595, 128, 3533, 2443, 6652, 3820, 796, 3518, 110, 575, 4142, 1950, 1216, \n",
    "2084, 2534, 3291, 4807, 2269, 3625, 1154, 5049, 5653, 5924, 2366, 3425, 1821, 3610, 348, 2181, 6432, 7560, 6981, 5382, 3898, 2889, 4019, 80, \n",
    "5578, 3515, 3151, 4652, 1838, 2447, 2319, 763, 2963, 1914, 5210, 5609, 2609, 3915, 4370, 6654, 5096, 3640, 3634, 3073, 3092, \n",
    "794, 695, 4275, 757, 4527, 5194, 421, 2121, 649, 2306, 3796, 4805, 6340, 2930, 3963, 7422, 4290, 736, 900]\n",
    "sample_indices_train = [84, 19712, 32822, 32984, 40753, 14695, 30445, 16250, 2821, 6273, 25174, 2161, 27073, 11090, 43025, 39840, \n",
    "31322, 16112, 29403, 6409, 4967, 28014, 26647, 43307, 17783, 17906, 27331, 16215, 43685, 12797, 22501, 13549, 32395, 20889, 37749, \n",
    "37389, 25457, 24402, 29397, 34136, 28158, 30120, 24216, 17195, 20179, 44263, 21956, 4533, 7449, 26523, 1847, 41873, 18049, 1441, \n",
    "5721, 8534, 44336, 24693, 17965, 28259, 5536, 32428, 7676, 11491, 13094, 31243, 36254, 13872, 13704, 29117, 33016, 44498, 12866, \n",
    "19055, 11028, 42682, 22531, 28441, 13140, 17308, 39325, 20056, 41138, 8451, 9399, 9314, 23269, 27604, 12184, 35150, 26391, 28982, \n",
    "24946, 29393, 28172, 43355, 9124, 26595, 29999, 40449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occured_features_pos_label = []\n",
    "occured_features_neg_label = []\n",
    "occured_features_pos_pred = []\n",
    "occured_features_neg_pred = []\n",
    "occured_features_err = []\n",
    "top_features = explanation.as_list()\n",
    "stoplist = [\">\", \"<\", \".\", \"=\"]\n",
    "print(top_features)\n",
    "i = 4433\n",
    "for f in top_features:\n",
    "    print(111, f[0])\n",
    "    candidates = f[0].split()\n",
    "    print(000, candidates)\n",
    "    for c in candidates:\n",
    "        \n",
    "        if not any(x in c for x in stoplist):\n",
    "            feature = int(c)\n",
    "            print('feaure', feature)\n",
    "    if test_labels[i] == 1:\n",
    "        occured_features_pos_label.append([feature, f])\n",
    "    if test_labels[i] == 0:\n",
    "        occured_features_neg_label.append([feature, f])\n",
    "    if predictions[i] == 1:\n",
    "        occured_features_pos_pred.append([feature, f])\n",
    "    if predictions[i] == 0:\n",
    "        occured_features_neg_pred.append([feature, f])\n",
    "    if test_labels[i] != predictions[i]:\n",
    "        occured_features_err.append([feature, f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = [\">\", \"<\", \".\", \"=\"]\n",
    "\n",
    "occured_features_pos_pred = []\n",
    "occured_features_neg_pred = []\n",
    "occured_features_err = []\n",
    "occured_features_pos_pred_dict = []\n",
    "occured_features_neg_pred_dict = []\n",
    "\n",
    "for y, i in enumerate(sample_indices_train):\n",
    "    print(y)\n",
    "    explanation = explainer.explain_instance(train_features[i], pytpredict)\n",
    "    name = \"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\Explanations_train\\\\exp_{}.html\".format(i)\n",
    "    explanation.save_to_file(name)\n",
    "    top_features = explanation.as_list()\n",
    "    print(top_features)\n",
    "    for f in top_features:\n",
    "        candidates = f[0].split()\n",
    "        for c in candidates:\n",
    "            if not any(x in c for x in stoplist):\n",
    "                feature = int(c)\n",
    "                print('feaure', feature)\n",
    "        if predictions[i] == 1:\n",
    "            occured_features_pos_pred.append(feature)\n",
    "            occured_features_pos_pred_dict.append([feature, f])\n",
    "        if predictions[i] == 0:\n",
    "            occured_features_neg_pred.append(feature)\n",
    "            occured_features_neg_pred_dict.append([feature, f])\n",
    "        if train_labels[i] != predictions[i]:\n",
    "            occured_features_err.append(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(\"occured_features_pos_pred: \", Counter(occured_features_pos_pred))\n",
    "print(\"occured_features_neg_pred: \", Counter(occured_features_neg_pred))\n",
    "print(\"occured_features_err: \", Counter(occured_features_err))\n",
    "\n",
    "#combined = occured_features_pos_label + occured_features_neg_label\n",
    "#print(\"\\n\\nTotal top-features:\", len(Counter(combined)), \"/\", len(combined))\n",
    "#print(Counter(combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "occured_features_pos_pred = {553: 51, 522: 35, 95: 33, 455: 31, 297: 27, 715: 21, 416: 18, 216: 17, 766: 14, 389: 13, 556: 13, 277: 12, 276: 10, 152: 9, 598: 9, 382: 9, 498: 8, \n",
    "137: 8, 585: 7, 289: 7, 36: 6, 524: 6, 195: 6, 85: 5, 461: 5, 746: 5, 100: 5, 559: 5, 502: 4, 384: 4, 443: 4, 205: 4, 125: 3, 138: 3, 31: 3, 438: 3, 673: 3, 653: 3, 582: 2, 607: 2,\n",
    " 724: 2, 745: 2, 127: 2, 44: 2, 754: 2, 375: 2, 554: 2, 345: 2, 532: 2, 533: 2, 381: 2, 635: 2, 579: 2, 312: 2, 292: 2, 218: 2, 437: 2, 275: 2, 751: 2, 96: 2, 566: 2, 729: 2, 620: 2, \n",
    " 227: 2, 322: 1, 563: 1, 494: 1, 158: 1, 426: 1, 298: 1, 290: 1, 704: 1, 758: 1, 288: 1, 574: 1, 485: 1, 517: 1, 244: 1, 66: 1, 87: 1, 29: 1, 13: 1, 2: 1, 698: 1, 611: 1, 108: 1,\n",
    "  113: 1, 243: 1, 173: 1, 399: 1, 454: 1, 613: 1, 240: 1, 733: 1, 341: 1, 86: 1, 222: 1, 626: 1, 471: 1, 519: 1, 77: 1, 428: 1, 667: 1, 463: 1, 712: 1, 258: 1, 175: 1, 293: 1, 752: 1, \n",
    "59: 1, 54: 1, 615: 1, 565: 1, 738: 1, 135: 1, 475: 1, 376: 1, 342: 1, 534: 1, 725: 1, 181: 1, 98: 1, 50: 1, 130: 1, 164: 1, 153: 1, 5: 1, 308: 1, 360: 1, 144: 1, 422: 1, 284: 1, 198: 1}\n",
    "\n",
    "occured_features_neg_pred = {553: 31, 455: 29, 95: 26, 522: 23, 389: 22, 297: 20, 715: 20, 416: 14, 216: 12, 382: 11, 277: 10, 556: 9, 559: 8, 766: 7, 137: 7, 312: 6, 152: 6, \n",
    "276: 6, 85: 5, 36: 5, 275: 4, 375: 4, 289: 4, 402: 4, 215: 3, 673: 3, 751: 3, 471: 3, 329: 3, 585: 3, 195: 3, 620: 3, 524: 3, 443: 3, 598: 3, 381: 2, 127: 2, 653: 2, 540: 2, 502: 2,\n",
    " 66: 2, 175: 2, 218: 2, 53: 2, 164: 2, 474: 2, 41: 2, 399: 2, 712: 2, 498: 2, 100: 2, 441: 2, 745: 2, 59: 2, 138: 2, 516: 2, 205: 2, 461: 2, 563: 1, 13: 1, 463: 1, 475: 1, 239: 1, \n",
    " 344: 1, 353: 1, 188: 1, 144: 1, 31: 1, 110: 1, 557: 1, 737: 1, 532: 1, 0: 1, 408: 1, 32: 1, 725: 1, 512: 1, 286: 1, 658: 1, 161: 1, 393: 1, 676: 1, 404: 1, 572: 1, 348: 1, 724: 1, \n",
    " 582: 1, 384: 1, 321: 1, 499: 1, 194: 1, 309: 1, 20: 1, 372: 1, 328: 1, 367: 1, 153: 1, 255: 1, 636: 1, 200: 1, 73: 1, 185: 1, 523: 1, 517: 1, 298: 1, 351: 1, 258: 1, 413: 1, 135: 1, \n",
    " 593: 1, 478: 1, 345: 1, 212: 1, 130: 1, 60: 1, 149: 1, 278: 1, 44: 1, 354: 1, 292: 1, 533: 1, 223: 1, 141: 1, 597: 1, 534: 1, 424: 1, 547: 1, 727: 1, 469: 1, 181: 1, 226: 1, 703: 1, 357: 1, 635: 1, 158: 1, 392: 1}\n",
    "\n",
    "\n",
    "#train\n",
    "\"\"\"occured_features_pos_pred = {135: 44, 498: 32, 47: 28, 553: 22, 216: 21, 297: 17, 585: 17, 153: 16, 137: 16, 766: 14, 294: 13, 275: 12, 638: 12, 727: 11, 715: 11, 758: 10, 416: 10, 517: 8, 502: 8, 378: 8, \n",
    "653: 7, 345: 7, 130: 6, 596: 6, 222: 6, 532: 6, 412: 6, 646: 6, 608: 6, 114: 6, 127: 5, 635: 5, 382: 5, 241: 4, 215: 4, 66: 4, 295: 4, 243: 4, 401: 4, 729: 4, 566: 3, 286: 3, 438: 3, 624: 3, 265: 3, 613: 3, \n",
    "253: 3, 708: 3, 85: 3, 478: 3, 492: 3, 522: 2, 573: 2, 176: 2, 540: 2, 121: 2, 304: 2, 421: 2, 159: 2, 337: 2, 484: 2, 178: 2, 563: 2, 361: 2, 181: 2, 334: 2, 661: 2, 523: 1, 289: 1, 1: 1, 639: 1, \n",
    "544: 1, 480: 1, 166: 1, 357: 1, 156: 1, 16: 1, 651: 1, 163: 1, 761: 1, 543: 1, 597: 1, 11: 1, 524: 1, 384: 1, 161: 1, 360: 1, 175: 1, 410: 1, 724: 1, 172: 1, 106: 1, 226: 1, 701: 1, 329: 1, 691: 1, \n",
    "168: 1, 647: 1, 154: 1, 35: 1, 736: 1, 533: 1, 618: 1, 255: 1, 399: 1, 270: 1, 636: 1, 370: 1, 276: 1, 108: 1, 31: 1, 94: 1, 753: 1, 408: 1, 299: 1, 198: 1, 245: 1}\n",
    "\n",
    "occured_features_neg_pred = {135: 25, 553: 22, 498: 20, 294: 16, 297: 14, 47: 13, 608: 13, 137: 12, 766: 12, 275: 11, 758: 10, 416: 10, 585: 10, 153: 9, 216: 8, 638: 8, 540: 7, 215: 7, 378: 7, \n",
    "176: 7, 596: 6, 517: 6, 624: 6, 653: 6, 85: 6, 127: 6, 295: 6, 94: 6, 712: 5, 243: 5, 502: 5, 727: 5, 532: 5, 635: 4, 130: 4, 412: 4, 566: 4, 159: 4, 729: 4, 444: 4, 646: 4, 401: 3, 114: 3, 478: 3, \n",
    "426: 3, 438: 3, 241: 3, 563: 3, 286: 3, 222: 3, 636: 2, 715: 2, 745: 2, 337: 2, 484: 2, 265: 2, 181: 2, 724: 2, 108: 2, 528: 2, 625: 2, 676: 2, 691: 2, 31: 2, 345: 2, 615: 1, 4: 1, 370: 1, 708: 1, \n",
    "524: 1, 658: 1, 329: 1, 522: 1, 685: 1, 687: 1, 173: 1, 255: 1, 273: 1, 387: 1, 321: 1, 382: 1, 379: 1, 665: 1, 324: 1, 251: 1, 562: 1, 572: 1, 253: 1, 357: 1, 629: 1, 613: 1, 603: 1, 259: 1, 573: 1, \n",
    "556: 1, 597: 1, 258: 1, 66: 1, 10: 1, 761: 1, 58: 1, 225: 1, 178: 1, 375: 1, 460: 1, 583: 1, 399: 1, 334: 1, 516: 1, 72: 1, 592: 1, 747: 1}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing from dictionaries\n",
    "\n",
    "from collections import Counter\n",
    "combined = Counter(occured_features_pos_pred) + Counter(occured_features_neg_pred)\n",
    "#pos_counter, neg_counter = occured_features_pos_pred, occured_features_neg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if initializing directly after obtaining LIME statistics\n",
    "\n",
    "combined_counter = dict(Counter(combined))\n",
    "pos_counter = dict(Counter(occured_features_pos_pred))\n",
    "neg_counter = dict(Counter(occured_features_neg_pred))\n",
    "#err_counter = dict(Counter(occured_features_err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_counter = dict(sorted(combined_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "pos_counter = dict(sorted(pos_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "neg_counter = dict(sorted(neg_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "#err_counter = dict(sorted(err_counter.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_counter.keys()\n",
    "top_combined_counter = [f for f in combined_counter.keys() if combined_counter[f] > 10]\n",
    "top_pos_counter = [f for f in pos_counter.keys() if pos_counter[f] > 10]\n",
    "top_neg_counter = [f for f in neg_counter.keys() if neg_counter[f] > 10]\n",
    "#top_err_counter = [f for f in err_counter.keys() if err_counter[f] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg = set(top_pos_counter).intersection(set(top_neg_counter)) #which features among most popular are common among positive and negative\n",
    "pos_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Torch model data\n",
    "\n",
    "pos_neg = set(top_pos_counter).intersection(set(top_neg_counter)) #which features among most popular are common among positive and negative\n",
    "pos_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_err = set(top_pos_counter).intersection(set(top_err_counter))\n",
    "neg_err = set(top_neg_counter).intersection(set(top_err_counter))\n",
    "print(pos_err, neg_err)\n",
    "print(pos_err.intersection(neg_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_err = set(top_combined_counter).intersection(set(top_err_counter))\n",
    "print(comb_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counter[378]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the explanations statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "combined_counter_masked1 = combined_counter.copy()\n",
    "for key, value in combined_counter_masked1.items():\n",
    "    if key in pos_counter.keys():\n",
    "        combined_counter_masked1[key] = pos_counter[key]\n",
    "    else:\n",
    "        combined_counter_masked1[key] = 0\n",
    "\n",
    "combined_counter_masked2 = combined_counter.copy()\n",
    "for key, value in combined_counter_masked2.items():\n",
    "    if key in set(top_pos_counter):\n",
    "        combined_counter_masked2[key] = pos_counter[key]\n",
    "    else:\n",
    "        combined_counter_masked2[key] = 0\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(range(len(combined_counter.keys())), combined_counter.values(), width=1, edgecolor=(0, 0, 0), label = \"Total top-feature counts\")\n",
    "plt.bar(range(len(combined_counter_masked1.keys())), combined_counter_masked1.values(), width=1, edgecolor=(0, 0, 0), color = 'peachpuff', label = \"Top-feature counts for predicted label \\\"1\\\"\")\n",
    "plt.bar(range(len(combined_counter_masked2.keys())), combined_counter_masked2.values(), width=1, edgecolor=(0, 0, 0), color = 'coral', label = \"Top-features with > 10 counts for predicted label \\\"1\\\"\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Counts in explanations\")\n",
    "plt.title(\"Counts of top-10 features in 100 explanations + top features for label \\\"1\\\"\")\n",
    "plt.xticks(range(20), list(combined_counter.keys())[:20], rotation=90, size=7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\"\"\"\n",
    "combined_counter_masked = combined_counter.copy()\n",
    "for key, value in combined_counter_masked.items():\n",
    "    if key in set(top_neg_counter):\n",
    "        combined_counter_masked[key] = neg_counter[key]\n",
    "    else:\n",
    "        combined_counter_masked[key] = 0\n",
    "\"\"\"\n",
    "\n",
    "combined_counter_masked1 = combined_counter.copy()\n",
    "for key, value in combined_counter_masked1.items():\n",
    "    if key in neg_counter.keys():\n",
    "        combined_counter_masked1[key] = neg_counter[key]\n",
    "    else:\n",
    "        combined_counter_masked1[key] = 0\n",
    "\n",
    "combined_counter_masked2 = combined_counter.copy()\n",
    "for key, value in combined_counter_masked2.items():\n",
    "    if key in set(top_neg_counter):\n",
    "        combined_counter_masked2[key] = neg_counter[key]\n",
    "    else:\n",
    "        combined_counter_masked2[key] = 0\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(range(len(combined_counter.keys())), combined_counter.values(), width=1, edgecolor=(0, 0, 0))\n",
    "plt.bar(range(len(combined_counter_masked1.keys())), combined_counter_masked1.values(), width=1, edgecolor=(0, 0, 0), color = 'palegreen', label = \"Top-feature counts for predicted label \\\"0\\\"\")\n",
    "plt.bar(range(len(combined_counter_masked2.keys())), combined_counter_masked2.values(), width=1, edgecolor=(0, 0, 0), color = 'limegreen', label = \"Top-features with > 10 counts for predicted label \\\"0\\\"\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Counts in explanations\")\n",
    "plt.title(\"Counts of top-10 features in 100 explanations + top features for label \\\"0\\\"\")\n",
    "plt.xticks(range(20), list(combined_counter.keys())[:20], rotation=90, size=7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "combined_counter_masked = combined_counter.copy()\n",
    "for key, value in combined_counter_masked.items():\n",
    "    if key in comb_err:\n",
    "        combined_counter_masked[key] = err_counter[key] \n",
    "    else:\n",
    "        combined_counter_masked[key] = 0\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(range(len(combined_counter.keys())), combined_counter.values(), width=1, edgecolor=(0, 0, 0))\n",
    "plt.bar(range(len(combined_counter_masked.keys())), combined_counter_masked.values(), width=1, edgecolor=(0, 0, 0), color = 'red', label = \"10 most common top-features for erroneous classification\")\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Counts in explanations\")\n",
    "plt.title(\"Counts of top-10 features in 100 explanations + top features for mistakes\")\n",
    "plt.xticks(range(20), list(combined_counter.keys())[:20], rotation=90, size=7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(combined_counter.keys(), combined_counter.values(), width=2)\n",
    "plt.xlabel(\"Features ordered by number\")\n",
    "plt.ylabel(\"Counts in explanations\")\n",
    "plt.title(\"Counts of top-10 features in 100 explanations in raw order\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b2b79d7c925dc537986e7099a9668dba419b885563f1cfe4e7cfa1327f89933"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
