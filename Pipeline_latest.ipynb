{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# following function is adopted from bert4keras package (https://github.com/bojone/bert4keras)\n",
    "# we do not import this package to avoid compadibility issues (keras < 2.3.1 is required for this package, while a later version is already used)\n",
    "# if bert4keras package is already installed, this function can be loaded as follows:\n",
    "# from bert4keras.snippets import text_segmentate\n",
    "\n",
    "def text_segmentate(text, maxlen, seps='\\n', strips=None):\n",
    "    \"\"\"将文本按照标点符号划分为若干个短句\n",
    "    \"\"\"\n",
    "    text = text.strip().strip(strips)\n",
    "    if seps and len(text) > maxlen:\n",
    "        pieces = text.split(seps[0])\n",
    "        text, texts = '', []\n",
    "        for i, p in enumerate(pieces):\n",
    "            if text and p and len(text) + len(p) > maxlen - 1:\n",
    "                texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "                text = ''\n",
    "            if i + 1 == len(pieces):\n",
    "                text = text + p\n",
    "            else:\n",
    "                text = text + p + seps[0]\n",
    "        if text:\n",
    "            texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "        return texts\n",
    "    else:\n",
    "        return [text]\n",
    "\n",
    "\n",
    "# following function is adopted from https://github.com/Pzeyang/task-for-authorship-verification\n",
    "# a custom version tailored to our project will be added later\n",
    "\n",
    "def get_data(jsonl_dataset_path):\n",
    "    \"\"\"\n",
    "    Get data from JSONL dataset. Used in plain_pipeline and pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    with open(jsonl_dataset_path, 'r') as f:\n",
    "\n",
    "        datas = []\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            text1 = text_segmentate(data['pair'][0], maxlen=510, seps='.?!')\n",
    "            text2 = text_segmentate(data['pair'][1], maxlen=510, seps='.?!')\n",
    "            while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "            datas.append((text1, text2, str(data['id'])))\n",
    "\n",
    "        return datas\n",
    "\n",
    "# different data extractors for different types of input. See description to find in which pipeline each one should be used\n",
    "\n",
    "def get_data_from_two_textfiles(text1_path, text2_path):\n",
    "    \"\"\"\n",
    "    Get data from a two text files, one for each fragment. Used in pipeline \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from raw texts\")\n",
    "\n",
    "    datas = []\n",
    "    with open(text1_path, 'r') as text1, open(text2_path, 'r') as text2:\n",
    "        text1, text2 = text1.read(), text2.read()\n",
    "        text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "        text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "        while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "        datas.append((text1, text2))\n",
    "\n",
    "    return datas\n",
    "\n",
    "def get_data_from_single_textfile(text_path):\n",
    "    \"\"\"\n",
    "    Get data from a text file that contains two texts and a separator. Currently not used\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from single raw text file\")\n",
    "\n",
    "    datas = []\n",
    "    with open(text_path, 'r') as text:\n",
    "        text = text.read()\n",
    "        text1, text2 = text.split(\"$&*&*&$\")\n",
    "        text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "        text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "        while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "        datas.append((text1, text2))\n",
    "\n",
    "    return datas\n",
    "\n",
    "def get_data_from_combined_texts(text_or_list):\n",
    "    \"\"\"\n",
    "    Get data from raw text that contains two fragments and a separater, or from a list of texts,\n",
    "    each of them containing two fragments and a separater. Used in pipeline_onetext. The ONLY type\n",
    "    of data processor for LIME inputs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from raw text\")\n",
    "\n",
    "    datas = []\n",
    "\n",
    "    print(type(text_or_list), len(text_or_list))\n",
    "    if not isinstance(text_or_list, str):\n",
    "        for text_variant in text_or_list:\n",
    "            text1, text2 = text_variant.split(\"$&*&*&$\")\n",
    "            text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "            text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "            while len(text1) < 30 or len(text2) < 30:\n",
    "                    if len(text1) < 30:\n",
    "                        n_text1 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text1:\n",
    "                                n_text1.append(sent)\n",
    "                        text1 = n_text1\n",
    "                    elif len(text2) < 30:\n",
    "                        n_text2 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text2:\n",
    "                                n_text2.append(sent)\n",
    "                        text2 = n_text2\n",
    "            datas.append((text1, text2))\n",
    "    else:\n",
    "        text1, text2 = text_or_list.split(\"$&*&*&$\")\n",
    "        text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "        text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "        while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "        datas.append((text1, text2))\n",
    "    return datas\n",
    "\n",
    "\n",
    "global tokenizer \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_function(example):\n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    return tokenizer(example['0'][:30], example['1'][:30], truncation=True, padding='max_length', max_length=255)\n",
    "\n",
    "# used to creat text input for pipelines and explainers\n",
    "def combine_texts(index, write=False):\n",
    "    \"\"\"\n",
    "    Combine a pair of texts from dataset with a separator and turn into a single text\n",
    "    \"\"\"\n",
    "\n",
    "    text1 = orig_data['pair'][index][0]\n",
    "    text2 = orig_data['pair'][index][1]\n",
    "    text_combined = text1 + \"$&*&*&$\" + text2\n",
    "    \n",
    "    if write:\n",
    "        name = \"textcomb{}.txt\".format(index)\n",
    "        with open(name, 'w') as textcomb:\n",
    "            textcomb.write(text_combined)\n",
    "\n",
    "    return(text_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a final classifier (identical to FinalNetAvg in Final_model_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalNetAvg(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FinalNetAvg, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 768))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a standard JSONL dataset with 100 pairs and get corresponding truth labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a standard JSONL with a set number of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small.jsonl\", lines = True)\n",
    "df = df.sample(n = 1)\n",
    "df.to_json(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-one.jsonl\", orient='records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load untokenized evaluation set \n",
    "from datasets import load_from_disk\n",
    "df = load_from_disk(\"..\\\\BertAA_content\\\\Data\\\\100_examples\")\n",
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = pd.read_json(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-verysmall.jsonl\", lines = True)\n",
    "all_trues = pd.read_json(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-truth.jsonl\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = pd.merge(orig_data, all_trues, on=['id'], how='inner')\n",
    "trues['same'] = trues['same'].astype(int)\n",
    "labels = trues['same'].array\n",
    "\n",
    "#np.equal(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues.to_csv(\"100 examples to explain.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get input data in custom format (otherwise use combine_text function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a pair of texts from dataset\n",
    "\"\"\"\n",
    "\n",
    "with open(\"text3.txt\", 'w') as text1, open(\"text4.txt\", 'w') as text2:\n",
    "    text1.write(orig_data['pair'][0][0])\n",
    "    text2.write(orig_data['pair'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combine a pair of texts from files with a separator and turn into a single text\n",
    "\"\"\"\n",
    "\n",
    "with open(\"text3.txt\", 'r') as text1, open(\"text4.txt\", 'r') as text2, open(\"textcomb2.txt\", 'w') as textcomb:\n",
    "    text_combined = text1.read() + \"$&*&*&$\" + text2.read()\n",
    "    textcomb.write(text_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments_from_pd(textindex, segmentindex, sep_option=0, write=False):\n",
    "    \"\"\"\n",
    "    Combine a pair of segments from pandas dataset with a separator and turn into a single text\n",
    "    \"\"\"\n",
    "\n",
    "    text1 = df['0'][textindex][segmentindex]\n",
    "    text2 = df['1'][textindex][segmentindex]\n",
    "    sep = \"$&*&*&$\" if sep_option == 0 else \"[SEP]\"\n",
    "    text_combined = text1 + sep + text2\n",
    "    \n",
    "    if write:\n",
    "        name = \"textcomb{}_{}.txt\".format(textindex, segmentindex)\n",
    "        with open(name, 'w') as textcomb:\n",
    "            textcomb.write(text_combined)\n",
    "\n",
    "    return(text_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm00 = combine_segments_from_pd(0,0,1)\n",
    "segm00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-one.jsonl\"\n",
    "\n",
    "def plain_pipeline(data_path):\n",
    "    \"\"\"\n",
    "    Pipeline for input from a regular JSONL dataset \n",
    "    \"\"\"\n",
    "\n",
    "    segmented_data = get_data(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['0', '1', '2'])\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results2\\checkpoint-225000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            if i % 10 == 0:\n",
    "                print(\">{} processing batch {}/{}\".format(i//10*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cls = outputs.hidden_states[-1][:,0,:] # obtain last hidden layer's CLS tokens. [:,0,:] meaning: ':' for all sequences, '0' for first token in sequence, ':' for all 768 hidden layers\n",
    "            eval_outputs = torch.cat((eval_outputs, cls), 0)\n",
    "\n",
    "    eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "\n",
    "    model_classify = FinalNetAvg()\n",
    "    model_classify.load_state_dict(torch.load(r\"..\\BertAA_content\\Model\\Classifier\\model.pth\"))\n",
    "    model_classify.to(device)\n",
    "\n",
    "    logits = model_classify(eval_outputs)\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    print(\"Done!\")\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with input either from regular JSONL dataset (1 argument) or from a pair of texts (2 arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-one.jsonl\"\n",
    "\n",
    "def pipeline(data_path, data_path2=None, mode='probs'):\n",
    "    \"\"\"\n",
    "    Pipeline with input either from regular JSONL dataset (1 argument) or from a pair of texts (2 arguments)\n",
    "    \"\"\"\n",
    "\n",
    "    segmented_data = get_data_from_two_textfiles(data_path, data_path2) if data_path2 else get_data(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "    \n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results2\\checkpoint-225000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            if i % 10 == 0:\n",
    "                print(\">{} processing batch {}/{}\".format(i//10*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cls = outputs.hidden_states[-1][:,0,:] # obtain last hidden layer's CLS tokens. [:,0,:] meaning: ':' for all sequences, '0' for first token in sequence, ':' for all 768 hidden layers\n",
    "            eval_outputs = torch.cat((eval_outputs, cls), 0)\n",
    "\n",
    "    eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "\n",
    "    model_classify = FinalNetAvg()\n",
    "    model_classify.load_state_dict(torch.load(r\"..\\BertAA_content\\Model\\Classifier\\model.pth\"))\n",
    "    model_classify.to(device)\n",
    "\n",
    "    model_classify.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model_classify(eval_outputs)\n",
    "    \n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    print(\"Done!\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_from_pair = pipeline(\"text1.txt\", \"text2.txt\", mode='probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_from_jsonl = pipeline(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-one.jsonl\", mode='probs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with input from a combined text or a list of combined texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_onetext(data_path, mode='probs'):\n",
    "    \"\"\"\n",
    "    Pipeline with input from a combined text or a list of combined texts\n",
    "    \"\"\"\n",
    "\n",
    "    segmented_data = get_data_from_combined_texts(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results_45000\\checkpoint-225000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            print(batch)\n",
    "            step = 10 if (len(eval_dataloader) < 100) else 100\n",
    "            if i % step == 0:\n",
    "                print(\">{} processing item {}/{}\".format(int((i/len(eval_dataloader))*10)*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cls = outputs.hidden_states[-1][:,0,:] # obtain last hidden layer's CLS tokens. [:,0,:] meaning: ':' for all sequences, '0' for first token in sequence, ':' for all 768 hidden layers\n",
    "            eval_outputs = torch.cat((eval_outputs, cls), 0)\n",
    "\n",
    "    eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    #Save the text embedding for future analysis\n",
    "    global embedding\n",
    "    embedding = eval_outputs\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "\n",
    "    model_classify = FinalNetAvg()\n",
    "    model_classify.load_state_dict(torch.load(r\"..\\BertAA_content\\Model\\Classifier\\model.pth\"))\n",
    "    model_classify.to(device)\n",
    "\n",
    "    model_classify.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model_classify(eval_outputs)\n",
    "    \n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    print(\"Done!\")\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_combined_segments(text_or_list):\n",
    "    \"\"\"\n",
    "    Get data from raw text that contains two fragments and a separater, or from a list of texts,\n",
    "    each of them containing two fragments and a separater. Used in pipeline_onetext. The ONLY type\n",
    "    of data processor for LIME inputs\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"Getting data from raw text\")\n",
    "\n",
    "    datas = []\n",
    "\n",
    "    #print(type(text_or_list), len(text_or_list))\n",
    "    if not isinstance(text_or_list, str):\n",
    "        for text_variant in text_or_list:\n",
    "            #print(text_variant)\n",
    "            text1, text2 = text_variant.split(\"$&*&*&$\")\n",
    "            datas.append(([text1], [text2]))\n",
    "    else:\n",
    "        text1, text2 = text_or_list.split(\"$&*&*&$\")\n",
    "        datas.append(([text1], [text2]))\n",
    "    return datas\n",
    "\n",
    "def pipeline_onesegment(data_path, mode='probs'):\n",
    "    \n",
    "    segmented_data = get_data_from_combined_segments(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    #print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results_45000\\checkpoint-225000\")\n",
    "    #model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results_45000\\checkpoint-180000\")\n",
    "    model_feature_extract.to(device)\n",
    "    #print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    global eval_dataloader\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            #print(batch)\n",
    "            #step = 10 if (len(eval_dataloader) < 100) else 100\n",
    "            #if i % step == 0:\n",
    "            #    print(\">{} processing item {}/{}\".format(int((i/len(eval_dataloader))*10)*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cur_logits = outputs.logits\n",
    "            eval_outputs = torch.cat((eval_outputs, cur_logits), 0)\n",
    "\n",
    "    #eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    #Save the text embedding for future analysis\n",
    "    #global logits\n",
    "    logits = eval_outputs\n",
    "\n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    #print(\"Done!\")\n",
    "    return np.array(predictions)\n",
    "\n",
    "def get_data_from_listed_segments(text_or_list):\n",
    "    \"\"\"\n",
    "    Get data from raw text that contains two fragments and a separater, or from a list of texts,\n",
    "    each of them containing two fragments and a separater. Used in pipeline_onetext. The ONLY type\n",
    "    of data processor for LIME inputs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from raw text\")\n",
    "\n",
    "    datas = []\n",
    "\n",
    "    print(type(text_or_list), len(text_or_list))\n",
    "    if not isinstance(text_or_list[0], str):\n",
    "        for text_variant in text_or_list:\n",
    "            #print(text_variant)\n",
    "            text1, text2 = text_variant[0], text_variant[1]\n",
    "            datas.append(([text1], [text2]))\n",
    "    else:\n",
    "        text1, text2 = text_or_list[0], text_or_list[1]\n",
    "        datas.append(([text1], [text2]))\n",
    "    return datas\n",
    "\n",
    "def pipeline_twosegments(data_path, mode='probs'):\n",
    "\n",
    "    print(data_path)\n",
    "    \n",
    "    segmented_data = get_data_from_listed_segments(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results_45000\\checkpoint-225000\")\n",
    "    #model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results_45000\\checkpoint-180000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    global eval_dataloader\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            #print(batch)\n",
    "            step = 10 if (len(eval_dataloader) < 100) else 100\n",
    "            if i % step == 0:\n",
    "                print(\">{} processing item {}/{}\".format(int((i/len(eval_dataloader))*10)*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cur_logits = outputs.logits\n",
    "            eval_outputs = torch.cat((eval_outputs, cur_logits), 0)\n",
    "\n",
    "    #eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    #Save the text embedding for future analysis\n",
    "    global logits\n",
    "    logits = eval_outputs\n",
    "\n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    print(\"Done!\")\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_twosegments([segments[0][0][0], segments[0][1][0]], mode='logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_onesegment(segm00.replace(\"[SEP]\", \"$&*&*&$\"), mode='logits')\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.subtract(res[0], res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respd = pd.read_csv(\"res_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm00 = \"\"\"She convinced the guard to let her through the gates and she ran up the familiar stairs until she got to the potted bush next to the front door, \n",
    "she reached down and pulled out the spare key Joey kept there for her. She used them to unlock the door and then tossed her bag down, leaving the door open, and ran upstairs into his room. \n",
    "She then fell down on his bed, held onto a pillow and cried so hard that she didn\"t even hear anyone enter the house. \"MARY!\" Joey yelled, standing dumb-struck in the doorway.[SEP]\"Joh..n.\" \n",
    "her voice turned breathy, heat suffusing through her pores. A loud crash sounded behind them, echoing through the hall. They jerked apart and saw Marcos had accidentally dropped a cement block and created a hole in the floor while Lorna stood glaring at him. \n",
    "Then she bit out,\"Nice going laser. As usual I\"ll have to fix your mess.\" Then proceeded to maneuver a few metals plates through the hole in an attempt to mend it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results_45000\\checkpoint-225000\", output_attentions=True).eval()\n",
    "tokenizer1 = transformers.AutoTokenizer.from_pretrained(r\"..\\BertAA_content\\Model\\Checkpoints\\results_45000\\checkpoint-225000\")\n",
    "\n",
    "model2 = transformers.AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels = 2, output_attentions=True)\n",
    "tokenizer2 = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_view(segm, model, tokenizer, attention_layer):\n",
    "    model = model.eval()\n",
    "    text1, text2 = segm.split(\"$&*&*&$\")\n",
    "    inputs = tokenizer(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "\n",
    "    #sentence_a = \"The cat sat on the mat\"\n",
    "    #sentence_b = \"The cat lay on the rug\"\n",
    "    #inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    with torch.no_grad():\n",
    "        global attention   \n",
    "        attention = model(**inputs)[-1]\n",
    "    sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "    h = head_view([attention[attention_layer]], tokens, sentence_b_start, html_action='return')\n",
    "    return h\n",
    "\n",
    "def get_head_view_avg(segm, model, tokenizer):\n",
    "    model = model.eval()\n",
    "    text1, text2 = segm.split(\"$&*&*&$\")\n",
    "    inputs = tokenizer(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "\n",
    "    #sentence_a = \"The cat sat on the mat\"\n",
    "    #sentence_b = \"The cat lay on the rug\"\n",
    "    #inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    with torch.no_grad():\n",
    "        attention = model(**inputs)[-1]\n",
    "    sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "\n",
    "    attention_mean = torch.Tensor()\n",
    "    for l in attention:\n",
    "        attention_mean = torch.cat((attention_mean, l), 0)\n",
    "    attention_mean = torch.mean(attention_mean, 0, keepdim=True)\n",
    "\n",
    "    h = head_view([attention_mean], tokens, sentence_b_start, html_action='return')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text1, text2 = respd[0][0].split(\"$&*&*&$\")\n",
    "text1, text2 = respd['text'][0].split(\"$&*&*&$\")\n",
    "inputs = tokenizer1(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    global attention   \n",
    "    attention = model1(**inputs)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs['input_ids'][0][128] #SEP\n",
    "tokenizer1.decode(inputs['input_ids'][0][165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head4 = attention_mean[0][3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head1 = attention[4][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted, indices = torch.sort(head1, descending=True)\n",
    "print(indices[:10], sorted[:10])\n",
    "#for i in indices[:5]:\n",
    "#    print(tokenizer1.decode(inputs['input_ids'][0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "attention_mean = torch.Tensor()\n",
    "for l in attention:\n",
    "    attention_mean = torch.cat((attention_mean, l), 0)\n",
    "attention_mean = torch.mean(attention_mean, 0, keepdim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h00f= get_head_view(segm00, model1, tokenizer1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"h00cu.html\", \"w\") as file:\n",
    "    file.write(h.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention[11].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def attention_per_layer(attention, layer, threshold = 0.01, mode='set'):\n",
    "    \"\"\" For a given attention matrix, get the list of tokens with strongest attention from CLS\n",
    "    in a given layer in all attention heads\n",
    "    Parameters\n",
    "    threshhold : float\n",
    "        The minimal value that attention from CLS to a certain token should posess to be included into the list\n",
    "        Value around 0.02 only selects most sturdy links and in practice is mst common towards [SEP] token\n",
    "        Value around 0.01 selects most tokens with noticeably visualizable attention\n",
    "        Value around 0.15 filters the list, favouring more important tokens, yet creates a broad and meaningful list\n",
    "    \"\"\"\n",
    "    attended_by_cls = []\n",
    "\n",
    "    for headnum, head in enumerate(attention[layer][0]):\n",
    "        #print(headnum)\n",
    "        cls = head[0]\n",
    "        values, positions = torch.sort(cls, descending=True)\n",
    "        #for index, word in enumerate(positions[:10]):\n",
    "        for index, word in enumerate(positions):\n",
    "            if values[index] >= threshold:\n",
    "                attended_by_cls.append(tokenizer1.decode(inputs['input_ids'][0][word]))\n",
    "    if mode == 'set':\n",
    "        return set(attended_by_cls) #to proceed with counting jointly for all instances\n",
    "    if mode == 'counter':\n",
    "        count_attention = Counter(attended_by_cls) #count separately for each instance\n",
    "        count_attention = count_attention.most_common()\n",
    "        return count_attention\n",
    "\n",
    "def attention_general(attention, threshold = 0.01, mode='set'):\n",
    "    \"\"\" Get the list of tokens with strongest attention from CLS in all attention heads, for all layers, represented as\n",
    "    a list of per-layer lists\n",
    "    \"\"\"\n",
    "    attended_general = []\n",
    "    for layernum, layer in enumerate(attention):\n",
    "        #print(layernum)\n",
    "        attended_at_layer = attention_per_layer(attention, layernum, threshold, mode)\n",
    "        #print(attended_at_layer)\n",
    "        attended_general.append(attended_at_layer)\n",
    "    return attended_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_at_head(attention, layer, head, threshold = 0.01):\n",
    "    attended_by_cls = []\n",
    "    cls = attention[layer][0][head][0]\n",
    "    values, positions = torch.sort(cls, descending=True)\n",
    "    #print(positions)\n",
    "    for index, word in enumerate(positions):\n",
    "        #print(str(values[index]) + \": \" + tokenizer1.decode(inputs['input_ids'][0][word]))\n",
    "        if values[index] >= threshold:\n",
    "            attended_by_cls.append([tokenizer1.decode(inputs['input_ids'][0][word]), values[index], word])\n",
    "    return(attended_by_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_token(inputs, position, trackname = True):\n",
    "    curpos = position\n",
    "    startpos = position\n",
    "    endpos = position\n",
    "\n",
    "    if \"##\" in tokenizer1.decode(inputs['input_ids'][0][curpos]):\n",
    "        while \"##\" in tokenizer1.decode(inputs['input_ids'][0][curpos]):\n",
    "            curpos -= 1\n",
    "        startpos = curpos\n",
    "        curpos = position\n",
    "        while \"##\" in tokenizer1.decode(inputs['input_ids'][0][curpos]):\n",
    "            curpos += 1\n",
    "        endpos = curpos\n",
    "        \n",
    "    elif \"##\" in tokenizer1.decode(inputs['input_ids'][0][curpos + 1]):\n",
    "        curpos += 1\n",
    "        while \"##\" in tokenizer1.decode(inputs['input_ids'][0][curpos]):\n",
    "            curpos += 1\n",
    "        endpos = curpos\n",
    "\n",
    "    else:\n",
    "        endpos += 1\n",
    "\n",
    "    if trackname:\n",
    "        word = tokenizer1.decode(inputs['input_ids'][0][startpos:endpos])\n",
    "        if checkname(word):\n",
    "            return(tokenizer1.decode(inputs['input_ids'][0][startpos:endpos]))\n",
    "        else:\n",
    "            return(tokenizer1.decode(inputs['input_ids'][0][position]))\n",
    "    else:\n",
    "        return(tokenizer1.decode(inputs['input_ids'][0][startpos:endpos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkname(word):\n",
    "    return True if word[0].isupper() else False\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = reconstruct_token(inputs, 48, trackname=False)\n",
    "print(a)\n",
    "print(checkname(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(range(len(inputs['input_ids'][0]))):\n",
    "    print(index, tokenizer1.decode(inputs['input_ids'][0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = attention_at_head(attention, 10, 0)\n",
    "print(a)\n",
    "\n",
    "filtered_imptokens = []\n",
    "for imptoken in a:\n",
    "    if imptoken[0] not in ['[SEP]', '[CLS]']:\n",
    "        imptoken[0] = reconstruct_token(inputs, int(imptoken[2]), trackname = False)\n",
    "        filtered_imptokens.append(imptoken)\n",
    "print(filtered_imptokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atan = pd.read_csv(\"Attention analysis.csv\", sep=';')\n",
    "atan_from = atan.loc[atan[\"Direction\"] == \"From CLS\"]\n",
    "\n",
    "extract_attention_per_instance(attention, inputs):\n",
    "filtered_attention_per_instance = []\n",
    "imptokens_per_instance = []\n",
    "for index, head in enumerate(atan_from[\"Head\"]):\n",
    "    layer = int(atan_from.iloc[[index]][\"Layer\"])\n",
    "    #print(layer, head)\n",
    "    cur_attention_at_head = attention_at_head(attention, layer-1, head-1, threshold = 0.010)\n",
    "    filtered_attention_at_head = []\n",
    "    imptokens_at_head = []\n",
    "    for imptoken in cur_attention_at_head:\n",
    "        if imptoken[0] not in ['[SEP]', '[CLS]']:\n",
    "            imptoken[0] = reconstruct_token(inputs, int(imptoken[2]), trackname = False)\n",
    "            filtered_attention_at_head.append(imptoken)\n",
    "            imptokens_at_head.append(imptoken[0])\n",
    "    count_attention = Counter(imptokens_at_head) #count separately for each instance\n",
    "    count_attention = count_attention.most_common()\n",
    "    #print(filtered_attention_at_head)\n",
    "    #print(len(imptokens_at_head), count_attention)\n",
    "    imptokens_per_instance += imptokens_at_head\n",
    "    filtered_attention_per_instance += filtered_attention_at_head\n",
    "\n",
    "bow_attention_dict_per_instance = dict()\n",
    "for imptoken in filtered_attention_per_instance:\n",
    "    if imptoken[0] in bow_attention_dict_per_instance:\n",
    "        bow_attention_dict_per_instance[imptoken[0]] += float(imptoken[1])\n",
    "    else:\n",
    "        bow_attention_dict_per_instance[imptoken[0]] = float(imptoken[1])\n",
    "#print(Counter(imptokens_per_instance).most_common())\n",
    "#print(attention_dict_per_instance)\n",
    "\n",
    "\"\"\"attention_dict_per_instance = dict()\n",
    "for imptoken in filtered_attention_per_instance:\n",
    "    if imptoken[0] in attention_dict_per_instance:\n",
    "        attention_dict_per_instance[imptoken[0]+'@@'+str(int(imptoken[2]))] += float(imptoken[1])\n",
    "    else:\n",
    "        attention_dict_per_instance[imptoken[0]+'@@'+str(int(imptoken[2]))] = float(imptoken[1])\"\"\"\n",
    "\n",
    "apd = pd.DataFrame(bow_attention_dict_per_instance, index=[0])\n",
    "apd = apd.transpose().sort_values(by=[0], ascending = False)\n",
    "print(apd.head(10))\n",
    "print(Counter(imptokens_per_instance).most_common())\n",
    "\n",
    "acpd = pd.DataFrame(Counter(imptokens_per_instance), index=[0])\n",
    "acpd = acpd.transpose().sort_values(by=[0], ascending = False)\n",
    "print(acpd.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attended_tokens_per_instance(attention, inputs):\n",
    "    filtered_attention_per_instance = []\n",
    "    imptokens_per_instance = []\n",
    "    for index, head in enumerate(atan_from[\"Head\"]):\n",
    "        layer = int(atan_from.iloc[[index]][\"Layer\"])\n",
    "        #print(layer, head)\n",
    "        cur_attention_at_head = attention_at_head(attention, layer-1, head-1, threshold = 0.010)\n",
    "        filtered_attention_at_head = []\n",
    "        imptokens_at_head = []\n",
    "        for imptoken in cur_attention_at_head:\n",
    "            if imptoken[0] not in ['[SEP]', '[CLS]']:\n",
    "                imptoken[0] = reconstruct_token(inputs, int(imptoken[2]), trackname = False)\n",
    "                filtered_attention_at_head.append(imptoken)\n",
    "                imptokens_at_head.append(imptoken[0])\n",
    "\n",
    "        imptokens_per_instance += imptokens_at_head\n",
    "        filtered_attention_per_instance += filtered_attention_at_head\n",
    "\n",
    "    bow_attention_dict_per_instance = dict()\n",
    "    for imptoken in filtered_attention_per_instance:\n",
    "        if imptoken[0] in bow_attention_dict_per_instance:\n",
    "            bow_attention_dict_per_instance[imptoken[0]] += float(imptoken[1])\n",
    "        else:\n",
    "            bow_attention_dict_per_instance[imptoken[0]] = float(imptoken[1])\n",
    "\n",
    "    apd = pd.DataFrame(list(bow_attention_dict_per_instance.items()), columns = ['Feature','Weight'])\n",
    "    apd = apd.sort_values(by=['Weight'], ascending = False).reset_index(drop=True)\n",
    "    #print(apd.head(10))\n",
    "\n",
    "    token_counter_per_instance = Counter(imptokens_per_instance)\n",
    "    token_counter_per_instance = dict(token_counter_per_instance)\n",
    "\n",
    "    acpd = pd.DataFrame(list(token_counter_per_instance.items()), columns = ['Feature','Counts'])\n",
    "    acpd = acpd.sort_values(by=['Counts'], ascending = False).reset_index(drop=True)\n",
    "    #print(acpd.head(10))\n",
    "    return apd, acpd, imptokens_per_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attended_tokens_per_instance(attention, inputs):\n",
    "    filtered_attention_per_instance = []\n",
    "    imptokens_per_instance = []\n",
    "    for index, head in enumerate(atan_from[\"Head\"]):\n",
    "        layer = int(atan_from.iloc[[index]][\"Layer\"])\n",
    "        #print(layer, head)\n",
    "        cur_attention_at_head = attention_at_head(attention, layer-1, head-1, threshold = 0.010)\n",
    "        filtered_attention_at_head = []\n",
    "        imptokens_at_head = []\n",
    "        for imptoken in cur_attention_at_head:\n",
    "            if imptoken[0] not in ['[SEP]', '[CLS]']:\n",
    "                imptoken[0] = reconstruct_token(inputs, int(imptoken[2]), trackname = False)\n",
    "                filtered_attention_at_head.append(imptoken)\n",
    "                imptokens_at_head.append(imptoken[0])\n",
    "\n",
    "        imptokens_per_instance += imptokens_at_head\n",
    "        filtered_attention_per_instance += filtered_attention_at_head\n",
    "\n",
    "    bow_attention_dict_per_instance = dict()\n",
    "    for imptoken in filtered_attention_per_instance:\n",
    "        if imptoken[0] in bow_attention_dict_per_instance:\n",
    "            bow_attention_dict_per_instance[imptoken[0]] += float(imptoken[1])\n",
    "        else:\n",
    "            bow_attention_dict_per_instance[imptoken[0]] = float(imptoken[1])\n",
    "\n",
    "    mean_attention_dict_per_instance = dict()\n",
    "    counts_of_attention_per_instance = dict()\n",
    "    for imptoken in filtered_attention_per_instance:\n",
    "        if imptoken[0] in mean_attention_dict_per_instance:\n",
    "            mean_attention_dict_per_instance[imptoken[0]] += float(imptoken[1])\n",
    "            counts_of_attention_per_instance[imptoken[0]] += 1\n",
    "        else:\n",
    "            mean_attention_dict_per_instance[imptoken[0]] = float(imptoken[1])\n",
    "            counts_of_attention_per_instance[imptoken[0]] = 1\n",
    "\n",
    "    for key in mean_attention_dict_per_instance.keys():\n",
    "        mean_attention_dict_per_instance[key] = mean_attention_dict_per_instance[key] / counts_of_attention_per_instance[key]\n",
    "\n",
    "    apd = pd.DataFrame(list(bow_attention_dict_per_instance.items()), columns = ['Feature','Weight'])\n",
    "    apd = apd.sort_values(by=['Weight'], ascending = False).reset_index(drop=True)\n",
    "    #print(apd.head(10))\n",
    "\n",
    "    token_counter_per_instance = Counter(imptokens_per_instance)\n",
    "    token_counter_per_instance = dict(token_counter_per_instance)\n",
    "\n",
    "    acpd = pd.DataFrame(list(token_counter_per_instance.items()), columns = ['Feature','Counts'])\n",
    "    acpd = acpd.sort_values(by=['Counts'], ascending = False).reset_index(drop=True)\n",
    "    #print(acpd.head(10))\n",
    "\n",
    "    apd_mean = pd.DataFrame(list(mean_attention_dict_per_instance.items()), columns = ['Feature','Weight'])\n",
    "    apd_mean = apd_mean.sort_values(by=['Weight'], ascending = False).reset_index(drop=True)\n",
    "\n",
    "    return apd, acpd, apd_mean #imptokens_per_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = extract_attended_tokens_per_instance(attention, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_logits = pipeline_onesegment(text, mode='logits')\n",
    "\n",
    "def permute(segm, orig_logits, word):\n",
    "        \n",
    "    text1, text2 = segm.split(\"$&*&*&$\")\n",
    "    text1 = text1.replace(word, \"\")\n",
    "    text2 = text2.replace(word, \"\")\n",
    "    segm_changed = text1 + \"$&*&*&$\" + text2\n",
    "    logits_changed = pipeline_onesegment(segm_changed, mode='logits')\n",
    "    weight = np.subtract(orig_logits[0], logits_changed[0])\n",
    "\n",
    "    switch = False\n",
    "\n",
    "    if np.argmax(orig_logits[0]) != np.argmax(logits_changed[0]): \n",
    "        switch = True\n",
    "\n",
    "    weight = list(weight)\n",
    "    weight.append(switch)\n",
    "\n",
    "    return weight\n",
    "\n",
    "def permute_names(segm, orig_logits, word):\n",
    "    top_weights = []\n",
    "    top_switch = []\n",
    "    #change_importance = []\n",
    "\n",
    "    if checkname(word):\n",
    "        changes = [\"\", word.upper(), word.lower(), word + \" \" + word, \"John\", \"Mary\", \"Rinoa\", \"he\", \"she\", \"the person\", \"the Boy\"]\n",
    "\n",
    "        for change in changes:\n",
    "            segm_changed = segm.replace(word, change)\n",
    "            logits_changed = pipeline_onesegment(segm_changed, mode='logits')\n",
    "            #print(orig_logits[0], type(orig_logits[0]), logits_changed[0], type(logits_changed[0]))\n",
    "            weight = np.mean(np.abs(np.subtract(orig_logits[0], logits_changed[0])))\n",
    "\n",
    "            switch = False\n",
    "\n",
    "            #print(np.argmax(orig_logits[0]), np.argmax(logits_changed[0]))\n",
    "            if np.argmax(orig_logits[0]) != np.argmax(logits_changed[0]): \n",
    "                switch = True\n",
    "\n",
    "            #weight = [weight]\n",
    "            #weight.append(change)\n",
    "            top_weights.append(weight)\n",
    "            top_switch.append(switch)\n",
    "            #change_importance.append(weight[0])\n",
    "    return top_weights, top_switch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = [4433, 7582, 2457, 2291, 6345, 6737, 2662, 3839, 5039, 7304, 6388, 2813, 2869, 6150, 2665, 1756, 4589, \n",
    "6286, 4001, 1596]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = [4433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_importance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_for_names = [7, 12, 13, 14, 15, 16, 17, 22, 24, 25, 27, 28, 29, 34, 35, 39, 40, 42, 43, 48, 49, 53, 54, 55, 56, 57, 62, 63, 64, 66, 73, 78, 79, 83, 87, 88, 89, 92, \n",
    "95, 96, 97, 99, 100, 102, 103, 104, 107, 108, 109, 112, 113, 114, 115, 117, 120, 122, 123, 126, 127, 128, 131, 132, 133, 138, 140, 143, 153,  \n",
    "156, 159, 160, 161, 162, 168, 175, 176, 185, 189, 193, 194, 195, 205, 208, 218, 225, 240, 241, 245, 246, 247, 252, 253, 254, 255, 256, \n",
    "257, 263, 264, 265, 266, 270, 271, 272, 273, 274, 280, 281, 283, 284, 286, 287, 288, 292, 293, 297, 299, 300, 301, 302, 303, 304, 307, 312, 313, \n",
    "315, 316, 318, 359, 360, 361, 365, 370, 371, 374, 378, 387, 390, 392, 393, 399, 400, 404, 415, 417, 419, 420, 425, 426, 429, 432, 433, 434, 437, \n",
    "442, 443, 446, 447, 450, 453, 456, 457, 458, 461, 464, 465, 468, 471, 473, 477, 478, 481, 485, 489, 490, 492, 497, 499, 502, 503, \n",
    "504, 505, 506, 509, 510, 511, 512, 513, 514, 515, 516, 520, 521, 525, 527, 528, 537, 544, 545, 546, 547, 562, 567, 568, 570, 577, 578,\n",
    "581, 593, 594, 595, 596, 598, 599, 601, 602, 603, 604, 606, 608, 609, 611, 612, 615, 616, 618, 619, 620, 622, 623, 626, 627, 628, 629, 630,\n",
    "631, 632, 634, 635, 636, 638, 640, 641, 649, 652, 656, 657, 658, 659, 665, 673, 674, 680, 682, 686, 687, 688, 689, 690, 693,\n",
    "701, 703, 705, 706, 708, 709, 711, 716, 717, 718, 719, 720, 721, 730, 737, 760, 770, 776, 778, 779, 780, 781, 784, 789, 790, 796]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_importance_list_filtered = []\n",
    "\n",
    "for nameindex, name in enumerate(names_importance_list):\n",
    "    if nameindex not in filter_for_names:\n",
    "        names_importance_list_filtered.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_switches_list_filtered = []\n",
    "\n",
    "for nameindex, name in enumerate(names_switches_list):\n",
    "    if nameindex not in filter_for_names:\n",
    "        names_switches_list_filtered.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesswpd_filtered = pd.DataFrame(names_switches_list_filtered, \n",
    "                        columns = ['Name','Del','UPPER','lower','dupl','john','mary','rinoa','he','she','person','boy'])\n",
    "print(namesswpd_filtered.head(10))\n",
    "for c in namesswpd_filtered.columns:\n",
    "    if c != 'Name':\n",
    "        print(c, 1 - (namesswpd_filtered[c].value_counts()[False] / len(namesswpd_filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespd_filtered = pd.DataFrame(names_importance_list_filtered, \n",
    "                        columns = ['Name','Del','UPPER','lower','dupl','john','mary','rinoa','he','she','person','boy'])\n",
    "print(namespd_filtered.head(10))\n",
    "for c in namespd_filtered.columns:\n",
    "    if c != 'Name':\n",
    "        print(c, namespd_filtered[c].mean(), namespd_filtered[c].median(), namespd_filtered[c].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig7, ax7 = plt.subplots( figsize=(15, 15))\n",
    "\n",
    "\n",
    "ax7.set_title('Intensity of prediction alteration for different changes in input names')\n",
    "for c in namespd_filtered.columns:\n",
    "\n",
    "    ax7.boxplot([namespd_filtered['Del'], namespd_filtered['UPPER'], namespd_filtered['lower'],\n",
    "    namespd_filtered['dupl'], namespd_filtered['john'], namespd_filtered['mary'], namespd_filtered['rinoa'],\n",
    "    namespd_filtered['he'], namespd_filtered['she'], namespd_filtered['person'], namespd_filtered['boy']], notch=True)\n",
    "plt.xticks([1,2,3,4,5,6,7,8,9,10,11],['Del','Upper','Lower','Duplicate','John','Mary','Rinoa','He','She','The person','The boy'])\n",
    "plt.yticks(np.arange(0, 12, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespd = pd.DataFrame(names_importance_list, \n",
    "                        columns = ['Name','Del','UPPER','lower','dupl','john','mary','rinoa','he','she','person','boy'])\n",
    "print(namespd.head(10))\n",
    "for c in namespd.columns:\n",
    "    if c != 'Name':\n",
    "        print(c, namespd[c].mean(), namespd[c].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesswpd = pd.DataFrame(names_switches_list, \n",
    "                        columns = ['Name','Del','UPPER','lower','dupl','john','mary','rinoa','he','she','person','boy'])\n",
    "print(namesswpd.head(10))\n",
    "\n",
    "for c in namesswpd.columns:\n",
    "    if c != 'Name':\n",
    "        print(c, 1 - (namesswpd[c].value_counts()[False] / len(namesswpd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamples = [0, 5, 10, 15, 20]\n",
    "#res = []\n",
    "names_importance_list = []\n",
    "names_switches_list = []\n",
    "\n",
    "atan = pd.read_csv(\"Attention analysis.csv\", sep=';')\n",
    "atan_from = atan.loc[atan[\"Direction\"] == \"From CLS\"]\n",
    "\n",
    "for count, textindex in enumerate(sample_indices):\n",
    "\n",
    "    label = df['labels'][textindex]\n",
    "    for s in subsamples:\n",
    "        #cur_res = []\n",
    "\n",
    "        correct = False\n",
    "        segm = combine_segments_from_pd(textindex, s, 0) \n",
    "        #print(segm)\n",
    "        prediction = pipeline_onesegment(segm, mode='labels')\n",
    "        prediction = prediction[0]\n",
    "        orig_logits = pipeline_onesegment(segm, mode='logits')\n",
    "        if prediction == label:\n",
    "            correct = True\n",
    "\n",
    "        #cur_res += textindex, s, segm, label, correct, orig_logits\n",
    "\n",
    "        \n",
    "        text1, text2 = segm.split(\"$&*&*&$\")\n",
    "        inputs = tokenizer1(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            attention = model1(**inputs)[-1]\n",
    "        \n",
    "        top_features_sum, top_features_count, top_features_list = extract_attended_tokens_per_instance(attention, inputs)\n",
    "\n",
    "        for index, feature in enumerate(top_features_sum['Feature'][:20]):\n",
    "\n",
    "            if checkname(feature):\n",
    "                print(feature)\n",
    "                permutation, switches = permute_names(segm, orig_logits, feature)\n",
    "\n",
    "                names_importance_list.append([feature]+permutation)\n",
    "                names_switches_list.append([feature]+switches)\n",
    "names_importance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamples = [0, 5, 10, 15, 20]\n",
    "all_features_list = []\n",
    "\n",
    "atan = pd.read_csv(\"Attention analysis.csv\", sep=';')\n",
    "atan_from = atan.loc[atan[\"Direction\"] == \"From CLS\"]\n",
    "\n",
    "for count, textindex in enumerate(sample_indices):\n",
    "\n",
    "    label = df['labels'][textindex]\n",
    "    for s in subsamples:\n",
    " \n",
    "        segm = combine_segments_from_pd(textindex, s, 0) \n",
    "        \n",
    "        text1, text2 = segm.split(\"$&*&*&$\")\n",
    "        inputs = tokenizer1(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            attention = model1(**inputs)[-1]\n",
    "        \n",
    "        top_features_sum, top_features_count, top_features_list = extract_attended_tokens_per_instance(attention, inputs)\n",
    "\n",
    "        all_features_list += top_features_list\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_att[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections1 = []\n",
    "for index, i in enumerate(res_perc): \n",
    "    cur_intersection = []\n",
    "    intersection = set(res_perc[index][6]).intersection(set(res[index][6]))\n",
    "    percentage =  len(intersection) / len(set(res[index][6]))\n",
    "    cur_intersection.append(set(res_perc[index][6]).intersection(set(res[index][6])))\n",
    "    cur_intersection.append(percentage)\n",
    "    intersections1.append(cur_intersection)\n",
    "\n",
    "intersections2 = []\n",
    "for index, i in enumerate(res_perc): \n",
    "    cur_intersection = []\n",
    "    intersection = set(res_perc[index][7]).intersection(set(res[index][6]))\n",
    "    percentage =  len(intersection) / len(set(res[index][6]))\n",
    "    cur_intersection.append(set(res_perc[index][7]).intersection(set(res[index][6])))\n",
    "    cur_intersection.append(percentage)\n",
    "    intersections2.append(cur_intersection)\n",
    "\n",
    "intersections3 = []\n",
    "for index, i in enumerate(res_perc): \n",
    "    cur_intersection = []\n",
    "    intersection = set(res_perc[index][8]).intersection(set(res[index][6]))\n",
    "    percentage =  len(intersection) / len(set(res[index][6]))\n",
    "    cur_intersection.append(set(res_perc[index][8]).intersection(set(res[index][6])))\n",
    "    cur_intersection.append(percentage)\n",
    "    intersections3.append(cur_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_pd1 = pd.DataFrame(intersections1, columns = ['Intersection','Percent'])\n",
    "intersections_pd2 = pd.DataFrame(intersections2, columns = ['Intersection','Percent'])\n",
    "intersections_pd3 = pd.DataFrame(intersections3, columns = ['Intersection','Percent'])\n",
    "mean1 = intersections_pd1['Percent'].mean()\n",
    "mean2 = intersections_pd2['Percent'].mean()\n",
    "mean3 = intersections_pd3['Percent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_stat = []\n",
    "\n",
    "for index, i in enumerate(res):\n",
    "    #logits_stat.append(res[index][5][0][0])\n",
    "    logits_stat.append(np.mean([np.abs(res[index][5][0][0]), np.abs(res[index][5][0][1])]))\n",
    "\n",
    "logits_stat_pd1 = pd.DataFrame(logits_stat, columns = ['Logits'])\n",
    "mean4 = logits_stat_pd1['Logits'].mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_pd1['Percent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIME_no_Att = []\n",
    "Att_no_LIME = []\n",
    "for index, i in enumerate(res_att): \n",
    "    cur_intersection = []\n",
    "    intersection = set(res_att[index][6]).intersection(set(res[index][6]))\n",
    "    #percentage =  len(intersection) / len(set(res[index][6]))\n",
    "    cur_intersection.append(set(res[index][6]) - intersection)\n",
    "    #cur_intersection.append(percentage)\n",
    "    LIME_no_Att.append(cur_intersection)\n",
    "    cur_intersection = []\n",
    "    cur_intersection.append(set(res_att[index][6]) - intersection)\n",
    "    Att_no_LIME.append(cur_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_pd['Percent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resattpd['mean_switch'].mean()\n",
    "\n",
    "# 0.06799999999999996 -- Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_features = pd.DataFrame(lime_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(range(len(diff_sort)), diff_sort, width=1, edgecolor=(0, 0, 0))\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Average weight\")\n",
    "plt.title(\"Absolute difference in feature weights between classes, sorted\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Counter(lime_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_features = pd.DataFrame(att_features)\n",
    "att_features.to_csv(\"att_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_features = []\n",
    "\n",
    "for i in res_perc:\n",
    "    att_features += i[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imp = []\n",
    "\n",
    "for i in res:\n",
    "    mean_imp.append(np.mean(i[8]))\n",
    "\n",
    "mean_imp_pd = pd.DataFrame(mean_imp, columns = ['Mean'])\n",
    "mean_imp[0] = mean_imp_pd['Mean'].mean()\n",
    "\n",
    "# 0.8004299402236938 -- Count\n",
    "# 0.7821930646896362 -- Sum\n",
    "# 0.588703989982605 -- Mean\n",
    "# 0.982 -- LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_imp = []\n",
    "\n",
    "for i in res:\n",
    "    max_imp.append(np.max(i[8]))\n",
    "\n",
    "    \n",
    "max_imp_pd = pd.DataFrame(max_imp, columns = ['Max'])\n",
    "#max_imp[0] = max_imp_pd['Max'].mean()\n",
    "\n",
    "#4.490159034729004 -- Count\n",
    "#4.346518039703369 -- Sum\n",
    "#3.6743555068969727 -- Mean\n",
    "#3.626 -- LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_count = 0\n",
    "for i in res_att:\n",
    "    if i[8]:\n",
    "        switch_count += 1\n",
    "switch_count\n",
    "\n",
    "# 50 -- Count\n",
    "# 49 -- Sum\n",
    "# 42 -- Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_tokens = []\n",
    "for i in res_att:\n",
    "    max_imp = max(i[9])\n",
    "    max_index = i[9].index(max_imp)\n",
    "    most_imp_tokens.append(i[6][max_index])\n",
    "\n",
    "most_imp_tokens_counter = Counter(most_imp_tokens)\n",
    "most_imp_tokens_counter = dict(most_imp_tokens_counter.most_common())\n",
    "most_imp_tokens_pd = pd.DataFrame(list(most_imp_tokens_counter.items()), columns = ['Feature','Counts'])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanterns = []\n",
    "\n",
    "for i in res_att:\n",
    "    if \"Lanterns\" in i[2]:\n",
    "        lanterns.append(i[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamples = [0, 5, 10, 15, 20]\n",
    "res_perc = []\n",
    "\n",
    "atan = pd.read_csv(r\"..\\BertAA_content\\Attention analysis.csv\", sep=';')\n",
    "atan_from = atan.loc[atan[\"Direction\"] == \"From CLS\"]\n",
    "\n",
    "for count, textindex in enumerate(sample_indices):\n",
    "\n",
    "    label = df['labels'][textindex]\n",
    "    for s in subsamples:\n",
    "        cur_res = []\n",
    "\n",
    "        correct = False\n",
    "        segm = combine_segments_from_pd(textindex, s, 0) \n",
    "        #print(segm)\n",
    "        prediction = pipeline_onesegment(segm, mode='labels')\n",
    "        prediction = prediction[0]\n",
    "        orig_logits = pipeline_onesegment(segm, mode='logits')\n",
    "        if prediction == label:\n",
    "            correct = True\n",
    "\n",
    "        cur_res += textindex, s, segm, label, correct, orig_logits\n",
    "\n",
    "        \n",
    "        text1, text2 = segm.split(\"$&*&*&$\")\n",
    "        inputs = tokenizer1(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            attention = model1(**inputs)[-1]\n",
    "        \n",
    "        #top_features_sum, top_features_count, top_features_list = extract_attended_tokens_per_instance(attention, inputs)\n",
    "        top_features_sum, top_features_count, top_features_mean = extract_attended_tokens_per_instance(attention, inputs)\n",
    "\n",
    "\n",
    "        \n",
    "        top_wordlist1 = []\n",
    "        top_wordlist2= []\n",
    "        top_wordlist3 = []\n",
    "\n",
    "        for index, feature in enumerate(top_features_sum['Feature'][:20]):\n",
    "            top_wordlist1.append(feature)\n",
    "        for index, feature in enumerate(top_features_count['Feature'][:20]):\n",
    "            top_wordlist2.append(feature)\n",
    "        for index, feature in enumerate(top_features_mean['Feature'][:20]):\n",
    "            top_wordlist3.append(feature)\n",
    "        \n",
    "\n",
    "        cur_res += top_wordlist1, top_wordlist2, top_wordlist3\n",
    "\n",
    "\n",
    "        res_perc.append(cur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamples = [0, 5, 10, 15, 20]\n",
    "res_att = []\n",
    "all_features_list = []\n",
    "\n",
    "atan = pd.read_csv(r\"..\\BertAA_content\\Attention analysis.csv\", sep=';')\n",
    "atan_from = atan.loc[atan[\"Direction\"] == \"From CLS\"]\n",
    "\n",
    "for count, textindex in enumerate(sample_indices):\n",
    "\n",
    "    label = df['labels'][textindex]\n",
    "    for s in subsamples:\n",
    "        cur_res = []\n",
    "\n",
    "        correct = False\n",
    "        segm = combine_segments_from_pd(textindex, s, 0) \n",
    "        #print(segm)\n",
    "        prediction = pipeline_onesegment(segm, mode='labels')\n",
    "        prediction = prediction[0]\n",
    "        orig_logits = pipeline_onesegment(segm, mode='logits')\n",
    "        if prediction == label:\n",
    "            correct = True\n",
    "\n",
    "        cur_res += textindex, s, segm, label, correct, orig_logits\n",
    "\n",
    "        \n",
    "        text1, text2 = segm.split(\"$&*&*&$\")\n",
    "        inputs = tokenizer1(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            attention = model1(**inputs)[-1]\n",
    "        \n",
    "        #top_features_sum, top_features_count, top_features_list = extract_attended_tokens_per_instance(attention, inputs)\n",
    "        top_features_sum, top_features_count, top_features_mean = extract_attended_tokens_per_instance(attention, inputs)\n",
    "\n",
    "        top_wordlist = []\n",
    "        top_attention = []\n",
    "        top_weights = []\n",
    "        switches = []\n",
    "        mean_change = []\n",
    "        for index, feature in enumerate(top_features_sum['Feature'][:20]):\n",
    "        #for index, feature in enumerate(top_features_mean['Feature'][:20]):\n",
    "                \n",
    "            permutation = permute(segm, orig_logits, feature)\n",
    "\n",
    "            top_wordlist.append(feature)\n",
    "            #top_attention.append({feature: top_features_sum.iloc[[index]]['Weight']})\n",
    "            #top_weights.append({feature: permutation})\n",
    "            top_attention.append(top_features_sum.iloc[[index]]['Weight'])\n",
    "            top_weights.append([permutation[0:2]])\n",
    "            switches.append(permutation[2])\n",
    "            mean_change.append(np.mean([np.abs(permutation[0]), np.abs(permutation[1])]))\n",
    "            #else:\n",
    "            #    continue\n",
    "        \n",
    "        percent_switches = switches.count(True) / len(switches)\n",
    "\n",
    "        cur_res += top_wordlist, top_weights, percent_switches, mean_change\n",
    "\n",
    "        all_features_list += top_wordlist\n",
    "\n",
    "        res_att.append(cur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_count1 = Counter(all_features_list1)\n",
    "all_features_count1.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_count = Counter(all_features_list)\n",
    "all_features_count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resattpd = pd.DataFrame(res_att, columns = ['index', 'segment', 'text', 'label', 'correctness', 'logits',  'topwords', 'topwords_change', \"mean_switch\", 'mean_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resattpd.to_csv('res_att_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = permute(segm, orig_logits, feature)\n",
    "permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = np.mean([np.abs(permutation[0]), np.abs(permutation[1])])\n",
    "change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0][9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = permute(text, orig_logits, \"Sara\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in acpd.index[:10]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acpd[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = permute_names(text, orig_logits, \"Sara\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = attention_per_layer(attention, 0, threshold = 0.015, mode=\"counter\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = []\n",
    "for text in respd['text']:\n",
    "    text1, text2 = text.split(\"$&*&*&$\")\n",
    "    inputs = tokenizer1(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        attention = model1(**inputs)[-1]\n",
    "    cls_attention = attention_general(attention, threshold = 0.01, mode='counter')\n",
    "    attentions += [cls_attention]\n",
    "\n",
    "attentions_pd = pd.DataFrame(attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_attentions = []\n",
    "for text in respd['text']:\n",
    "    text1, text2 = text.split(\"$&*&*&$\")\n",
    "    inputs = tokenizer1(text1, text2, truncation=True, padding='max_length', max_length=255, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        attention = model1(**inputs)[-1]\n",
    "    cls_attention = attention_general(attention)\n",
    "    set_attentions += [cls_attention]\n",
    "\n",
    "set_attentions_pd = pd.DataFrame(set_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allattset = []\n",
    "for attset in set_attentions_pd[11]:\n",
    "    allattset += list(attset)\n",
    "\n",
    "count = Counter(allattset)\n",
    "count = count.most_common()\n",
    "count\n",
    "\n",
    "#set separately for each layer, but for all heads, for all instances in our sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attended_by_cls = []\n",
    "\n",
    "for layernum, layer in enumerate(attention):\n",
    "    #print(layernum)\n",
    "    for headnum, head in enumerate(layer[0]):\n",
    "        #print(headnum)\n",
    "        cls = head[0]\n",
    "        sorted, indices = torch.sort(cls, descending=True)\n",
    "        for word in indices[:10]:\n",
    "            attended_by_cls.append(tokenizer1.decode(inputs['input_ids'][0][word]))\n",
    "\n",
    "print(len(attended_by_cls))\n",
    "from collections import Counter\n",
    "\n",
    "count = Counter(attended_by_cls)\n",
    "count = count.most_common()\n",
    "count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[4433]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'][4433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['0'][1596]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(bow=True)\n",
    "exp = explainer.explain_instance(text_instance=segm, classifier_fn=pipeline_onesegment, num_features=10, num_samples=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for collecting explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = [4433, 7582, 2457, 2291, 6345, 6737, 2662, 3839, 5039, 7304, 6388, 2813, 2869, 6150, 2665, 1756, 4589, \n",
    "6286, 4001, 1596, #20\n",
    " 4817, 2261, 7292, 2595, 128, 3533, 2443, 6652, 3820, 796, 3518, 110, 575, 4142, 1950, 1216, 2084, 2534, 3291, 4807, 2269,\n",
    " 3625, 1154, 5049, 5653, 5924, 2366, 3425, 1821, 3610, 348, 2181, 6432, 7560, 6981, 5382, 3898, 2889, 4019, 80, \n",
    "5578, 3515, 3151, 4652, 1838, 2447, 2319, 763, 2963, 1914, 5210, 5609, 2609, 3915, 4370, 6654, 5096, 3640, 3634, 3073, 3092, \n",
    "794, 695, 4275, 757, 4527, 5194, 421, 2121, 649, 2306, 3796, 4805, 6340, 2930, 3963, 7422, 4290, 736, 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[50285]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, index in enumerate(sample_indices):\n",
    "    id = df['3'][index]\n",
    "    index = dataset.index[dataset['id'] == id]\n",
    "    print(dataset['fandoms'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, index in enumerate(sample_indices):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamples = [0, 5, 10, 15, 20]\n",
    "res = []\n",
    "\n",
    "explainer = LimeTextExplainer(bow=True)\n",
    "explainerleft = MyLimeTextExplainer(bow=True, mode='left')\n",
    "explainerright = MyLimeTextExplainer(bow=True, mode='right')\n",
    "\n",
    "for count, index in enumerate(sample_indices):\n",
    "\n",
    "\n",
    "    label = df['labels'][index]\n",
    "    for s in subsamples:\n",
    "        cur_res = []\n",
    "\n",
    "        correct = False\n",
    "        segm = combine_segments_from_pd(index, s, 0) \n",
    "        #print(segm)\n",
    "        prediction = pipeline_onesegment(segm, mode='labels')\n",
    "        prediction = prediction[0]\n",
    "        orig_logits = pipeline_onesegment(segm, mode='logits')\n",
    "        if prediction == label:\n",
    "            correct = True\n",
    "\n",
    "        cur_res += index, s, segm, label, correct, orig_logits\n",
    "\n",
    "        \"\"\"if s == 0:\n",
    "            hcf = get_head_view_avg(segm, model1, tokenizer1)\n",
    "            with open(\"hc{}f.html\".format(index), \"w\") as file:\n",
    "                file.write(hcf.data)\n",
    "            hcu = get_head_view_avg(segm, model2, tokenizer2)\n",
    "            with open(\"hc{}u.html\".format(index), \"w\") as file:\n",
    "                file.write(hcu.data)\n",
    "            for layer in range(12):\n",
    "                hf = get_head_view(segm, model1, tokenizer1, layer)\n",
    "                with open(\"h{}_{}f.html\".format(index, layer), \"w\") as file:\n",
    "                    file.write(hf.data)\n",
    "                hu = get_head_view(segm, model2, tokenizer2, 11)\n",
    "                with open(\"h{}_{}u.html\".format(index, layer), \"w\") as file:\n",
    "                    file.write(hu.data)\n",
    "        \"\"\"\n",
    "        if prediction == 0:\n",
    "            exp = explainer.explain_instance(text_instance=segm, classifier_fn=pipeline_onesegment, num_features=10, num_samples=5000)\n",
    "            top_features = exp.as_list()\n",
    "            top_wordlist = []\n",
    "            top_weights = []\n",
    "            for f in top_features:\n",
    "                top_wordlist.append(f[0])\n",
    "            for word in top_wordlist:\n",
    "                segm_changed = segm.replace(word, \"\")\n",
    "                logits_changed = pipeline_onesegment(segm_changed, mode='logits')\n",
    "                #print(orig_logits[0], type(orig_logits[0]), logits_changed[0], type(logits_changed[0]))\n",
    "                weight = np.subtract(orig_logits[0], logits_changed[0])\n",
    "                mean_weight = np.mean([np.abs(weight[0]), np.abs(weight[1])])\n",
    "                top_weights.append(mean_weight)\n",
    "\n",
    "            \"\"\"for word in top_wordlist:\n",
    "                segm = segm.replace(word, \"\")\n",
    "            exp = explainer.explain_instance(text_instance=segm, classifier_fn=pipeline_onesegment, num_features=10, num_samples=5000)\n",
    "            top_sec_features = exp.as_list()\n",
    "            top_sec_wordlist = []\n",
    "            for f in top_sec_features:\n",
    "                top_sec_wordlist.append(f[0])\"\"\"\n",
    "\n",
    "        if prediction == 1:\n",
    "\n",
    "            exp_l = explainerleft.explain_instance(text_instance=segm, classifier_fn=pipeline_onesegment, num_features=6, num_samples=5000)\n",
    "            top_features_l = exp_l.as_list()\n",
    "            top_wordlist = []\n",
    "            top_weights = []\n",
    "            for f in top_features_l:\n",
    "                top_wordlist.append(f[0])\n",
    "\n",
    "            exp_r = explainerright.explain_instance(text_instance=segm, classifier_fn=pipeline_onesegment, num_features=6, num_samples=5000)\n",
    "            top_features_r = exp_r.as_list()\n",
    "            for f in top_features_r:\n",
    "                top_wordlist.append(f[0])\n",
    "                \n",
    "            for word in top_wordlist:\n",
    "                segm_changed = segm.replace(word, \"\")\n",
    "                logits_changed = pipeline_onesegment(segm_changed, mode='logits')\n",
    "                #print(orig_logits[0], type(orig_logits[0]), logits_changed[0], type(logits_changed[0]))\n",
    "                weight = np.subtract(orig_logits[0], logits_changed[0])\n",
    "                mean_weight = np.mean([np.abs(weight[0]), np.abs(weight[1])])\n",
    "                top_weights.append(mean_weight)\n",
    "\n",
    "            \"\"\"for word in top_wordlist:\n",
    "                segm = segm.replace(word, \"\")\n",
    "            exp = explainer.explain_instance(text_instance=segm, classifier_fn=pipeline_onesegment, num_features=10, num_samples=5000)\n",
    "            top_sec_features = exp.as_list()\n",
    "            top_sec_wordlist = []\n",
    "            for f in top_sec_features:\n",
    "                top_sec_wordlist.append(f[0])\"\"\"\n",
    "\n",
    "        cur_res += top_wordlist, top_features, top_weights\n",
    "\n",
    "        res.append(cur_res)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm = \"\"\"\" Sir, I have the the reports you\"ve asked for\" The bridge commander stated. \" Ahhh,\n",
    " thank you commander. Prep the fleet for the jump to Kashyyik\" Admiral Segutav \n",
    "ordered in his thick German accent. \" Yessir. Setting location now\" The commander replied. \" if I may ask sir, why did you need the reports on the \n",
    "Ghost\"s crew?\" \" \n",
    "I\"ve been put in charge of this fleet by none other than Grand Admiral Thrawn himself. He put me in command of this fleet with the sole purpose to destroy the Phoenix Squadron.\n",
    "$&*&*&$Hercules woke up when he heard Xena dreaming so he went\n",
    " and fetched some ale and put some sleeping poison in it. \"Xena, here take a drink.\" Hercules kindly demanded. \n",
    "\"Thank you, I think I need a drink.\" Xena replied. \"Xena, what was that all about?\" Hercules asked. \n",
    "\"I had a dream I was on Mount Olympus and Ares jumped out from somewhere with Gabrielle, then we started talking and all of a sudden he killed Gabrielle.\" Xena sobbed.\"\"\"\n",
    "\n",
    "#segm = combine_segments_from_pd(4433, 0, 0) \n",
    "print(segm)\n",
    "\n",
    "res = pipeline_onesegment(segm.replace(\"Segutav\", \"\"), mode=\"logits\")\n",
    "res\n",
    "#[ 4.5062966, -4.566842 ]\n",
    "#[ 4.1582108, -4.244451 ]\n",
    "#[-3.4521582,  2.2805903]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respd = pd.DataFrame (res, columns = ['text', 'label', 'correctness', 'logits', 'topwords', 'topwords_lime', 'topwords_obf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respd.to_csv('res_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_onetext = pipeline_onetext(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_combined_texts_long(text_or_list):\n",
    "    \"\"\"\n",
    "    Get data from raw text that contains two fragments and a separater, or from a list of texts,\n",
    "    each of them containing two fragments and a separater. Used in pipeline_onetext. The ONLY type\n",
    "    of data processor for LIME inputs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from raw text\")\n",
    "\n",
    "    datas = []\n",
    "\n",
    "    print(type(text_or_list), len(text_or_list))\n",
    "    if not isinstance(text_or_list, str):\n",
    "        for text_variant in text_or_list:\n",
    "            text1, text2 = text_variant.split(\"$&*&*&$\")\n",
    "            text1 = text_segmentate(text1, maxlen=750, seps='.?!;')\n",
    "            text2 = text_segmentate(text2, maxlen=750, seps='.?!;')\n",
    "            while len(text1) < 30 or len(text2) < 30:\n",
    "                    if len(text1) < 30:\n",
    "                        n_text1 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text1:\n",
    "                                n_text1.append(sent)\n",
    "                        text1 = n_text1\n",
    "                    elif len(text2) < 30:\n",
    "                        n_text2 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text2:\n",
    "                                n_text2.append(sent)\n",
    "                        text2 = n_text2\n",
    "            datas.append((text1, text2))\n",
    "    else:\n",
    "        text1, text2 = text_or_list.split(\"$&*&*&$\")\n",
    "        text1 = text_segmentate(text1, maxlen=750, seps='.?!;')\n",
    "        text2 = text_segmentate(text2, maxlen=750, seps='.?!;')\n",
    "        while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "        datas.append((text1, text2))\n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('textcomb3.txt', 'r') as text:\n",
    "    text3 = text.read()\n",
    "\n",
    "segments = get_data_from_combined_texts_long(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments(index, sep_option=0, write=False):\n",
    "    \"\"\"\n",
    "    Combine a pair of texts from dataset with a separator and turn into a single text\n",
    "    \"\"\"\n",
    "\n",
    "    text1 = segments[0][0][index]\n",
    "    text2 = segments[0][1][index]\n",
    "    sep = \"$&*&*&$\" if sep_option == 0 else \"[SEP]\"\n",
    "    text_combined = text1 + sep + text2\n",
    "    \n",
    "    if write:\n",
    "        name = \"textcomb{}.txt\".format(index)\n",
    "        with open(name, 'w') as textcomb:\n",
    "            textcomb.write(text_combined)\n",
    "\n",
    "    return(text_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm2 = combine_segments(1)\n",
    "segm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1 = combine_segments(0, 1)\n",
    "segm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext16 = combine_segments_from_pd(0,16)\n",
    "testtext16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext0_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext0_changed = \"\"\"\"OW!\" Mary yelled, jumping around, trying very hard not to curse or swear. \"Ow, mother flipping chicken poop.\" MARY yelled as she continued to yell and jump about.\n",
    " \"Okay now it\"s time for you to go to the hospital.\" Joey said, knocked her knees out and picked her up in order to carry her downstairs into the kitchen for ice. \"Wow your lighter \n",
    " then I remember. Have you not been eating?\" Joey asked, slightly alarmed. He could feel all of her bones through her clothes.$&*&*&$\"Are you okay?\" His simple inquiry about her \n",
    " farewell touched her more than a well-worded lecture would have. So she softened a little. Distress quickly morphing into a bout of self-pity and embarrassment. \"What do you think?\" her face now  the \n",
    "evidence of the strain she had been carrying for weeks. Her separation from Marcos made her achy and distressed. Her heart was breaking into a million\n",
    " pieces every day as the distance between her and Marcos continued to widen. \"I think you need to talk to him.\"\"\"\n",
    "\n",
    "\"\"\"She convinced the guard to let her through the gates and she ran up the familiar stairs until she got to the potted bush next to the front door, she reached \n",
    "down and pulled out the spare key Joey kept there for her. She used them to unlock the door and then tossed her bag down, leaving the door open, and ran upstairs \n",
    "into his room. She then fell down on his bed, held onto a pillow and cried so hard that she didn\"t even hear anyone enter the house. \"Mary!\" Joey yelled, standing \n",
    "dumb-struck in the doorway.$&*&*&$\"Joh..n.\" her voice turned breathy, heat suffusing through her pores. A loud crash sounded behind them, echoing through the hall. \n",
    "They jerked apart and saw Marcos had accidentally dropped a cement block and created a hole in the floor while Lorna stood glaring at him. \n",
    "Then she bit out,\"Nice going laser. As usual I\"ll have to fix your mess.\" Then proceeded to maneuver a few metals plates through the hole in an attempt to mend it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_onesegment(testtext16, mode='logits')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext0_changed = testtext16.replace(\"Clarice\",\"\")\n",
    "\n",
    "\n",
    "res = pipeline_onesegment(testtext0_changed, mode='logits')\n",
    "print(res)\n",
    "\n",
    "\n",
    "#example 0-0 \n",
    "#[[ 4.7861366 -4.759954 ]] class 0 correct confidence 1\n",
    "#['She', 'MARY', 'and', 'proceeded', 'ran', 'bush']\n",
    "#without She (weight 0.28) [[ 4.1627555 -4.1556478]]\n",
    "#without MARY (weight 0.24) [[ 4.9938765 -5.0127206]] contrary to Lime!\n",
    "#if we replace MARY with Mary the difference becomes larger [[ 5.4506316 -5.633418 ]] which means that CASING MATTERS\n",
    "#if we replace MARY with JOHN texts get even more different [[ 5.380384 -5.513166]]. Keep increasing slightly if we add more JOHN\n",
    "#if we add JOHN to the second part as well, gets less different [[-4.439573  3.488577]]\n",
    "#without all topwords [[-2.9259872  1.8621671]] LABEL CHANGES\n",
    "\n",
    "#example 0-4\n",
    "#[ 1.371373 , -1.5087308] class 0 correct confidence 0.95\n",
    "#['bore', 'yelled', 'strain', 'worded', 'What', 'said']\n",
    "# without bore (weight 0.15) label SUDDENLY CHANGES [[-2.2827718  1.4593749]]\n",
    "# without worded (weight 0.14) label ALSO SUDDENLY CHANGES [[-1.9648905  1.2226754]]\n",
    "# without farewell (weight 0.12) ALSO [[-2.2466617  1.45032  ]], same for touched, said\n",
    "# and they somewhat combine: when we remove all 3 -- bigger changer [[-4.2394676  3.2303755]]\n",
    "# while without yelled (weight 0.11) it only goes in another direction [[ 2.6601286 -2.9694483]]\n",
    "# SO here the model very easily switches to class 1\n",
    "\n",
    "#example 0-16\n",
    "#[[-5.0536942,  4.766159 ]] class 1 incorrect confidence 1\n",
    "# however, it's enough to remove 1 name. Without Clarice (weight 0.26) [[ 2.7647822 -3.1579168]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1 = \"\"\"Rinoa let out a soft giggle. \"Okay Uncle Laguna.\" \"As always, make yourselves at home!\" Kiros cleared his throat. \"Laguna, I believe our guests are hungry.\" \"OH! Yes yes, I\"m sorry.\" \n",
    "Laguna scratched the back of his neck in embarassment. \"To the dining hall we go!\" They all followed Laguna as he went through one of the sliding doors on the right side of the room. \n",
    "The dining hall was a plain one, though the lenghty table was pleasingly decorated with foods on the table.$&*&*&$The crown prince already entreated help from the glaives, a last resort he would \n",
    "rather not do as he did not want his father to be anymore involved. But they were heavily outnumbered, and their chances of surviving were slimming to none.\n",
    " It was when nea decided to show up with her own infantry jumping from her red ship. There was so much distrust towards her at first, knowing how long she had served the Emperor and carried out his orders.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm3 = \"\"\"\"\" Mary yelled, jumping around, trying very hard not to curse or swear. \"Ow, mother flipping chicken poop.\" Mary yelled as she continued to yell and jump about. \n",
    "\"Okay now it\"s time for you to go to the hospital.\" Joey said, knocked her knees out and picked her up in order to carry her downstairs into the kitchen for ice. \n",
    "\"Wow your lighter then I remember. Have you not been eating?\" Joey asked, slightly alarmed. He could feel all of her bones through her clothes.$&*&*&$\"Are you okay?\" \n",
    "His simple inquiry about her farewell touched her more than a well-worded lecture would have. So she softened a little. Distress quickly morphing into a bout of self-pity and embarrassment.\n",
    " \"What do you think?\" her face now bore the evidence of the strain she had been carrying for weeks. \n",
    "Her separation from Marcos made her achy and distressed. Her heart was breaking into a million pieces every day as the distance between her and Marcos continued to widen. \"I think you need to talk to him.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm4 = \"\"\"What hospital are you at?\" Dustin said and there was a lot of rustling at the other end. \"The one near my house, Clarice. I didn\"t catch the name I was a little occupied. \n",
    "Mary was throwing a fit.\" Joey answered sincerely. \"Kay, we are coming. See you in a few.\" Dustin said and the line went dead. Joey took a seat along the wall and waited for news about Mary.\n",
    " Just a tad bit of drama, hope its not too confusing. Blondie : P Oh gosh, sorry. I left you a cliff hanger. I wonder what will happen to Mary today.$&*&*&$Clarice bobbed her head slightly from side to side and with a sheepish look remarked,\n",
    "  \"Not exactly but something to the effect, Clarice. I did call him a ninny though.\" John flashed his killer smile, it made her gooey every time. In an instant his eyes turned intense again and he rested his forehead on hers. The change in him didn\"t go unnoticed by Clarice. \n",
    "\"What?\" \"Clarice I...feel it again.\" He had told her about how he sometimes felt a foreboding feeling. \"Hey.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_onesegment(segm1, mode='logits')\n",
    "res = [-5.531711 ,  5.7830925], [-5.3570204,  5.4185996]\n",
    "#Without Ara\n",
    "#shap prediction 1: 0.506, 0: -0.474\n",
    "#in reality 1: 0.364, 0: -0.175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_onesegment(segm4, mode='logits')\n",
    "res\n",
    "#[-5.0536942,  4.766159 ]\n",
    "#One more Clarice [-5.1230087,  4.9003825]\n",
    "#Two more Clarice[-5.2671213,  5.1808963]\n",
    "#Third Clarice now in the first segment [-5.4629855,  5.5636225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_onesegment(segm3, mode='logits')\n",
    "res\n",
    "#[ 1.371373 , -1.5087308]\n",
    "#OW [ 2.905075 , -3.2013812]\n",
    "#! [ 2.2600317, -2.5298123]\n",
    "#OW! [-0.00994844, -0.42599773]\n",
    "#shap prediction OW for class 1 OW -0.305 ! -0.27\n",
    "#shap prediction OW for class 0 OW 0.303 ! 0.308\n",
    "#in reality for class 1 OW +1.54 ! +1.02 OW! -1.08\n",
    "#in reality for class 0 OW -1.53 ! -0.89 OW! 1.38 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get explanations for separate segments using pipeline_onesegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(bow=True)\n",
    "explainerleft = MyLimeTextExplainer(bow=True, mode='left')\n",
    "explainerright = MyLimeTextExplainer(bow=True, mode='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainerchar = LimeTextExplainer(char_level=True)#, split_expression = r'[^A-Za-z.?!,\"-]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp16 = explainer.explain_instance(text_instance=testtext16, classifier_fn=pipeline_onesegment, num_features=10, num_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp16.show_in_notebook(text=True)\n",
    "#lime simple, test example 0-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#lime simple, test example 0-4\n",
    "exp4.show_in_notebook(text=True)\n",
    "#lime simple, test example 0-4 without bore\n",
    "exp4_2.show_in_notebook(text=True)\n",
    "#lime simple, test example 0-4 with 10k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#lime simple, test example 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#lime simple, test example 0-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = exp.as_list()\n",
    "top_list = []\n",
    "for f in top_features:\n",
    "    top_list.append(f[0])\n",
    "top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_top(text, top):\n",
    "    for word in top:\n",
    "        text = text.replace(word, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext0_changed = remove_top(testtext0, top_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#main model, text without top features excluding Aranea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#main model, text without top features including Aranea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exppunct.show_in_notebook(text=True)\n",
    "#with punctuation, simple lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#180000 model, simple lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expleft.show_in_notebook(text=True)\n",
    "#180000 model, left lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expleft.show_in_notebook(text=True)\n",
    "#main model, left lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expright.show_in_notebook(text=True)\n",
    "#180000 model, right lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expright.show_in_notebook(text=True)\n",
    "#main model, right lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2 = explainer2.explain_instance(text_instance=segm2, labels=(1,), classifier_fn=pipeline_onesegment, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_onesegment(segm1.replace('giggle','giggggggle'), mode='logits')\n",
    "print(res)\n",
    "\n",
    "#[-5.531711   5.7830925] 0 1\n",
    "#Without Aranea:\n",
    "#[-4.927722   4.5944533]\n",
    "#Without Aranea and all top features (as in segm1_notop): Label changes\n",
    "#[[ 1.4439765 -1.4362624]]\n",
    "\n",
    "#Without Rinoa:\n",
    "#[[-5.381693   5.3939533]]\n",
    "#Without Rinoa and Laguna:\n",
    "#[[-5.2352524  5.1498575]]\n",
    "#and though\n",
    "#[[-5.0144567  4.673427 ]]\n",
    "#and Kiros\n",
    "#[[-4.0238514  2.966291 ]]\n",
    "#and OH: grows back!\n",
    "#[[-4.8581166  4.3767138]]\n",
    "\n",
    "#DEFAULT NOTOP without cleared instead of OH: significant drop (ALL top-features for class 1 removed)\n",
    "#[[-2.8962233  1.7099165]]\n",
    "# without cleared and OH: grows back\n",
    "#[[-4.005427   2.9661796]]\n",
    "#If we return OH but remove a quote before it -> LABLE CHANGES\n",
    "#[[-0.14309846 -0.2922034 ]]\n",
    "#If we remove a quote before OH without other changes -> only a tiny drop\n",
    "#[[-5.5299683  5.7772665]]\n",
    "#If we remove a quote AND OH after removing the top features -> lack of OH pushes back, nothing important\n",
    "#[-4.096264   3.1056435]\n",
    "\n",
    "# [[-5.5027714  5.6508436]] Rinoa removed and OH instead of \"OH\n",
    "# [[-5.498661  5.7157  ]] Laguna and\n",
    "# [[-5.5193887  5.702038 ]] Kiros and\n",
    "# [[-5.5278864  5.8021803]] though and\n",
    "# [[-5.5328455  5.756836 ]] cleared and\n",
    "\n",
    "# when \"OH is turned into OH, other top features weight more. But when Laguna was the last, it didn't change much until we removed Laguna\n",
    "\n",
    "#Removed some \" -> label 1 grows a bit\n",
    "#[[-5.5252357  5.802293 ]]\n",
    "#Removed all \" -> label 0 grows more\n",
    "#[[-5.3608084  5.3494406]]\n",
    "\n",
    "#Removed all \" but also top features -> label changes and opposite logits are HUGE!\n",
    "#[[ 2.9483943 -3.1187959]]\n",
    "\n",
    "\n",
    "\n",
    "#replacing \" with ' -> not significant; . with ! -> NS, ! with , or . -> NS; removing , NS\n",
    "\n",
    "#after removing all top features, changing . with ! becomes a lot more important and now class 1 is a lot more probable\n",
    "#[[-4.64954    4.0420275]]\n",
    "#changing only in the first text doesn't matter much (even though 8 occurences)\n",
    "#[[-2.8171382  1.7177728]]\n",
    "#changing all . to ! in the second didn't matter much until we changed all ! to . in the first:\n",
    "#[[ 2.222923  -2.2992437]]\n",
    "#however, if we simply change all ! to . (that is, only change the first), we only get small increase of dissimilarity (although technically pieces become more uniform!)\n",
    "#[[-2.6927602  1.4867294]]\n",
    "# changing ! to ? instead of . is a bit more important\n",
    "#[[-2.616405  1.396568]]\n",
    "# removing . completely from one or both makes texts more similar\n",
    "#[[-4.8784447  4.341698 ]]\n",
    "# and it goes further when we remove other punctuation as well as .\n",
    "#[[-5.441371   5.5717177]]\n",
    "\n",
    "#it is EVEN MORE SALIENT with , if we only remove ,\n",
    "#[[-5.252601   5.1758327]]\n",
    "\n",
    "#getting in another directinon (label 0) is not as easy. We only found one case before\n",
    "\n",
    "#adding new punctuations allows to do so rather fast. After adding a few extra . to the first text (only when also removing from second):\n",
    "#[[ 1.8393167 -1.9136158]]\n",
    "# When we kept the second text unchanged, the , didn't change the label\n",
    "#[[-2.5794852  1.3692824]]\n",
    "#however, as soon as we added , after I believe, the LABEL CHANGED!\n",
    "#[[ 1.5631607 -1.6246212]]\n",
    "# and changed even more when we added , after our guests instead (and they DIDN'T COMBINE!)\n",
    "#[[ 1.7775549 -1.8065834]]\n",
    "#not every change is so important. , after let out only changed the results slightly and didn't influence the label\n",
    "#[[-2.0973256  1.0720851]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1.replace('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1_changed.replace('Laguna', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1_changed.replace('cleared', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1_changed.replace('though', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1_changed.replace('Kiros', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1_changed.replace('\"OH', 'OH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1_notop.replace('\"! Yes', 'OH! Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = segm1_notop.replace('.', ',', 8).replace('!', '?', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = \"\"\" let out a soft giggle. \"Okay Uncle .\" \"As always, make yourselves at home!\"   his throat. \", I believe our guests are hungry.\" \"OH! Yes yes, I\"m sorry.\"  \n",
    "scratched the back of his neck in embarassment. \"To the dining hall we go!\" They all followed  as he went through one of the sliding doors on the right side of the room. \n",
    "The dining hall was a plain one,  the lenghty table was pleasingly decorated with foods on the table.$&*&*&$The crown prince already entreated help from the glaives, \n",
    "a last resort he would rather not do as he did not want his father to be anymore involved. But they were heavily outnumbered, and their chances of surviving were slimming to none. \n",
    "It was when Aranea decided to show up with her own infantry jumping from her red ship. There was so much distrust towards her at first, knowing how long she had served the Emperor and carried out his orders.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm1_changed = \"\"\" let out a soft giggle. \"Okay Uncle .\" \"As always, make yourselves at home!\"   his throat. \", I believe our guests are hungry.\" \"OH! Yes yes, I\"m sorry.\"  \n",
    "scratched the back of his neck in embarassment. \"To the dining hall we go!\" They all followed  as he went through one of the sliding doors on the right side of the room. \n",
    "The dining hall was a plain one,  the lenghty table was pleasingly decorated with foods on the table.$&*&*&$The crown prince already entreated help from the glaives, \n",
    "a last resort he would rather not do as he did not want his father to be anymore involved. But they were heavily outnumbered, and their chances of surviving were slimming to none. \n",
    "It was when Aranea decided to show up with her own infantry jumping from her red ship. There was so much distrust towards her at first, knowing how long she had served the Emperor and carried out his orders.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_segmentate(text, maxlen, seps='\\n', strips=None):\n",
    "    \"\"\"将文本按照标点符号划分为若干个短句\n",
    "    \"\"\"\n",
    "    text = text.strip().strip(strips)\n",
    "    if seps and len(text) > maxlen:\n",
    "        pieces = text.split(seps[0])\n",
    "        text, texts = '', []\n",
    "        for i, p in enumerate(pieces):\n",
    "            if text and p and len(text) + len(p) > maxlen - 1:\n",
    "                texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "                text = ''\n",
    "            if i + 1 == len(pieces):\n",
    "                text = text + p\n",
    "            else:\n",
    "                text = text + p + seps[0]\n",
    "        if text:\n",
    "            texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "        return texts\n",
    "    else:\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_4 = text_segmentate(text4, seps=\".!?\", maxlen=510)\n",
    "print(segmented_4[0], len(segmented_4[0]), len(segmented_4[0].split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get explanations using pipeline_onetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "#explainer = LimeTextExplainer(bow=False)\n",
    "#explainer2 = LimeTextExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = combine_texts(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer2.explain_instance(text_instance=text4, labels=(0,), classifier_fn=pipeline_onetext, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2 = explainer2.explain_instance(text_instance=text4, labels=(0,1), classifier_fn=pipeline_onetext, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_true = explainer.explain_instance(text_instance=text_combined, classifier_fn=pipeline_onetext, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_true3 = explainer.explain_instance(text_instance=text_combined, classifier_fn=pipeline_onetext, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.save_to_file('textcomb1.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_true.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_true.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_true2.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_true3.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a certain prediction and save corresponding embedding for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_truth_uncertain = pipeline_onetext(text_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding is formed during the execution of pipeline_onetext\n",
    "\n",
    "torch.save(embedding, \"..\\\\BertAA_content\\\\predictions_truth_uncertain.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def vocab_size(text, k=15, r=0):\n",
    "    \"\"\"This functions calculates and prints the results for a text's vocabulary\n",
    "    It returns vocabulary in terms of unique:\n",
    "    1. Tokens\n",
    "    2. Words or Numbers\n",
    "    3. Words Only\n",
    "    4. Words (less English stopwords)\n",
    "    5. Complex Words > Length k (default = 15)\"\"\"\n",
    "\n",
    "    #Default Option, Print\n",
    "    if r == 0:\n",
    "        #Unique or \"u\" Sets\n",
    "        text = text.split()\n",
    "        u_tokens = len(set(text))\n",
    "        u_words_num = len(set([w for w in text if w.isalnum()]))\n",
    "        u_words = len(set([w for w in text if w.isalpha()]))\n",
    "        #u_words_less_stopwords = len(set([w for w in text if w.lower() not in stopwords.words('english')]))\n",
    "        u_complex_words = len(set([w for w in text if w.isalnum() and len(w.lower()) > k ]))\n",
    "\n",
    "        print (\"__\"*30, '\\n',  \\\n",
    "               \"The text has the following vocabulary: \", '\\n', \\\n",
    "               \"Unique Tokens: \", u_tokens, '\\n', \\\n",
    "               \"Unique Words or Numbers: \", u_words_num, '\\n', \\\n",
    "               \"Unique Words: \", u_words,'\\n', \\\n",
    "               #\"Unique Words (Less Stopwords): \", u_words_less_stopwords, '\\n', \\\n",
    "               \"Unique Complex Words (> \", k, \" Characters): \", u_complex_words, '\\n', \\\n",
    "               \"__\"*30 \\\n",
    "               )\n",
    "        print(Counter([w for w in text if w.isalpha()]))\n",
    "\n",
    "    #Option to Return Complex Words\n",
    "    elif r == 1:\n",
    "        ucw = [w.lower() for w in text if w.isalnum() and len(w.lower()) > k ]\n",
    "        return ucw\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabtext = \"textcomb.txt\"\n",
    "\n",
    "def stat_for_text(vocabtext, split=False):\n",
    "    with open(vocabtext, 'r') as vocabtext:\n",
    "        vocabtext = vocabtext.read()\n",
    "\n",
    "        if split:\n",
    "            text1, text2 = vocabtext.split(\"$&*&*&$\")\n",
    "            vocab_size(text1)\n",
    "            print(\"\\n\"*2+\"#\"*60+\"\\n\"*2)\n",
    "            vocab_size(text2)\n",
    "        else:\n",
    "            vocab_size(vocabtext)\n",
    "\n",
    "stat_for_text(vocabtext, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = MyLimeTextExplainer(bow=False, mode='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer2 = MyLimeTextExplainer(bow=True, mode='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer2.explain_instance(text4, classifier_fn=pipeline_onetext, num_samples=5000, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 permutations, bow, left mode\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 permutations, bow, right mode\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.intercept\n",
    "\n",
    "# compare with ret_exp.intercept {1: 0.998998835482347}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myprediction = pipeline_onetext(text4, mode='logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myprediction1 = pipeline_onetext(text4, mode='logits')\n",
    "myprediction2 = pipeline_onetext(text_4_no_aranea, mode='logits')\n",
    "print(myprediction1, myprediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_4_no_aranea = text4.replace('Aranea','').replace('Ignis','').replace('Gladiolus','').replace('Rinoa','').replace('Squall','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_4_no_aranea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)\n",
    "#right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import TextDomainMapper\n",
    "\n",
    "\n",
    "explainer.explain_instance(text4, classifier_fn=pipeline_onetext, num_samples=5000, labels=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,yss,distances=explainer._MyLimeTextExplainer__data_labels_distances(MyIndexedString(text4, bow = False),classifier_fn=pipeline_onetext,num_samples=5000,mode='rand')\n",
    "## Top 2 closest perturbed samples\n",
    "df=pd.DataFrame(distances,columns=['distance'])\n",
    "df1=df.sort_values(by='distance')\n",
    "req_index=df.index[1:50]\n",
    "closest_perturbed_sample=[]\n",
    "for k in req_index:\n",
    "    perturbed_text =' '.join([re.split(r'\\W+',text)[i] for i,x in enumerate(data[k]) if x==1.0])\n",
    "    closest_perturbed_sample.append(perturbed_text)\n",
    "closest_perturbed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_exp.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import TextDomainMapper\n",
    "from lime import lime_base\n",
    "import numpy\n",
    "\n",
    "def kernel(d, kernel_width):\n",
    "    return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "kernel_fn = partial(kernel, kernel_width=25)\n",
    "\n",
    "random_state=numpy.random.RandomState()\n",
    "\n",
    "base = lime_base.LimeBase(kernel_fn, verbose=False,\n",
    "                                       random_state=random_state)\n",
    "\n",
    "domain_mapper = TextDomainMapper(MyIndexedString(text4, bow = False))\n",
    "class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "ret_exp = explanation.Explanation(domain_mapper=domain_mapper,\n",
    "                                    class_names=class_names,\n",
    "                                    random_state=random_state)\n",
    "ret_exp.predict_proba = yss[0]\n",
    "\n",
    "for label in (1,):\n",
    "    (ret_exp.intercept[label],\n",
    "    ret_exp.local_exp[label],\n",
    "    ret_exp.score[label],\n",
    "    ret_exp.local_pred[label]) = base.explain_instance_with_data(\n",
    "        data, yss, distances, label, num_features=5,\n",
    "        model_regressor=None,\n",
    "        feature_selection='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = base.explain_instance_with_data(\n",
    "        data, yss, distances, label, num_features=5,\n",
    "        model_regressor=None,\n",
    "        feature_selection='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_exp.local_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in (1,):\n",
    "    (ret_exp.intercept[label],\n",
    "    ret_exp.local_exp[label],\n",
    "    ret_exp.score[label],\n",
    "    ret_exp.local_pred[label]) = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_exp.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from lime import lime_base\n",
    "\n",
    "from lime.lime_text import TextDomainMapper, LimeTextExplainer\n",
    "\n",
    "class MyLimeTextExplainer(LimeTextExplainer):\n",
    "    \"\"\"Explains text classifiers.\n",
    "       Currently, we are using an exponential kernel on cosine distance, and\n",
    "       restricting explanations to words that are present in documents.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel_width=25,\n",
    "                 kernel=None,\n",
    "                 verbose=False,\n",
    "                 class_names=None,\n",
    "                 feature_selection='auto',\n",
    "                 split_expression=r'\\W+',\n",
    "                 bow=True,\n",
    "                 mask_string=None,\n",
    "                 random_state=None,\n",
    "                 char_level=False,\n",
    "                 mode='rand'):\n",
    "        \"\"\"Init function.\n",
    "        Args:\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "            kernel: similarity kernel that takes euclidean distances and kernel\n",
    "                width as input and outputs weights in (0,1). If None, defaults to\n",
    "                an exponential kernel.\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            class_names: list of class names, ordered according to whatever the\n",
    "                classifier is using. If not present, class names will be '0',\n",
    "                '1', ...\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            split_expression: Regex string or callable. If regex string, will be used with re.split.\n",
    "                If callable, the function should return a list of tokens.\n",
    "            bow: if True (bag of words), will perturb input data by removing\n",
    "                all occurrences of individual words or characters.\n",
    "                Explanations will be in terms of these words. Otherwise, will\n",
    "                explain in terms of word-positions, so that a word may be\n",
    "                important the first time it appears and unimportant the second.\n",
    "                Only set to false if the classifier uses word order in some way\n",
    "                (bigrams, etc), or if you set char_level=True.\n",
    "            mask_string: String used to mask tokens or characters if bow=False\n",
    "                if None, will be 'UNKWORDZ' if char_level=False, chr(0)\n",
    "                otherwise.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "            char_level: an boolean identifying that we treat each character\n",
    "                as an independent occurence in the string\n",
    "        \"\"\"\n",
    "\n",
    "        if kernel is None:\n",
    "            def kernel(d, kernel_width):\n",
    "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
    "\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.base = lime_base.LimeBase(kernel_fn, verbose,\n",
    "                                       random_state=self.random_state)\n",
    "        self.class_names = class_names\n",
    "        self.vocabulary = None\n",
    "        self.feature_selection = feature_selection\n",
    "        self.bow = bow\n",
    "        self.mask_string = mask_string\n",
    "        self.split_expression = split_expression\n",
    "        self.char_level = char_level\n",
    "        self.mode = mode\n",
    "\n",
    "    def explain_instance(self,\n",
    "                         text_instance,\n",
    "                         classifier_fn,\n",
    "                         labels=(1,),\n",
    "                         top_labels=None,\n",
    "                         num_features=10,\n",
    "                         num_samples=5000,\n",
    "                         distance_metric='cosine',\n",
    "                         model_regressor=None):\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "        First, we generate neighborhood data by randomly hiding features from\n",
    "        the instance (see __data_labels_distance_mapping). We then learn\n",
    "        locally weighted linear models on this neighborhood data to explain\n",
    "        each of the classes in an interpretable way (see lime_base.py).\n",
    "        Args:\n",
    "            text_instance: raw text string to be explained.\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a list of d strings and outputs a (d, k) numpy array with\n",
    "                prediction probabilities, where k is the number of classes.\n",
    "                For ScikitClassifiers , this is classifier.predict_proba.\n",
    "            labels: iterable with labels to be explained.\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for sample weighting,\n",
    "                defaults to cosine similarity\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "            to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
    "            and 'sample_weight' as a parameter to model_regressor.fit()\n",
    "        Returns:\n",
    "            An Explanation object (see explanation.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        indexed_string = (IndexedCharacters(\n",
    "            text_instance, bow=self.bow, mask_string=self.mask_string)\n",
    "                          if self.char_level else\n",
    "                          MyIndexedString(text_instance, bow=self.bow,\n",
    "                                        split_expression=self.split_expression,\n",
    "                                        mask_string=self.mask_string, mode=self.mode))\n",
    "        domain_mapper = TextDomainMapper(indexed_string)\n",
    "        data, yss, distances = self.__data_labels_distances(\n",
    "            indexed_string, classifier_fn, num_samples,\n",
    "            distance_metric=distance_metric, mode = self.mode)\n",
    "        if self.class_names is None:\n",
    "            self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "        #ret_exp = explanation.Explanation(domain_mapper=domain_mapper,\n",
    "        ret_exp = MyExplanation(domain_mapper=domain_mapper,\n",
    "                                          class_names=self.class_names,\n",
    "                                          random_state=self.random_state)\n",
    "        ret_exp.predict_proba = yss[0]\n",
    "        if top_labels:\n",
    "            labels = np.argsort(yss[0])[-top_labels:]\n",
    "            ret_exp.top_labels = list(labels)\n",
    "            ret_exp.top_labels.reverse()\n",
    "        for label in labels:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score[label],\n",
    "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
    "                data, yss, distances, label, num_features,\n",
    "                model_regressor=model_regressor,\n",
    "                feature_selection=self.feature_selection)\n",
    "        return ret_exp\n",
    "\n",
    "    def __data_labels_distances(self,\n",
    "                                indexed_string,\n",
    "                                classifier_fn,\n",
    "                                num_samples,\n",
    "                                distance_metric='cosine',\n",
    "                                mode='rand'):\n",
    "        \"\"\"Generates a neighborhood around a prediction.\n",
    "        Generates neighborhood data by randomly removing words from\n",
    "        the instance, and predicting with the classifier. Uses cosine distance\n",
    "        to compute distances between original and perturbed instances.\n",
    "        Args:\n",
    "            indexed_string: document (IndexedString) to be explained,\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a string and outputs prediction probabilities. For\n",
    "                ScikitClassifier, this is classifier.predict_proba.\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for sample weighting,\n",
    "                defaults to cosine similarity.\n",
    "        Returns:\n",
    "            A tuple (data, labels, distances), where:\n",
    "                data: dense num_samples * K binary matrix, where K is the\n",
    "                    number of tokens in indexed_string. The first row is the\n",
    "                    original instance, and thus a row of ones.\n",
    "                labels: num_samples * L matrix, where L is the number of target\n",
    "                    labels\n",
    "                distances: cosine distance between the original instance and\n",
    "                    each perturbed instance (computed in the binary 'data'\n",
    "                    matrix), times 100.\n",
    "        \"\"\"\n",
    "        def distance_fn(x):\n",
    "            return sklearn.metrics.pairwise.pairwise_distances(\n",
    "                    x, x[0], metric=distance_metric).ravel() * 100\n",
    "\n",
    "        doc_size = indexed_string.num_words()\n",
    "\n",
    "        sep = indexed_string.return_sep()\n",
    "        print(\"sep: \", sep, \"docsize: \", doc_size)\n",
    "\n",
    "        global sample\n",
    "        sample = self.random_state.randint(1, doc_size + 1, num_samples - 1)\n",
    "\n",
    "        data = np.ones((num_samples, doc_size))\n",
    "        data[0] = np.ones(doc_size)\n",
    "        inverse_data = [indexed_string.raw_string()]\n",
    "\n",
    "        if not indexed_string.bow: \n",
    "            \n",
    "            if sep < doc_size:\n",
    "                print(\"separation on\")\n",
    "\n",
    "                sample_left = self.random_state.randint(1, sep + 1, num_samples - 1)\n",
    "                sample_right = self.random_state.randint(1, doc_size - sep, num_samples - 1)\n",
    "\n",
    "                features_range_left = range(0, sep)\n",
    "                features_range_right = range(sep, doc_size)\n",
    "\n",
    "                if mode == 'left':\n",
    "                    print(\"left\")\n",
    "                    sample = sample_left\n",
    "                    features_range = features_range_left\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                elif mode == 'right':\n",
    "                    print(\"right\")\n",
    "                    sample = sample_right\n",
    "                    features_range = features_range_right\n",
    "\n",
    "                    #global save_sample\n",
    "                    #save_sample = sample\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        print(i, len(features_range), size)\n",
    "                        inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "                else:\n",
    "                    print('rand')\n",
    "                    dir = self.random_state.randint(2, size = num_samples - 1)\n",
    "\n",
    "                    for i, size in enumerate(sample, start=1):\n",
    "                        #print(dir[i-1])\n",
    "                        if dir[i-1]:\n",
    "                            inactive = self.random_state.choice(features_range_right, sample_right[i-1],\n",
    "                                                        replace=False)\n",
    "                        else:\n",
    "                            inactive = self.random_state.choice(features_range_left, sample_left[i-1],\n",
    "                                                        replace=False)\n",
    "                        data[i, inactive] = 0\n",
    "                        inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "            else:\n",
    "                features_range = range(doc_size)\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "        else:\n",
    "\n",
    "            #added in lime22\n",
    "            if indexed_string.positions_right:\n",
    "                print(\"separation on, bow mode\")\n",
    "\n",
    "                sample_left = self.random_state.randint(1, sep + 1, num_samples - 1)\n",
    "                sample_right = self.random_state.randint(1, doc_size - sep, num_samples - 1)\n",
    "\n",
    "                features_range_left = range(0, sep)\n",
    "                features_range_right = range(0, doc_size - sep)\n",
    "\n",
    "                #global save_sample\n",
    "                #save_sample = sample\n",
    "\n",
    "                if mode == 'left':\n",
    "                    print(\"left bow\")\n",
    "                    sample = sample_left\n",
    "                    #print(\"sample: \", sample)\n",
    "                    features_range = features_range_left\n",
    "\n",
    "                elif mode == 'right':\n",
    "                    print(\"right bow\")\n",
    "                    sample = sample_right\n",
    "                    #print(\"sample: \", sample)\n",
    "                    features_range = features_range_right\n",
    "\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    #print(i, len(features_range), size)\n",
    "                    inactive = self.random_state.choice(features_range, size,\n",
    "                                                    replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    #print('inactive', inactive)\n",
    "                    inverse_data.append(indexed_string.inverse_removing_modes(inactive, mode=mode))\n",
    "\n",
    "            else:\n",
    "                features_range = range(doc_size)\n",
    "                for i, size in enumerate(sample, start=1):\n",
    "                    inactive = self.random_state.choice(features_range, size,\n",
    "                                                        replace=False)\n",
    "                    data[i, inactive] = 0\n",
    "                    inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "\n",
    "\n",
    "        labels = classifier_fn(inverse_data)\n",
    "        distances = distance_fn(sp.sparse.csr_matrix(data))\n",
    "        return data, labels, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedCharacters(object):\n",
    "    \"\"\"String with various indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_string, bow=True, mask_string=None):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            raw_string: string with raw text in it\n",
    "            bow: if True, a char is the same everywhere in the text - i.e. we\n",
    "                 will index multiple occurrences of the same character. If False,\n",
    "                 order matters, so that the same word will have different ids\n",
    "                 according to position.\n",
    "            mask_string: If not None, replace characters with this if bow=False\n",
    "                if None, default value is chr(0)\n",
    "        \"\"\"\n",
    "        self.raw = raw_string\n",
    "        self.as_list = list(self.raw)\n",
    "        self.as_np = np.array(self.as_list)\n",
    "        self.mask_string = chr(0) if mask_string is None else mask_string\n",
    "        self.string_start = np.arange(len(self.raw))\n",
    "        vocab = {}\n",
    "        self.inverse_vocab = []\n",
    "        self.positions = []\n",
    "        self.bow = bow\n",
    "        non_vocab = set('$','&','*')\n",
    "        for i, char in enumerate(self.as_np):\n",
    "            if char in non_vocab:\n",
    "                continue\n",
    "            if bow:\n",
    "                if char not in vocab:\n",
    "                    vocab[char] = len(vocab)\n",
    "                    self.inverse_vocab.append(char)\n",
    "                    self.positions.append([])\n",
    "                idx_char = vocab[char]\n",
    "                self.positions[idx_char].append(i)\n",
    "            else:\n",
    "                self.inverse_vocab.append(char)\n",
    "                self.positions.append(i)\n",
    "        if not bow:\n",
    "            self.positions = np.array(self.positions)\n",
    "\n",
    "    def raw_string(self):\n",
    "        \"\"\"Returns the original raw string\"\"\"\n",
    "        return self.raw\n",
    "\n",
    "    def num_words(self):\n",
    "        \"\"\"Returns the number of tokens in the vocabulary for this document.\"\"\"\n",
    "        return len(self.inverse_vocab)\n",
    "\n",
    "    def word(self, id_):\n",
    "        \"\"\"Returns the word that corresponds to id_ (int)\"\"\"\n",
    "        return self.inverse_vocab[id_]\n",
    "\n",
    "    def string_position(self, id_):\n",
    "        \"\"\"Returns a np array with indices to id_ (int) occurrences\"\"\"\n",
    "        if self.bow:\n",
    "            return self.string_start[self.positions[id_]]\n",
    "        else:\n",
    "            return self.string_start[[self.positions[id_]]]\n",
    "\n",
    "    def inverse_removing(self, words_to_remove):\n",
    "        \"\"\"Returns a string after removing the appropriate words.\n",
    "        If self.bow is false, replaces word with UNKWORDZ instead of removing\n",
    "        it.\n",
    "        Args:\n",
    "            words_to_remove: list of ids (ints) to remove\n",
    "        Returns:\n",
    "            original raw string with appropriate words removed.\n",
    "        \"\"\"\n",
    "        mask = np.ones(self.as_np.shape[0], dtype='bool')\n",
    "        mask[self.__get_idxs(words_to_remove)] = False\n",
    "        if not self.bow:\n",
    "            return ''.join(\n",
    "                [self.as_list[i] if mask[i] else self.mask_string\n",
    "                 for i in range(mask.shape[0])])\n",
    "        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n",
    "\n",
    "    def __get_idxs(self, words):\n",
    "        \"\"\"Returns indexes to appropriate words.\"\"\"\n",
    "        if self.bow:\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [self.positions[z] for z in words]))\n",
    "        else:\n",
    "            return self.positions[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIndexedString(object):\n",
    "    \"\"\"String with various indexes.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_string, split_expression=r'\\W+', bow=True,\n",
    "                 mask_string=None, mode='left'):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            raw_string: string with raw text in it\n",
    "            split_expression: Regex string or callable. If regex string, will be used with re.split.\n",
    "                If callable, the function should return a list of tokens.\n",
    "            bow: if True, a word is the same everywhere in the text - i.e. we\n",
    "                 will index multiple occurrences of the same word. If False,\n",
    "                 order matters, so that the same word will have different ids\n",
    "                 according to position.\n",
    "            mask_string: If not None, replace words with this if bow=False\n",
    "                if None, default value is UNKWORDZ\n",
    "        \"\"\"\n",
    "        self.raw = raw_string\n",
    "        self.mask_string = 'UNKWORDZ' if mask_string is None else mask_string\n",
    "\n",
    "        if callable(split_expression):\n",
    "            tokens = split_expression(self.raw)\n",
    "            self.as_list = self._segment_with_tokens(self.raw, tokens)\n",
    "            tokens = set(tokens)\n",
    "\n",
    "            def non_word(string):\n",
    "                return string not in tokens\n",
    "\n",
    "        else:\n",
    "            # with the split_expression as a non-capturing group (?:), we don't need to filter out\n",
    "            # the separator character from the split results.\n",
    "            splitter = re.compile(r'(%s)|$' % split_expression)\n",
    "            self.as_list = [s for s in splitter.split(self.raw) if s]\n",
    "            non_word = splitter.match\n",
    "\n",
    "        self.as_np = np.array(self.as_list)\n",
    "        self.string_start = np.hstack(\n",
    "            ([0], np.cumsum([len(x) for x in self.as_np[:-1]])))\n",
    "        vocab = {}\n",
    "        self.inverse_vocab = []\n",
    "        self.positions = []\n",
    "        self.bow = bow\n",
    "\n",
    "        #added in lime22\n",
    "        self.sep = None\n",
    "        self.mode = mode\n",
    "\n",
    "        #added in lime22\n",
    "        vocab_right = {}\n",
    "        self.inverse_vocab_right = []\n",
    "        self.positions_right = []\n",
    "\n",
    "        non_vocab = set()\n",
    "        for i, word in enumerate(self.as_np):\n",
    "            if \"$&*&*&$\" in word:\n",
    "                self.sep = len(self.inverse_vocab)\n",
    "            if word in non_vocab:\n",
    "                continue\n",
    "            if non_word(word):\n",
    "                non_vocab.add(word)\n",
    "                continue\n",
    "            if bow:\n",
    "                #added in lime22\n",
    "                if not self.sep:\n",
    "                    if word not in vocab:\n",
    "                        vocab[word] = len(vocab)\n",
    "                        self.inverse_vocab.append(word)\n",
    "                        self.positions.append([])\n",
    "                    idx_word = vocab[word]\n",
    "                    #print(1, idx_word)\n",
    "                    self.positions[idx_word].append(i)\n",
    "                else:\n",
    "                    if word not in vocab_right:\n",
    "                        vocab_right[word] = len(vocab_right)\n",
    "                        self.inverse_vocab_right.append(word)\n",
    "                        self.positions_right.append([])\n",
    "                    idx_word = vocab_right[word]\n",
    "                    #print(2, idx_word)\n",
    "                    self.positions_right[idx_word].append(i)\n",
    "                #was originally\n",
    "                \"\"\"if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "                    self.inverse_vocab.append(word)\n",
    "                    self.positions.append([])\n",
    "                idx_word = vocab[word]\n",
    "                self.positions[idx_word].append(i)\"\"\"\n",
    "            else:\n",
    "                self.inverse_vocab.append(word)\n",
    "                self.positions.append(i)\n",
    "        if not bow:\n",
    "            self.positions = np.array(self.positions)\n",
    "\n",
    "        if not self.sep:\n",
    "            self.sep = len(self.inverse_vocab)\n",
    "\n",
    "    def return_sep(self):\n",
    "        \"\"\"Return the index of the separator sequence\"\"\"\n",
    "        return self.sep\n",
    "\n",
    "    def raw_string(self):\n",
    "        \"\"\"Returns the original raw string\"\"\"\n",
    "        return self.raw\n",
    "\n",
    "    def num_words(self):\n",
    "        \"\"\"Returns the number of tokens in the vocabulary for this document.\"\"\"\n",
    "        #added in lime22\n",
    "        return len(self.inverse_vocab) if not self.inverse_vocab_right else len(self.inverse_vocab) + len(self.inverse_vocab_right)\n",
    "\n",
    "    def word(self, id_):\n",
    "        #print(self.mode)\n",
    "        \"\"\"Returns the word that corresponds to id_ (int)\"\"\"\n",
    "        if not self.inverse_vocab_right:\n",
    "            return self.inverse_vocab[id_]\n",
    "        #added in lime22\n",
    "        else:\n",
    "            if self.mode == 'left':\n",
    "                #print('left', self.inverse_vocab[id_])\n",
    "                return self.inverse_vocab[id_]\n",
    "            if self.mode == 'right':\n",
    "                #print('right', self.inverse_vocab_right[id_])\n",
    "                return self.inverse_vocab_right[id_]\n",
    "            else:\n",
    "                raise ValueError(\"BOW only supports 'left' and 'right' modes\")\n",
    "\n",
    "\n",
    "    def string_position(self, id_):\n",
    "        \"\"\"Returns a np array with indices to id_ (int) occurrences\"\"\"\n",
    "        if self.bow:\n",
    "            #added in lime22\n",
    "            if self.mode == 'left':\n",
    "                #print(self.string_start[self.positions[id_]])\n",
    "                return self.string_start[self.positions[id_]]\n",
    "            if self.mode == 'right':\n",
    "                #print(self.string_start[self.positions_right[id_]])\n",
    "                return self.string_start[self.positions_right[id_]]\n",
    "        else:\n",
    "            return self.string_start[[self.positions[id_]]]\n",
    "\n",
    "    def inverse_removing(self, words_to_remove):\n",
    "        \"\"\"Returns a string after removing the appropriate words.\n",
    "        If self.bow is false, replaces word with UNKWORDZ instead of removing\n",
    "        it.\n",
    "        Args:\n",
    "            words_to_remove: list of ids (ints) to remove\n",
    "        Returns:\n",
    "            original raw string with appropriate words removed.\n",
    "        \"\"\"\n",
    "        mask = np.ones(self.as_np.shape[0], dtype='bool')\n",
    "        mask[self.__get_idxs(words_to_remove)] = False\n",
    "        if not self.bow:\n",
    "            return ''.join(\n",
    "                [self.as_list[i] if mask[i] else self.mask_string\n",
    "                 for i in range(mask.shape[0])])\n",
    "        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n",
    "\n",
    "    #added in lime22\n",
    "    def inverse_removing_modes(self, words_to_remove, mode='left'):\n",
    "        \"\"\"Returns a string after removing the appropriate words.\n",
    "        Removes the anappropriate words from the text BEFIORE ('left') or AFTER\n",
    "        ('right') the separator\n",
    "        If self.bow is false, replaces word with UNKWORDZ instead of removing\n",
    "        it.\n",
    "        Args:\n",
    "            words_to_remove: list of ids (ints) to remove\n",
    "        Returns:\n",
    "            original raw string with appropriate words removed.\n",
    "        \"\"\"\n",
    "        mask = np.ones(self.as_np.shape[0], dtype='bool')\n",
    "        if mode == \"left\":\n",
    "            mask[self.__get_idxs(words_to_remove)] = False\n",
    "        elif mode == \"right\":\n",
    "            mask[self.__get_idxs_right(words_to_remove)] = False\n",
    "        if not self.bow:\n",
    "            return ''.join(\n",
    "                [self.as_list[i] if mask[i] else self.mask_string\n",
    "                 for i in range(mask.shape[0])])\n",
    "        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])\n",
    "\n",
    "    @staticmethod\n",
    "    def _segment_with_tokens(text, tokens):\n",
    "        \"\"\"Segment a string around the tokens created by a passed-in tokenizer\"\"\"\n",
    "        list_form = []\n",
    "        text_ptr = 0\n",
    "        for token in tokens:\n",
    "            inter_token_string = []\n",
    "            while not text[text_ptr:].startswith(token):\n",
    "                inter_token_string.append(text[text_ptr])\n",
    "                text_ptr += 1\n",
    "                if text_ptr >= len(text):\n",
    "                    raise ValueError(\"Tokenization produced tokens that do not belong in string!\")\n",
    "            text_ptr += len(token)\n",
    "            if inter_token_string:\n",
    "                list_form.append(''.join(inter_token_string))\n",
    "            list_form.append(token)\n",
    "        if text_ptr < len(text):\n",
    "            list_form.append(text[text_ptr:])\n",
    "        return list_form\n",
    "\n",
    "    def __get_idxs(self, words):\n",
    "        \"\"\"Returns indexes to appropriate words.\"\"\"\n",
    "        if self.bow:\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [self.positions[z] for z in words]))\n",
    "        else:\n",
    "            return self.positions[words]\n",
    "\n",
    "    #added in lime22\n",
    "    def __get_idxs_right(self, words):\n",
    "        \"\"\"Returns indexes to appropriate words for the text after the separator.\"\"\"\n",
    "        if self.bow:\n",
    "            return list(itertools.chain.from_iterable(\n",
    "                [self.positions_right[z] for z in words]))\n",
    "        else:\n",
    "            return self.positions_right[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from lime.exceptions import LimeError\n",
    "import lime.explanation\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def id_generator(size=15, random_state=None):\n",
    "    \"\"\"Helper function to generate random div ids. This is useful for embedding\n",
    "    HTML into ipython notebooks.\"\"\"\n",
    "    chars = list(string.ascii_uppercase + string.digits)\n",
    "    return ''.join(random_state.choice(chars, size, replace=True))\n",
    "\n",
    "\n",
    "class DomainMapper(object):\n",
    "    \"\"\"Class for mapping features to the specific domain.\n",
    "    The idea is that there would be a subclass for each domain (text, tables,\n",
    "    images, etc), so that we can have a general Explanation class, and separate\n",
    "    out the specifics of visualizing features in here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map_exp_ids(self, exp, **kwargs):\n",
    "        \"\"\"Maps the feature ids to concrete names.\n",
    "        Default behaviour is the identity function. Subclasses can implement\n",
    "        this as they see fit.\n",
    "        Args:\n",
    "            exp: list of tuples [(id, weight), (id,weight)]\n",
    "            kwargs: optional keyword arguments\n",
    "        Returns:\n",
    "            exp: list of tuples [(name, weight), (name, weight)...]\n",
    "        \"\"\"\n",
    "        return exp\n",
    "\n",
    "    def visualize_instance_html(self,\n",
    "                                exp,\n",
    "                                label,\n",
    "                                div_name,\n",
    "                                exp_object_name,\n",
    "                                **kwargs):\n",
    "        \"\"\"Produces html for visualizing the instance.\n",
    "        Default behaviour does nothing. Subclasses can implement this as they\n",
    "        see fit.\n",
    "        Args:\n",
    "             exp: list of tuples [(id, weight), (id,weight)]\n",
    "             label: label id (integer)\n",
    "             div_name: name of div object to be used for rendering(in js)\n",
    "             exp_object_name: name of js explanation object\n",
    "             kwargs: optional keyword arguments\n",
    "        Returns:\n",
    "             js code for visualizing the instance\n",
    "        \"\"\"\n",
    "        return ''\n",
    "\n",
    "\n",
    "class MyExplanation(object):\n",
    "    \"\"\"Object returned by explainers.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 domain_mapper,\n",
    "                 mode='classification',\n",
    "                 class_names=None,\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Initializer.\n",
    "        Args:\n",
    "            domain_mapper: must inherit from DomainMapper class\n",
    "            type: \"classification\" or \"regression\"\n",
    "            class_names: list of class names (only used for classification)\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.mode = mode\n",
    "        self.domain_mapper = domain_mapper\n",
    "        self.local_exp = {}\n",
    "        self.intercept = {}\n",
    "        self.score = {}\n",
    "        self.local_pred = {}\n",
    "        if mode == 'classification':\n",
    "            self.class_names = class_names\n",
    "            self.top_labels = None\n",
    "            self.predict_proba = None\n",
    "        elif mode == 'regression':\n",
    "            self.class_names = ['negative', 'positive']\n",
    "            self.predicted_value = None\n",
    "            self.min_value = 0.0\n",
    "            self.max_value = 1.0\n",
    "            self.dummy_label = 1\n",
    "        else:\n",
    "            raise LimeError('Invalid explanation mode \"{}\". '\n",
    "                            'Should be either \"classification\" '\n",
    "                            'or \"regression\".'.format(mode))\n",
    "\n",
    "    def available_labels(self):\n",
    "        \"\"\"\n",
    "        Returns the list of classification labels for which we have any explanations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            assert self.mode == \"classification\"\n",
    "        except AssertionError:\n",
    "            raise NotImplementedError('Not supported for regression explanations.')\n",
    "        else:\n",
    "            ans = self.top_labels if self.top_labels else self.local_exp.keys()\n",
    "            return list(ans)\n",
    "\n",
    "    def as_list(self, label=1, **kwargs):\n",
    "        \"\"\"Returns the explanation as a list.\n",
    "        Args:\n",
    "            label: desired label. If you ask for a label for which an\n",
    "                explanation wasn't computed, will throw an exception.\n",
    "                Will be ignored for regression explanations.\n",
    "            kwargs: keyword arguments, passed to domain_mapper\n",
    "        Returns:\n",
    "            list of tuples (representation, weight), where representation is\n",
    "            given by domain_mapper. Weight is a float.\n",
    "        \"\"\"\n",
    "        label_to_use = label if self.mode == \"classification\" else self.dummy_label\n",
    "        ans = self.domain_mapper.map_exp_ids(self.local_exp[label_to_use], **kwargs)\n",
    "        ans = [(x[0], float(x[1])) for x in ans]\n",
    "        return ans\n",
    "\n",
    "    def as_map(self):\n",
    "        \"\"\"Returns the map of explanations.\n",
    "        Returns:\n",
    "            Map from label to list of tuples (feature_id, weight).\n",
    "        \"\"\"\n",
    "        return self.local_exp\n",
    "\n",
    "    def as_pyplot_figure(self, label=1, figsize=(4,4), **kwargs):\n",
    "        \"\"\"Returns the explanation as a pyplot figure.\n",
    "        Will throw an error if you don't have matplotlib installed\n",
    "        Args:\n",
    "            label: desired label. If you ask for a label for which an\n",
    "                   explanation wasn't computed, will throw an exception.\n",
    "                   Will be ignored for regression explanations.\n",
    "            figsize: desired size of pyplot in tuple format, defaults to (4,4).\n",
    "            kwargs: keyword arguments, passed to domain_mapper\n",
    "        Returns:\n",
    "            pyplot figure (barchart).\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        exp = self.as_list(label=label, **kwargs)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        vals = [x[1] for x in exp]\n",
    "        names = [x[0] for x in exp]\n",
    "        vals.reverse()\n",
    "        names.reverse()\n",
    "        colors = ['green' if x > 0 else 'red' for x in vals]\n",
    "        pos = np.arange(len(exp)) + .5\n",
    "        plt.barh(pos, vals, align='center', color=colors)\n",
    "        plt.yticks(pos, names)\n",
    "        if self.mode == \"classification\":\n",
    "            title = 'Local explanation for class %s' % self.class_names[label]\n",
    "        else:\n",
    "            title = 'Local explanation'\n",
    "        plt.title(title)\n",
    "        return fig\n",
    "\n",
    "    def show_in_notebook(self,\n",
    "                         labels=None,\n",
    "                         predict_proba=True,\n",
    "                         show_predicted_value=True,\n",
    "                         **kwargs):\n",
    "        \"\"\"Shows html explanation in ipython notebook.\n",
    "        See as_html() for parameters.\n",
    "        This will throw an error if you don't have IPython installed\"\"\"\n",
    "\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(self.as_html(labels=labels,\n",
    "                                  predict_proba=predict_proba,\n",
    "                                  show_predicted_value=show_predicted_value,\n",
    "                                  **kwargs)))\n",
    "\n",
    "    def save_to_file(self,\n",
    "                     file_path,\n",
    "                     labels=None,\n",
    "                     predict_proba=True,\n",
    "                     show_predicted_value=True,\n",
    "                     **kwargs):\n",
    "        \"\"\"Saves html explanation to file. .\n",
    "        Params:\n",
    "            file_path: file to save explanations to\n",
    "        See as_html() for additional parameters.\n",
    "        \"\"\"\n",
    "        file_ = open(file_path, 'w', encoding='utf8')\n",
    "        file_.write(self.as_html(labels=labels,\n",
    "                                 predict_proba=predict_proba,\n",
    "                                 show_predicted_value=show_predicted_value,\n",
    "                                 **kwargs))\n",
    "        file_.close()\n",
    "\n",
    "    def as_html(self,\n",
    "                labels=None,\n",
    "                predict_proba=True,\n",
    "                show_predicted_value=True,\n",
    "                **kwargs):\n",
    "        \"\"\"Returns the explanation as an html page.\n",
    "        Args:\n",
    "            labels: desired labels to show explanations for (as barcharts).\n",
    "                If you ask for a label for which an explanation wasn't\n",
    "                computed, will throw an exception. If None, will show\n",
    "                explanations for all available labels. (only used for classification)\n",
    "            predict_proba: if true, add  barchart with prediction probabilities\n",
    "                for the top classes. (only used for classification)\n",
    "            show_predicted_value: if true, add  barchart with expected value\n",
    "                (only used for regression)\n",
    "            kwargs: keyword arguments, passed to domain_mapper\n",
    "        Returns:\n",
    "            code for an html page, including javascript includes.\n",
    "        \"\"\"\n",
    "\n",
    "        def jsonize(x):\n",
    "            return json.dumps(x, ensure_ascii=False)\n",
    "\n",
    "        if labels is None and self.mode == \"classification\":\n",
    "            labels = self.available_labels()\n",
    "\n",
    "        this_dir, _ = os.path.split(lime.explanation.__file__)\n",
    "        bundle = open(os.path.join(this_dir, 'bundle.js'),\n",
    "                      encoding=\"utf8\").read()\n",
    "\n",
    "        out = u'''<html>\n",
    "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF8\">\n",
    "        <head><script>%s </script></head><body>''' % bundle\n",
    "        random_id = id_generator(size=15, random_state=check_random_state(self.random_state))\n",
    "        out += u'''\n",
    "        <div class=\"lime top_div\" id=\"top_div%s\"></div>\n",
    "        ''' % random_id\n",
    "\n",
    "        predict_proba_js = ''\n",
    "        if self.mode == \"classification\" and predict_proba:\n",
    "            predict_proba_js = u'''\n",
    "            var pp_div = top_div.append('div')\n",
    "                                .classed('lime predict_proba', true);\n",
    "            var pp_svg = pp_div.append('svg').style('width', '100%%');\n",
    "            var pp = new lime.PredictProba(pp_svg, %s, %s);\n",
    "            ''' % (jsonize([str(x) for x in self.class_names]),\n",
    "                   jsonize(list(self.predict_proba.astype(float))))\n",
    "\n",
    "        predict_value_js = ''\n",
    "        if self.mode == \"regression\" and show_predicted_value:\n",
    "            # reference self.predicted_value\n",
    "            # (svg, predicted_value, min_value, max_value)\n",
    "            predict_value_js = u'''\n",
    "                    var pp_div = top_div.append('div')\n",
    "                                        .classed('lime predicted_value', true);\n",
    "                    var pp_svg = pp_div.append('svg').style('width', '100%%');\n",
    "                    var pp = new lime.PredictedValue(pp_svg, %s, %s, %s);\n",
    "                    ''' % (jsonize(float(self.predicted_value)),\n",
    "                           jsonize(float(self.min_value)),\n",
    "                           jsonize(float(self.max_value)))\n",
    "\n",
    "        exp_js = '''var exp_div;\n",
    "            var exp = new lime.Explanation(%s);\n",
    "        ''' % (jsonize([str(x) for x in self.class_names]))\n",
    "\n",
    "        if self.mode == \"classification\":\n",
    "            for label in labels:\n",
    "                exp = jsonize(self.as_list(label))\n",
    "                exp_js += u'''\n",
    "                exp_div = top_div.append('div').classed('lime explanation', true);\n",
    "                exp.show(%s, %d, exp_div);\n",
    "                ''' % (exp, label)\n",
    "        else:\n",
    "            exp = jsonize(self.as_list())\n",
    "            exp_js += u'''\n",
    "            exp_div = top_div.append('div').classed('lime explanation', true);\n",
    "            exp.show(%s, %s, exp_div);\n",
    "            ''' % (exp, self.dummy_label)\n",
    "\n",
    "        raw_js = '''var raw_div = top_div.append('div');'''\n",
    "\n",
    "        if self.mode == \"classification\":\n",
    "            html_data = self.local_exp[labels[0]]\n",
    "        else:\n",
    "            html_data = self.local_exp[self.dummy_label]\n",
    "\n",
    "        raw_js += self.domain_mapper.visualize_instance_html(\n",
    "                html_data,\n",
    "                labels[0] if self.mode == \"classification\" else self.dummy_label,\n",
    "                'raw_div',\n",
    "                'exp',\n",
    "                **kwargs)\n",
    "        out += u'''\n",
    "        <script>\n",
    "        var top_div = d3.select('#top_div%s').classed('lime top_div', true);\n",
    "        %s\n",
    "        %s\n",
    "        %s\n",
    "        %s\n",
    "        </script>\n",
    "        ''' % (random_id, predict_proba_js, predict_value_js, exp_js, raw_js)\n",
    "        out += u'</body></html>'\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b2b79d7c925dc537986e7099a9668dba419b885563f1cfe4e7cfa1327f89933"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
