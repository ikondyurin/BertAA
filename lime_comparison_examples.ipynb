{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple text classification model to apply LIME and totwoLIME explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix,roc_curve,auc\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read the data\n",
    "df_train=pd.read_csv(\"/Users/ivankondyurin/Downloads/nlp-getting-started/train.csv\")\n",
    "df_test=pd.read_csv(\"/Users/ivankondyurin/Downloads/nlp-getting-started/test.csv\")\n",
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text=' '.join([i for i in text.split() if i not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "# function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "def finalpreprocess(text):\n",
    "    return lemmatizer(preprocess(text))\n",
    "df_train['cleaned_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\n",
    "\n",
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train[\"cleaned_text\"],df_train[\"target\"],test_size=0.2, shuffle=True)\n",
    "\n",
    "#TF-IDF\n",
    "# Convert x_train to vector\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "#model\n",
    "model=RandomForestClassifier(n_estimators = 100, random_state = 10)\n",
    "model.fit(X_train_vectors_tfidf, y_train) \n",
    "#Predict y value for test dataset\n",
    "y_pred = model.predict(X_val_vectors_tfidf)\n",
    "y_prob = model.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_val,y_pred))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_pred))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LIME import necessary packages\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import IndexedString,IndexedCharacters\n",
    "from lime.lime_base import LimeBase\n",
    "from sklearn.linear_model import Ridge, lars_path\n",
    "from lime.lime_text import explanation\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Explaining the predictions and important features for predicting the label 1\n",
    "c = make_pipeline(tfidf_vectorizer, model)\n",
    "explainer = LimeTextExplainer(class_names=model.classes_)\n",
    "# classifier_fn is the probability function that takes a string and returns prediction probabilities.\n",
    "# num_features is the max. number of features we want in the explanation(default is 10).\n",
    "# labels=(1,) means we want the explanation for the label 1\n",
    "exp = explainer.explain_instance(X_val.iloc[20], c.predict_proba, num_features=5,labels=(1,))\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.iloc[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the explanations of standard LIME and totwoLIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Perturbed samples are created in the neighbourhood of the instance of interest. &&  nice job? good job calgary transit, co http co rgoguyt lf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_standard = LimeTextExplainer(class_names=model.classes_, bow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_standard = explainer_standard.explain_instance(text, classifier_fn=c.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_standard.show_in_notebook(text = True)\n",
    "#standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_standard.show_in_notebook(text = True)\n",
    "#standatd with bow=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_totwo = MyLimeTextExplainer(class_names=model.classes_, bow=False, mode='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_totwo = explainer_totwo.explain_instance(text, classifier_fn=c.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_totwo.show_in_notebook(text = True)\n",
    "#right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_totwo.show_in_notebook(text = True)\n",
    "#left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_totwo.show_in_notebook(text = True)\n",
    "#rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore how the explanation is constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perturbed samples are created in the neighbourhood of the instance of interest.\n",
    "# classifier_fn is the probability function that takes a string and returns prediction probabilities.\n",
    "# 5000 samples are created in the neighbourhood as default.\n",
    "# Cosine distance is computed to calculate the distance between original and perturbed samples(default).\n",
    "data,yss,distances=explainer._LimeTextExplainer__data_labels_distances(IndexedString(X_val.iloc[20]),classifier_fn=c.predict_proba,num_samples=5000)\n",
    "## Top 2 closest perturbed samples\n",
    "df=pd.DataFrame(distances,columns=['distance'])\n",
    "df1=df.sort_values(by='distance')\n",
    "req_index=df1.index[1:3]\n",
    "closest_perturbed_sample=[]\n",
    "for k in req_index:\n",
    "    perturbed_text =' '.join([re.split(r'\\W+',X_val.iloc[20])[i] for i,x in enumerate(data[k]) if x==1.0])\n",
    "    closest_perturbed_sample.append(perturbed_text)\n",
    "closest_perturbed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3,yss3,distances3=explainer._MyLimeTextExplainer__data_labels_distances(MyIndexedString(text, bow = False),classifier_fn=c.predict_proba,num_samples=5000,mode='rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Giving weightage to the perturbed samples\n",
    "# Exponential kernel\n",
    "def kernel(d, kernel_width):\n",
    "    return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "# exponential kernel with kernel width 25\n",
    "kernel_fn = partial(kernel, kernel_width=25)\n",
    "# Samples weight using exponential kernel\n",
    "weights=kernel_fn(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_base import LimeBase\n",
    "\n",
    "local_model=LimeBase(kernel_fn, verbose=False)\n",
    "# method is the method of selecting the features.\n",
    "# data is the perturbed samples that are created\n",
    "# labels_column is the label for which we want the explanation\n",
    "# weights is the weights that are given by the exponential kernel to the perturbed samples\n",
    "# num_features is the max. number of features we need in the explanation\n",
    "labels_column = yss[:, 1]\n",
    "used_features=local_model.feature_selection(data,labels_column,weights,num_features=5,method='auto')\n",
    "used_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After getting the features Ridge regression is used to fit the local model as default\n",
    "from sklearn.linear_model import Ridge, lars_path\n",
    "\n",
    "\n",
    "model_regressor = Ridge(alpha=1, fit_intercept=True)\n",
    "                                \n",
    "easy_model = model_regressor\n",
    "easy_model.fit(data[:, used_features],\n",
    "               labels_column, sample_weight=weights)\n",
    "prediction_score = easy_model.score(\n",
    "    data[:, used_features],\n",
    "    labels_column, sample_weight=weights)\n",
    "\n",
    "local_pred = easy_model.predict(data[0, used_features].reshape(1, -1))\n",
    "\n",
    "# final output \n",
    "l = local_model.explain_instance_with_data(data,yss,distances,label=1,num_features=5,feature_selection='highest_weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_mapper = TextDomainMapper(IndexedString(X_val.iloc[20], bow=False))\n",
    "random_state = numpy.random.RandomState()\n",
    "class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "\n",
    "ret_exp = explanation.Explanation(domain_mapper=domain_mapper, class_names=class_names, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import TextDomainMapper\n",
    "from lime import lime_base\n",
    "import numpy\n",
    "\n",
    "def kernel(d, kernel_width):\n",
    "    return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "kernel_fn = partial(kernel, kernel_width=25)\n",
    "\n",
    "random_state = numpy.random.RandomState()\n",
    "\n",
    "base = lime_base.LimeBase(kernel_fn, verbose=False, random_state=random_state)\n",
    "\n",
    "domain_mapper = TextDomainMapper(IndexedString(X_val.iloc[20], bow=False))\n",
    "class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "\n",
    "ret_exp = explanation.Explanation(domain_mapper=domain_mapper, class_names=class_names, random_state=random_state)\n",
    "\n",
    "print(ret_exp.intercept, ret_exp.local_exp, ret_exp.score, ret_exp.local_pred)\n",
    "\n",
    "for label in (1,):\n",
    "    b = base.explain_instance_with_data(data, yss, distances, label, num_features=5, model_regressor=None, feature_selection='auto')\n",
    "\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lime.lime_text import TextDomainMapper\n",
    "from lime import lime_base\n",
    "import numpy\n",
    "\n",
    "def kernel(d, kernel_width):\n",
    "    return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "kernel_fn = partial(kernel, kernel_width=25)\n",
    "\n",
    "random_state = numpy.random.RandomState()\n",
    "\n",
    "base = lime_base.LimeBase(kernel_fn, verbose=False, random_state=random_state)\n",
    "\n",
    "domain_mapper = MyTextDomainMapper(MyIndexedString(text, bow=False))\n",
    "class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "\n",
    "ret_exp = MyExplanation(domain_mapper=domain_mapper, class_names=class_names, random_state=random_state)\n",
    "ret_exp.predict_proba = yss[0]\n",
    "\n",
    "print(ret_exp.intercept, ret_exp.local_exp, ret_exp.score, ret_exp.local_pred)\n",
    "\n",
    "for label in (1,):\n",
    "    b = base.explain_instance_with_data(data3, yss3, distances3, label, num_features=5, model_regressor=None, feature_selection='auto')\n",
    "\n",
    "    print(b)\n",
    "\n",
    "    (ret_exp.intercept[label], ret_exp.local_exp[label], ret_exp.score[label], ret_exp.local_pred[label]) = b\n",
    "\n",
    "print(ret_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_exp.predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_exp.show_in_notebook(text = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perturbed samples are created in the neighbourhood of the instance of interest.\n",
    "# classifier_fn is the probability function that takes a string and returns prediction probabilities.\n",
    "# 5000 samples are created in the neighbourhood as default.\n",
    "# Cosine distance is computed to calculate the distance between original and perturbed samples(default).\n",
    "data,yss,distances=explainer._MyLimeTextExplainer__data_labels_distances(MyIndexedString(text, bow = False),classifier_fn=c.predict_proba,num_samples=5000,mode='rand')\n",
    "## Top 2 closest perturbed samples\n",
    "df=pd.DataFrame(distances,columns=['distance'])\n",
    "df1=df.sort_values(by='distance')\n",
    "req_index=df.index[1:50]\n",
    "closest_perturbed_sample=[]\n",
    "for k in req_index:\n",
    "    perturbed_text =' '.join([re.split(r'\\W+',text)[i] for i,x in enumerate(data[k]) if x==1.0])\n",
    "    closest_perturbed_sample.append(perturbed_text)\n",
    "closest_perturbed_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "96d7c5f1ded37e429f197635c2bd0274735efde1a7acd6754cdde172eae465e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
