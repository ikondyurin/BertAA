{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "\n",
    "#from pan20_verif_evaluator import evaluate_all\n",
    "\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    print(a, b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def rescale(value, orig_min, orig_max, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Rescales a `value` in the old range defined by\n",
    "    `orig_min` and `orig_max`, to the new range\n",
    "    `new_min` and `new_max`. Assumes that\n",
    "    `orig_min` <= value <= `orig_max`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    value: float, default=None\n",
    "        The value to be rescaled.\n",
    "    orig_min: float, default=None\n",
    "        The minimum of the original range.\n",
    "    orig_max: float, default=None\n",
    "        The minimum of the original range.\n",
    "    new_min: float, default=None\n",
    "        The minimum of the new range.\n",
    "    new_max: float, default=None\n",
    "        The minimum of the new range.\n",
    "    Returns\n",
    "    ----------\n",
    "    new_value: float\n",
    "        The rescaled value.\n",
    "    \"\"\"\n",
    "\n",
    "    orig_span = orig_max - orig_min\n",
    "    new_span = new_max - new_min\n",
    "\n",
    "    try:\n",
    "        scaled_value = float(value - orig_min) / float(orig_span)\n",
    "    except ZeroDivisionError:\n",
    "        orig_span += 1e-6\n",
    "        scaled_value = float(value - orig_min) / float(orig_span)\n",
    "\n",
    "    return new_min + (scaled_value * new_span)\n",
    "\n",
    "\n",
    "def correct_scores(scores, p1, p2):\n",
    "    for sc in scores:\n",
    "        if sc <= p1:\n",
    "            yield rescale(sc, 0, p1, 0, 0.49)\n",
    "        elif p1 < sc < p2:\n",
    "            yield 0.5\n",
    "        else:\n",
    "            yield rescale(sc, p2, 1, 0.51, 1)  # np.array(list\n",
    "\n",
    "def test(test_pairs, output_dir, model_directory, num_iterations):\n",
    "    vectorizer = pickle.load(open(model_directory / 'vectorizer.pickle', 'rb'))\n",
    "    opt_p1 = pickle.load(open(model_directory / 'opt_p1.pickle', 'rb'))\n",
    "    opt_p2 = pickle.load(open(model_directory / 'opt_p2.pickle', 'rb'))\n",
    "    if num_iterations:\n",
    "        rnd_feature_idxs = pickle.load(open(model_directory / 'rnd_feature_idxs.pickle', 'rb'))\n",
    "\n",
    "    print('-> calculating test similarities')\n",
    "    with open(output_dir / 'answers.jsonl', 'w') as outf:\n",
    "        for line in open(test_pairs):\n",
    "            d = json.loads(line.strip())\n",
    "            problem_id = d['id']\n",
    "            x1, x2 = vectorizer.transform(d['pair']).toarray()\n",
    "            if num_iterations:\n",
    "                similarities_ = []\n",
    "                for i in range(num_iterations):\n",
    "                    similarities_.append(cosine_sim(x1[rnd_feature_idxs[i, :]],\n",
    "                                                    x2[rnd_feature_idxs[i, :]]))\n",
    "                    similarity = np.mean(similarities_)\n",
    "            else:\n",
    "                similarity = cosine_sim(x1, x2)\n",
    "\n",
    "            similarity = np.array(list(correct_scores([similarity], p1=opt_p1, p2=opt_p2)))[0]\n",
    "            r = {'id': problem_id, 'value': similarity}\n",
    "            outf.write(json.dumps(r) + '\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_base(text, num_iterations=0, model_directory=\"C:\\\\Users\\\\ivank\\\\OneDrive\\\\clef21\\\\authorship-verification\\\\mod\"):\n",
    "\n",
    "    vectorizer = pickle.load(open(model_directory + '\\\\vectorizer.pickle', 'rb'))\n",
    "    opt_p1 = pickle.load(open(model_directory + '\\\\opt_p1.pickle', 'rb'))\n",
    "    opt_p2 = pickle.load(open(model_directory + '\\\\opt_p2.pickle', 'rb'))\n",
    "\n",
    "    if num_iterations:\n",
    "        rnd_feature_idxs = pickle.load(open(model_directory + '\\\\rnd_feature_idxs.pickle', 'rb'))\n",
    "\n",
    "    print('-> calculating test similarities')\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        predictions = []\n",
    "        print('буль')\n",
    "        for text_variant in text:\n",
    "            text1, text2 = text_variant.split(\"$&*&*&$\")\n",
    "            text_list = [text1, text2]\n",
    "            x1, x2 = vectorizer.transform(text_list).toarray()\n",
    "            if num_iterations:\n",
    "                similarities_ = []\n",
    "                for i in range(num_iterations):\n",
    "                    similarities_.append(cosine_sim(x1[rnd_feature_idxs[i, :]],\n",
    "                                                    x2[rnd_feature_idxs[i, :]]))\n",
    "                    similarity = np.mean(similarities_)\n",
    "            else:\n",
    "                similarity = cosine_sim(x1, x2)\n",
    "            similarity = np.array(list(correct_scores([similarity], p1=opt_p1, p2=opt_p2)))[0]\n",
    "            similarity = similarity.astype('float32')\n",
    "            probabilities = np.array([1-similarity,similarity])\n",
    "            predictions.append(probabilities)\n",
    "        return(np.array(predictions))#, type(predictions), predictions.size)\n",
    "\n",
    "    else:\n",
    "        text1, text2 = text.split(\"$&*&*&$\")\n",
    "        text_list = [text1, text2]\n",
    "        x1, x2 = vectorizer.transform(text_list).toarray()\n",
    "        if num_iterations:\n",
    "            similarities_ = []\n",
    "            for i in range(num_iterations):\n",
    "                similarities_.append(cosine_sim(x1[rnd_feature_idxs[i, :]],\n",
    "                                                x2[rnd_feature_idxs[i, :]]))\n",
    "                similarity = np.mean(similarities_)\n",
    "        else:\n",
    "            similarity = cosine_sim(x1, x2)\n",
    "        similarity = np.array(list(correct_scores([similarity], p1=opt_p1, p2=opt_p2)))[0]\n",
    "        similarity = similarity.astype('float32')\n",
    "        probabilities = np.array([1-similarity,similarity])\n",
    "        return(probabilities)#, type(probabilities), probabilities.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"textcomb4.txt\", 'r') as textcomb:\n",
    "    textcomb=textcomb.read()\n",
    "    p = pipeline_base(textcomb, num_iterations = 4)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcomb = open(\"textcomb3.txt\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(text_instance=textcomb, classifier_fn=pipeline_base, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_iter = explainer.explain_instance(text_instance=textcomb, classifier_fn=pipeline_base, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = explainer.explain_instance(text_instance=textcomb, classifier_fn=pipeline_base, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2 = explainer.explain_instance(text_instance=textcomb, classifier_fn=pipeline_base, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3 = explainer.explain_instance(text_instance=textcomb, classifier_fn=pipeline_base, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_iter.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3.show_in_notebook(text=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7263f0799e12b35b542d5d86c0de34b5ed7166764fe9e965271e4e6fb43a61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
