{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pipelines for different formats of prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# following function is adopted from bert4keras package (https://github.com/bojone/bert4keras)\n",
    "# we do not import this package to avoid compadibility issues (keras < 2.3.1 is required for this package, while a later version is already used)\n",
    "# if bert4keras package is already installed, this function can be loaded as follows:\n",
    "# from bert4keras.snippets import text_segmentate\n",
    "\n",
    "def text_segmentate(text, maxlen, seps='\\n', strips=None):\n",
    "    \"\"\"将文本按照标点符号划分为若干个短句\n",
    "    \"\"\"\n",
    "    text = text.strip().strip(strips)\n",
    "    if seps and len(text) > maxlen:\n",
    "        pieces = text.split(seps[0])\n",
    "        text, texts = '', []\n",
    "        for i, p in enumerate(pieces):\n",
    "            if text and p and len(text) + len(p) > maxlen - 1:\n",
    "                texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "                text = ''\n",
    "            if i + 1 == len(pieces):\n",
    "                text = text + p\n",
    "            else:\n",
    "                text = text + p + seps[0]\n",
    "        if text:\n",
    "            texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "        return texts\n",
    "    else:\n",
    "        return [text]\n",
    "\n",
    "\n",
    "# following function is adopted from https://github.com/Pzeyang/task-for-authorship-verification\n",
    "# a custom version tailored to our project will be added later\n",
    "\n",
    "def get_data(jsonl_dataset_path):\n",
    "    \"\"\"\n",
    "    Get data from JSONL dataset. Used in plain_pipeline and pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    with open(jsonl_dataset_path, 'r') as f:\n",
    "\n",
    "        datas = []\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            text1 = text_segmentate(data['pair'][0], maxlen=510, seps='.?!')\n",
    "            text2 = text_segmentate(data['pair'][1], maxlen=510, seps='.?!')\n",
    "            while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "            datas.append((text1, text2, str(data['id'])))\n",
    "\n",
    "        return datas\n",
    "\n",
    "# different data extractors for different types of input. See description to find in which pipeline each one should be used\n",
    "\n",
    "def get_data_from_two_textfiles(text1_path, text2_path):\n",
    "    \"\"\"\n",
    "    Get data from a two text files, one for each fragment. Used in pipeline \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from raw texts\")\n",
    "\n",
    "    datas = []\n",
    "    with open(text1_path, 'r') as text1, open(text2_path, 'r') as text2:\n",
    "        text1, text2 = text1.read(), text2.read()\n",
    "        text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "        text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "        while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "        datas.append((text1, text2))\n",
    "\n",
    "    return datas\n",
    "\n",
    "def get_data_from_single_textfile(text_path):\n",
    "    \"\"\"\n",
    "    Get data from a text file that contains two texts and a separator. Currently not used\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from single raw text file\")\n",
    "\n",
    "    datas = []\n",
    "    with open(text_path, 'r') as text:\n",
    "        text = text.read()\n",
    "        text1, text2 = text.split(\"$&*&*&$\")\n",
    "        text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "        text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "        while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "        datas.append((text1, text2))\n",
    "\n",
    "    return datas\n",
    "\n",
    "def get_data_from_combined_texts(text_or_list):\n",
    "    \"\"\"\n",
    "    Get data from raw text that contains two fragments and a separater, or from a list of texts,\n",
    "    each of them containing two fragments and a separater. Used in pipeline_onetext. The ONLY type\n",
    "    of data processor for LIME inputs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from raw text\")\n",
    "\n",
    "    datas = []\n",
    "\n",
    "    print(type(text_or_list), len(text_or_list))\n",
    "    if not isinstance(text_or_list, str):\n",
    "        for text_variant in text_or_list:\n",
    "            text1, text2 = text_variant.split(\"$&*&*&$\")\n",
    "            text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "            text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "            while len(text1) < 30 or len(text2) < 30:\n",
    "                    if len(text1) < 30:\n",
    "                        n_text1 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text1:\n",
    "                                n_text1.append(sent)\n",
    "                        text1 = n_text1\n",
    "                    elif len(text2) < 30:\n",
    "                        n_text2 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text2:\n",
    "                                n_text2.append(sent)\n",
    "                        text2 = n_text2\n",
    "            datas.append((text1, text2))\n",
    "    else:\n",
    "        text1, text2 = text_or_list.split(\"$&*&*&$\")\n",
    "        text1 = text_segmentate(text1, maxlen=510, seps='.?!')\n",
    "        text2 = text_segmentate(text2, maxlen=510, seps='.?!')\n",
    "        while len(text1) < 30 or len(text2) < 30:\n",
    "                if len(text1) < 30:\n",
    "                    n_text1 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text1:\n",
    "                            n_text1.append(sent)\n",
    "                    text1 = n_text1\n",
    "                elif len(text2) < 30:\n",
    "                    n_text2 = []\n",
    "                    for i in range(30):\n",
    "                        for sent in text2:\n",
    "                            n_text2.append(sent)\n",
    "                    text2 = n_text2\n",
    "        datas.append((text1, text2))\n",
    "    return datas\n",
    "\n",
    "\n",
    "global tokenizer \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize_function(example):\n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    return tokenizer(example['0'][:30], example['1'][:30], truncation=True, padding='max_length', max_length=255)\n",
    "\n",
    "# used to creat text input for pipelines and explainers\n",
    "def combine_texts(index, write=False):\n",
    "    \"\"\"\n",
    "    Combine a pair of texts from dataset with a separator and turn into a single text\n",
    "    \"\"\"\n",
    "\n",
    "    text1 = orig_data['pair'][index][0]\n",
    "    text2 = orig_data['pair'][index][1]\n",
    "    text_combined = text1 + \"$&*&*&$\" + text2\n",
    "    \n",
    "    if write:\n",
    "        name = \"textcomb{}.txt\".format(index)\n",
    "        with open(name, 'w') as textcomb:\n",
    "            textcomb.write(text_combined)\n",
    "\n",
    "    return(text_combined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a final classifier (identical to FinalNetAvg in Final_model_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalNetAvg(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FinalNetAvg, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 768))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a standard JSONL dataset with 100 pairs and get corresponding truth labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a standard JSONL with a set number of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small.jsonl\", lines = True)\n",
    "df = df.sample(n = 1)\n",
    "df.to_json(\"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small-one.jsonl\", orient='records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load untokenized evaluation set \n",
    "from datasets import load_from_disk\n",
    "df = load_from_disk(\"C:\\\\Users\\\\ivank\\\\Documents\\\\BERT_projects\\\\can_delete\")\n",
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = pd.read_json(\"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small-verysmall.jsonl\", lines = True)\n",
    "all_trues = pd.read_json(\"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small-truth.jsonl\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = pd.merge(orig_data, all_trues, on=['id'], how='inner')\n",
    "trues['same'] = trues['same'].astype(int)\n",
    "labels = trues['same'].array\n",
    "\n",
    "#np.equal(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues.to_csv(\"100 examples to explain.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get input data in custom format (otherwise use combine_text function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a pair of texts from dataset\n",
    "\"\"\"\n",
    "\n",
    "with open(\"text3.txt\", 'w') as text1, open(\"text4.txt\", 'w') as text2:\n",
    "    text1.write(orig_data['pair'][0][0])\n",
    "    text2.write(orig_data['pair'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combine a pair of texts from files with a separator and turn into a single text\n",
    "\"\"\"\n",
    "\n",
    "with open(\"text3.txt\", 'r') as text1, open(\"text4.txt\", 'r') as text2, open(\"textcomb2.txt\", 'w') as textcomb:\n",
    "    text_combined = text1.read() + \"$&*&*&$\" + text2.read()\n",
    "    textcomb.write(text_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments_from_pd(textindex, segmentindex, sep_option=0, write=False):\n",
    "    \"\"\"\n",
    "    Combine a pair of segments from pandas dataset with a separator and turn into a single text\n",
    "    \"\"\"\n",
    "\n",
    "    text1 = df['0'][textindex][segmentindex]\n",
    "    text2 = df['1'][textindex][segmentindex]\n",
    "    sep = \"$&*&*&$\" if sep_option == 0 else \"[SEP]\"\n",
    "    text_combined = text1 + sep + text2\n",
    "    \n",
    "    if write:\n",
    "        name = \"textcomb{}_{}.txt\".format(textindex, segmentindex)\n",
    "        with open(name, 'w') as textcomb:\n",
    "            textcomb.write(text_combined)\n",
    "\n",
    "    return(text_combined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small-one.jsonl\"\n",
    "\n",
    "def plain_pipeline(data_path):\n",
    "    \"\"\"\n",
    "    Pipeline for input from a regular JSONL dataset \n",
    "    \"\"\"\n",
    "\n",
    "    segmented_data = get_data(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['0', '1', '2'])\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\results2\\checkpoint-225000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            if i % 10 == 0:\n",
    "                print(\">{} processing batch {}/{}\".format(i//10*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cls = outputs.hidden_states[-1][:,0,:] # obtain last hidden layer's CLS tokens. [:,0,:] meaning: ':' for all sequences, '0' for first token in sequence, ':' for all 768 hidden layers\n",
    "            eval_outputs = torch.cat((eval_outputs, cls), 0)\n",
    "\n",
    "    eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "\n",
    "    model_classify = FinalNetAvg()\n",
    "    model_classify.load_state_dict(torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Classifier\\model.pth\"))\n",
    "    model_classify.to(device)\n",
    "\n",
    "    logits = model_classify(eval_outputs)\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    print(\"Done!\")\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with input either from regular JSONL dataset (1 argument) or from a pair of texts (2 arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small-one.jsonl\"\n",
    "\n",
    "def pipeline(data_path, data_path2=None, mode='probs'):\n",
    "    \"\"\"\n",
    "    Pipeline with input either from regular JSONL dataset (1 argument) or from a pair of texts (2 arguments)\n",
    "    \"\"\"\n",
    "\n",
    "    segmented_data = get_data_from_two_textfiles(data_path, data_path2) if data_path2 else get_data(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "    \n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\results2\\checkpoint-225000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            if i % 10 == 0:\n",
    "                print(\">{} processing batch {}/{}\".format(i//10*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cls = outputs.hidden_states[-1][:,0,:] # obtain last hidden layer's CLS tokens. [:,0,:] meaning: ':' for all sequences, '0' for first token in sequence, ':' for all 768 hidden layers\n",
    "            eval_outputs = torch.cat((eval_outputs, cls), 0)\n",
    "\n",
    "    eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "\n",
    "    model_classify = FinalNetAvg()\n",
    "    model_classify.load_state_dict(torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Classifier\\model.pth\"))\n",
    "    model_classify.to(device)\n",
    "\n",
    "    model_classify.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model_classify(eval_outputs)\n",
    "    \n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    print(\"Done!\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from raw text\n",
      "Tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1ex [00:00, 333.33ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining embeddings...\n",
      "> processing batch 0/1\n",
      "Making predictions...\n",
      "torch.Size([1, 2])\n",
      "prediction:  tensor([ 8.3187, -4.1354], device='cuda:0')\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "predictions_from_pair = pipeline(\"text1.txt\", \"text2.txt\", mode='probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1ex [00:00, 332.96ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining embeddings...\n",
      "> processing batch 0/1\n",
      "Making predictions...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "predictions_from_jsonl = pipeline(\"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small-one.jsonl\", mode='probs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline with input from a combined text or a list of combined texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_onetext(data_path, mode='probs'):\n",
    "    \"\"\"\n",
    "    Pipeline with input from a combined text or a list of combined texts\n",
    "    \"\"\"\n",
    "\n",
    "    segmented_data = get_data_from_combined_texts(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\BERT\\results_45000\\checkpoint-225000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            print(batch)\n",
    "            step = 10 if (len(eval_dataloader) < 100) else 100\n",
    "            if i % step == 0:\n",
    "                print(\">{} processing item {}/{}\".format(int((i/len(eval_dataloader))*10)*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cls = outputs.hidden_states[-1][:,0,:] # obtain last hidden layer's CLS tokens. [:,0,:] meaning: ':' for all sequences, '0' for first token in sequence, ':' for all 768 hidden layers\n",
    "            eval_outputs = torch.cat((eval_outputs, cls), 0)\n",
    "\n",
    "    eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    #Save the text embedding for future analysis\n",
    "    global embedding\n",
    "    embedding = eval_outputs\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "\n",
    "    model_classify = FinalNetAvg()\n",
    "    model_classify.load_state_dict(torch.load(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\Classifier\\model.pth\"))\n",
    "    model_classify.to(device)\n",
    "\n",
    "    model_classify.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model_classify(eval_outputs)\n",
    "    \n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    print(\"Done!\")\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_combined_segments(text_or_list):\n",
    "    \"\"\"\n",
    "    Get data from raw text that contains two fragments and a separater, or from a list of texts,\n",
    "    each of them containing two fragments and a separater. Used in pipeline_onetext. The ONLY type\n",
    "    of data processor for LIME inputs\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\"Getting data from raw text\")\n",
    "\n",
    "    datas = []\n",
    "\n",
    "    #print(type(text_or_list), len(text_or_list))\n",
    "    if not isinstance(text_or_list, str):\n",
    "        for text_variant in text_or_list:\n",
    "            #print(text_variant)\n",
    "            text1, text2 = text_variant.split(\"$&*&*&$\")\n",
    "            datas.append(([text1], [text2]))\n",
    "    else:\n",
    "        text1, text2 = text_or_list.split(\"$&*&*&$\")\n",
    "        datas.append(([text1], [text2]))\n",
    "    return datas\n",
    "\n",
    "def pipeline_onesegment(data_path, mode='probs'):\n",
    "    \n",
    "    segmented_data = get_data_from_combined_segments(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    #print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\BERT\\results_45000\\checkpoint-225000\")\n",
    "    #model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\BERT\\results_45000\\checkpoint-180000\")\n",
    "    model_feature_extract.to(device)\n",
    "    #print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    global eval_dataloader\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            #print(batch)\n",
    "            #step = 10 if (len(eval_dataloader) < 100) else 100\n",
    "            #if i % step == 0:\n",
    "            #    print(\">{} processing item {}/{}\".format(int((i/len(eval_dataloader))*10)*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cur_logits = outputs.logits\n",
    "            eval_outputs = torch.cat((eval_outputs, cur_logits), 0)\n",
    "\n",
    "    #eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    #Save the text embedding for future analysis\n",
    "    #global logits\n",
    "    logits = eval_outputs\n",
    "\n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    #print(\"Done!\")\n",
    "    return np.array(predictions)\n",
    "\n",
    "def get_data_from_listed_segments(text_or_list):\n",
    "    \"\"\"\n",
    "    Get data from raw text that contains two fragments and a separater, or from a list of texts,\n",
    "    each of them containing two fragments and a separater. Used in pipeline_onetext. The ONLY type\n",
    "    of data processor for LIME inputs\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting data from raw text\")\n",
    "\n",
    "    datas = []\n",
    "\n",
    "    print(type(text_or_list), len(text_or_list))\n",
    "    if not isinstance(text_or_list[0], str):\n",
    "        for text_variant in text_or_list:\n",
    "            #print(text_variant)\n",
    "            text1, text2 = text_variant[0], text_variant[1]\n",
    "            datas.append(([text1], [text2]))\n",
    "    else:\n",
    "        text1, text2 = text_or_list[0], text_or_list[1]\n",
    "        datas.append(([text1], [text2]))\n",
    "    return datas\n",
    "\n",
    "def pipeline_twosegments(data_path, mode='probs'):\n",
    "\n",
    "    print(data_path)\n",
    "    \n",
    "    segmented_data = get_data_from_listed_segments(data_path)\n",
    "    dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "    del segmented_data\n",
    "\n",
    "    print(\"Tokenization...\")\n",
    "\n",
    "    #only ititialize tokenizer if you don't do it before calling the function (which is faster)\n",
    "    #global tokenizer \n",
    "    #tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenized_dataset = dataset.map(tokenize_function)\n",
    "    #print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))\n",
    "\n",
    "    flat_dataset = tokenized_dataset.to_pandas()\n",
    "    flat_dataset = flat_dataset.drop(['0', '1'], axis=1)\n",
    "    if '2' in flat_dataset: #we may or may not have this column depending on the input type\n",
    "         flat_dataset = flat_dataset.drop(['2'], axis=1)\n",
    "    flat_dataset = flat_dataset.explode(['input_ids', 'token_type_ids', \"attention_mask\"]).reset_index(drop=True)\n",
    "    dataset = datasets.Dataset.from_pandas(flat_dataset)\n",
    "\n",
    "    global datacheck\n",
    "    datacheck = flat_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\BERT\\results_45000\\checkpoint-225000\")\n",
    "    #model_feature_extract = transformers.AutoModelForSequenceClassification.from_pretrained(r\"C:\\Users\\ivank\\Documents\\BERT_projects\\BERT\\results_45000\\checkpoint-180000\")\n",
    "    model_feature_extract.to(device)\n",
    "    print(\"Obtaining embeddings...\")\n",
    "\n",
    "    dataset.set_format('torch')\n",
    "    global eval_dataloader\n",
    "    eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=30)\n",
    "\n",
    "    eval_outputs = torch.Tensor()\n",
    "    eval_outputs = eval_outputs.to(device)\n",
    "\n",
    "    model_feature_extract.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(eval_dataloader):\n",
    "            #print(batch)\n",
    "            step = 10 if (len(eval_dataloader) < 100) else 100\n",
    "            if i % step == 0:\n",
    "                print(\">{} processing item {}/{}\".format(int((i/len(eval_dataloader))*10)*\">\", i, len(eval_dataloader))) \n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_feature_extract(**batch, output_hidden_states=True)\n",
    "            cur_logits = outputs.logits\n",
    "            eval_outputs = torch.cat((eval_outputs, cur_logits), 0)\n",
    "\n",
    "    #eval_outputs = torch.reshape(eval_outputs, (len(eval_dataloader), 30, 768))\n",
    "\n",
    "    #Save the text embedding for future analysis\n",
    "    global logits\n",
    "    logits = eval_outputs\n",
    "\n",
    "    predictions = []\n",
    "    for prediction in logits:\n",
    "        if mode == 'labels':\n",
    "            prediction = torch.argmax(prediction, dim=-1)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        elif mode == 'probs':\n",
    "            m = nn.Softmax()\n",
    "            prediction = m(prediction)\n",
    "            prediction = prediction.cpu().numpy()\n",
    "            prediction = np.around(prediction, decimals=3)\n",
    "            #prediction = prediction.tolist()\n",
    "        else:\n",
    "            prediction = prediction.cpu().numpy()\n",
    "        predictions.append(prediction)\n",
    "    print(\"Done!\")\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline_twosegments([segments[0][0][0], segments[0][1][0]], mode='logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1ex [00:00, 499.86ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining embeddings...\n",
      "> processing item 0/1\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.7861366, -4.759954 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = pipeline_onesegment(segm00.replace(\"[SEP]\", \"$&*&*&$\"), mode='logits')\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.subtract(res[0], res[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
