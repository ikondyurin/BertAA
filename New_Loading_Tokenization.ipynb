{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "fvFvBLJV0Dkv",
    "outputId": "140119e5-4cee-4604-c0d2-be279c18b125"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ-42fh0hjsF"
   },
   "source": [
    "## Segmentate the dataset and tokenize it as in Peng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following function is adopted from bert4keras package (https://github.com/bojone/bert4keras)\n",
    "# we do not import this package to avoid compadibility issues (keras < 2.3.1 is required for this package, while a later version is already used)\n",
    "# if bert4keras package is already installed, this function can be loaded as follows:\n",
    "# from bert4keras.snippets import text_segmentate\n",
    "\n",
    "def text_segmentate(text, maxlen, seps='\\n', strips=None):\n",
    "    \"\"\"将文本按照标点符号划分为若干个短句\n",
    "    \"\"\"\n",
    "    text = text.strip().strip(strips)\n",
    "    if seps and len(text) > maxlen:\n",
    "        pieces = text.split(seps[0])\n",
    "        text, texts = '', []\n",
    "        for i, p in enumerate(pieces):\n",
    "            if text and p and len(text) + len(p) > maxlen - 1:\n",
    "                texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "                text = ''\n",
    "            if i + 1 == len(pieces):\n",
    "                text = text + p\n",
    "            else:\n",
    "                text = text + p + seps[0]\n",
    "        if text:\n",
    "            texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
    "        return texts\n",
    "    else:\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                if len(text1) < 30:\n",
    "                        augment = 30 - len(text1)\n",
    "                        n_text1 = []\n",
    "                        for i in range(augment):\n",
    "                            n_text1.append(text1[i])\n",
    "                        text1 = n_text1                     \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following function is adopted from https://github.com/Pzeyang/task-for-authorship-verification\n",
    "# a custom version tailored to our project will be added later\n",
    "\n",
    "def get_data(truth_path, text_path):\n",
    "\n",
    "    truth=[]\n",
    "    with open(truth_path,'r') as f:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            truth.append((data['id'], data['same'], data['authors']))\n",
    "\n",
    "    index=0\n",
    "    counter = 0\n",
    "\n",
    "    with open(text_path,'r') as f:\n",
    "        datas=[]\n",
    "        for l in tqdm(f):\n",
    "            data = json.loads(l)\n",
    "            if truth[index][0]==data['id']:\n",
    "\n",
    "                text1 = text_segmentate(data['pair'][0], maxlen=760, seps='.?!')\n",
    "                text2 = text_segmentate(data['pair'][1], maxlen=760, seps='.?!')\n",
    "\n",
    "                if len(text1) < 30 or len(text2) < 30:\n",
    "                    #return text1, text2, data['pair'][0], data['pair'][1]\n",
    "                    counter +=1\n",
    "\n",
    "                \"\"\"while len(text1) < 30 or len(text2) < 30:\n",
    "                    if len(text1) < 30:\n",
    "                        n_text1 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text1:\n",
    "                                n_text1.append(sent)\n",
    "                        text1 = n_text1\n",
    "                    elif len(text2) < 30:\n",
    "                        n_text2 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text2:\n",
    "                                n_text2.append(sent)\n",
    "                        text2 = n_text2\n",
    "\n",
    "                datas.append((text1, text2, int(truth[index][-2]), str(data['id']), truth[index][-2], truth[index][-1]))\"\"\"\n",
    "\n",
    "            index+=1\n",
    "\n",
    "    print(counter)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(truth_path, text_path):\n",
    "\n",
    "    truth=[]\n",
    "    with open(truth_path,'r') as f:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            truth.append((data['id'], data['same'], data['authors']))\n",
    "\n",
    "    index=0\n",
    "    counter = 0\n",
    "\n",
    "    with open(text_path,'r') as f:\n",
    "        datas=[]\n",
    "        for l in tqdm(f):\n",
    "            data = json.loads(l)\n",
    "            if truth[index][0]==data['id']:\n",
    "\n",
    "                if \"Aranea\" in data['pair'][0]:\n",
    "                    print(truth[index][-1][0], truth[index][-2])\n",
    "\n",
    "                if \"Aranea\" in data['pair'][1]:\n",
    "                    print(truth[index][-1][1], truth[index][-2])\n",
    "\n",
    "                text1 = text_segmentate(data['pair'][0], maxlen=750, seps='.?!;')\n",
    "                text2 = text_segmentate(data['pair'][1], maxlen=750, seps='.?!;')\n",
    "\n",
    "                if len(text1) < 30 or len(text2) < 30:\n",
    "                    #return text1, text2, data['pair'][0], data['pair'][1]\n",
    "                    counter +=1\n",
    "                    \n",
    "                while len(text1) < 30 or len(text2) < 30:\n",
    "                    if len(text1) < 30:\n",
    "                        n_text1 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text1:\n",
    "                                n_text1.append(sent)\n",
    "                        text1 = n_text1\n",
    "                    elif len(text2) < 30:\n",
    "                        n_text2 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text2:\n",
    "                                n_text2.append(sent)\n",
    "                        text2 = n_text2\n",
    "\n",
    "                datas.append((text1, text2, int(truth[index][-2]), str(data['id']), truth[index][-2], truth[index][-1]))\n",
    "\n",
    "            index+=1\n",
    "\n",
    "    print(counter)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(truth_path, text_path):\n",
    "\n",
    "    truth=[]\n",
    "    with open(truth_path,'r') as f:\n",
    "        for l in f:\n",
    "            data = json.loads(l)\n",
    "            truth.append((data['id'], data['same'], data['authors']))\n",
    "\n",
    "    index=0\n",
    "    counter = 0\n",
    "    false = 0\n",
    "    true = 0\n",
    "\n",
    "    with open(text_path,'r') as f:\n",
    "        datas=[]\n",
    "        for l in tqdm(f):\n",
    "            data = json.loads(l)\n",
    "            if truth[index][0]==data['id']:\n",
    "\n",
    "                if \"Clarice\" in data['pair'][0]:\n",
    "                    print(truth[index][-1][0], truth[index][-2], data['fandoms'][0])\n",
    "                    counter+=1\n",
    "                    if truth[index][-2]:\n",
    "                        true += 1\n",
    "                    else:\n",
    "                        false += 1\n",
    "\n",
    "                if \"Clarice\" in data['pair'][1]:\n",
    "                    print(truth[index][-1][1], truth[index][-2], data['fandoms'][1])\n",
    "                    counter+=1\n",
    "                    if truth[index][-2]:\n",
    "                        true += 1\n",
    "                    else:\n",
    "                        false += 1\n",
    "                \"\"\"if (truth[index][-1][0] == '1892519'):\n",
    "                    print(0, truth[index][-1], data['fandoms'][0], data['id'])\n",
    "\n",
    "                if (truth[index][-1][1] == '1892519'):\n",
    "                    print(1, truth[index][-1], data['fandoms'][1], data['id'])\"\"\"\n",
    "\n",
    "                \"\"\"text1 = text_segmentate(data['pair'][0], maxlen=750, seps='.?!;')\n",
    "                text2 = text_segmentate(data['pair'][1], maxlen=750, seps='.?!;')\n",
    "\n",
    "                if len(text1) < 30 or len(text2) < 30:\n",
    "                    #return text1, text2, data['pair'][0], data['pair'][1]\n",
    "                    counter +=1\n",
    "                    \n",
    "                while len(text1) < 30 or len(text2) < 30:\n",
    "                    if len(text1) < 30:\n",
    "                        n_text1 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text1:\n",
    "                                n_text1.append(sent)\n",
    "                        text1 = n_text1\n",
    "                    elif len(text2) < 30:\n",
    "                        n_text2 = []\n",
    "                        for i in range(30):\n",
    "                            for sent in text2:\n",
    "                                n_text2.append(sent)\n",
    "                        text2 = n_text2\"\"\"\n",
    "\n",
    "                #datas.append((text1, text2, int(truth[index][-2]), str(data['id']), truth[index][-2], truth[index][-1]))\n",
    "\n",
    "            index+=1\n",
    "\n",
    "    print(counter, true, false)\n",
    "    #return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small.jsonl\"\n",
    "\n",
    "data = pd.read_json(path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-truth.jsonl\"\n",
    "\n",
    "truth_data = pd.read_json(path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pair'][26379][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = truth_data.loc[truth_data['authors'] == '1892519']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['fandoms'][26380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = get_data(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-truth.jsonl\",\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small.jsonl\")\n",
    "\n",
    "#195 -- 750\n",
    "#96 -- 714\n",
    "#54 -- 510\n",
    "\n",
    "#207 -- 750 without ;\n",
    "\n",
    "#59 -- 510 without ;\n",
    "#64 -- 510 without ; and 31 segment\n",
    "\n",
    "#1892519 writs about Aranea twice. ['Final Fantasy Versus XIII', 'Final Fantasy XV'] 26380 f6c0bdf6-2ac5-5c11-93ce-bc25cbc80274\n",
    "# and ['Final Fantasy VIII', 'Final Fantasy XV'] 26379 da8a4632-c82d-5dc7-986d-db3d0d4e9bbb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the small version of `PAN20` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = get_data(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small-truth.jsonl\",\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-small.jsonl\")\n",
    "dataset = datasets.Dataset.from_pandas(pd.DataFrame(segmented_data))\n",
    "del segmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = get_data(\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-large-truth.jsonl\",\"..\\BertAA_content\\Data\\pan20-authorship-verification-training-large.jsonl\")\n",
    "large_segmented_data = pd.DataFrame(segmented_data)\n",
    "del segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_segmented_data = large_segmented_data.sample(n = 10000)\n",
    "dataset = datasets.Dataset.from_pandas(sampled_segmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_segmented_data[2].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a HuggingFace tokenizer and tokenize first 30 segments from each text in each pair. Tokenized fragments contain no more than 255 items, since the standard BERT input is limited to 512 tokens, and the input should contain two fratgments and a SEP token between them. We then remove unnecessary features, including the raw texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['0'][:30], example['1'][:30], truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!do_lower_case = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['0'][:30], example['1'][:30], truncation=True, padding='max_length', max_length=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['0', '1', '3', '4', '5'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('2', 'labels')\n",
    "print(tokenizer.decode(tokenized_dataset[0]['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating segmented, but not tokenized dataset\n",
    "\n",
    "dataset = dataset.rename_column('2', 'labels')\n",
    "\n",
    "dataset_tosplit = dataset.train_test_split(test_size=0.1445, shuffle=True, seed = 42)\n",
    "dataset_train = dataset_tosplit['train']\n",
    "dataset_val = dataset_tosplit['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val.save_to_disk(r\"..\\BertAA_content\\can_delete2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dv = load_from_disk(r\"..\\BertAA_content\\can_delete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the data and split it into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_tosplit = tokenized_dataset.train_test_split(test_size=0.1445, shuffle=True, seed = 42)\n",
    "tokenized_dataset_train = tokenized_dataset_tosplit['train']\n",
    "tokenized_dataset_val = tokenized_dataset_tosplit['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train.save_to_disk(r\"..\\BertAA_content\\Data\\45000\\final_processed_dataset_512_low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_val.save_to_disk(r\"..\\BertAA_content\\Data\\45000\\final_validation_set_512_low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['__index_level_0__'])\n",
    "tokenized_dataset.save_to_disk(r\"..\\BertAA_content\\new_sampled_set\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b2b79d7c925dc537986e7099a9668dba419b885563f1cfe4e7cfa1327f89933"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
